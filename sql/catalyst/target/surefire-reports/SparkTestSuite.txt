[36mDiscovery starting.[0m
[36mDiscovery completed in 5 seconds, 690 milliseconds.[0m
[36mRun starting. Expected test count is: 4096[0m
[32mReassignLambdaVariableIDSuite:[0m
[32m- basic: replace positive IDs with unique negative IDs[0m
[32m- ignore LambdaVariable with negative IDs[0m
[32m- fail if positive ID LambdaVariable and negative LambdaVariable both exist[0m
[32mAnalysisHelperSuite:[0m
[32m- setAnalyze is recursive[0m
[32m- resolveOperator runs on operators recursively[0m
[32m- resolveOperatorsDown runs on operators recursively[0m
[32m- resolveExpressions runs on operators recursively[0m
[32m- resolveOperator skips all ready resolved plans[0m
[32m- resolveOperatorsDown skips all ready resolved plans[0m
[32m- resolveExpressions skips all ready resolved plans[0m
[32m- resolveOperator skips partially resolved plans[0m
[32m- resolveOperatorsDown skips partially resolved plans[0m
[32m- resolveExpressions skips partially resolved plans[0m
[32m- do not allow transform in analyzer[0m
[32m- allow transform in resolveOperators in the analyzer[0m
[32m- allow transform with allowInvokingTransformsInAnalyzer in the analyzer[0m
[32mAggregateEstimationSuite:[0m
[32m- SPARK-26894: propagate child stats for aliases in Aggregate[0m
[32m- set an upper bound if the product of ndv's of group-by columns is too large[0m
[32m- data contains all combinations of distinct values of group-by columns.[0m
[32m- empty group-by column[0m
[32m- aggregate on empty table - with or without group-by column[0m
[32m- group-by column with only null value[0m
[32m- group-by column with null value[0m
[32m- non-cbo estimation[0m
[32mLiteralExpressionSuite:[0m
[32m- null[0m
[32m- default[0m
[32m- boolean literals[0m
[32m- int literals[0m
[32m- double literals[0m
[32m- string literals[0m
[32m- sum two literals[0m
[32m- binary literals[0m
[32m- decimal[0m
[32m- array[0m
[32m- seq[0m
[32m- map[0m
[32m- struct[0m
[32m- unsupported types (map and struct) in Literal.apply[0m
[32m- SPARK-24571: char literals[0m
[32m- construct literals from java.time.LocalDate[0m
[32m- construct literals from arrays of java.time.LocalDate[0m
[32m- construct literals from java.time.Instant[0m
[32m- construct literals from arrays of java.time.Instant[0m
[32m- format timestamp literal using spark.sql.session.timeZone[0m
[32m- format date literal independently from time zone[0m
[32mMiscExpressionsSuite:[0m
[32m- assert_true[0m
[32m- uuid[0m
[32m- PrintToStderr[0m
[32mReplaceOperatorSuite:[0m
[32m- replace Intersect with Left-semi Join[0m
[32m- replace Except with Filter while both the nodes are of type Filter[0m
[32m- replace Except with Filter while only right node is of type Filter[0m
[32m- replace Except with Filter while both the nodes are of type Project[0m
[32m- replace Except with Filter while only right node is of type Project[0m
[32m- replace Except with Filter while left node is Project and right node is Filter[0m
[32m- replace Except with Left-anti Join[0m
[32m- replace Except with Filter when only right filter can be applied to the left[0m
[32m- replace Distinct with Aggregate[0m
[32m- replace batch Deduplicate with Aggregate[0m
[32m- add one grouping key if necessary when replace Deduplicate with Aggregate[0m
[32m- don't replace streaming Deduplicate[0m
[32m- SPARK-26366: ReplaceExceptWithFilter should handle properly NULL[0m
[32m- SPARK-26366: ReplaceExceptWithFilter should not transform non-deterministic[0m
[32mSameResultSuite:[0m
[32m- relations[0m
[32m- projections[0m
[32m- filters[0m
[32m- sorts[0m
[32m- union[0m
[32m- hint[0m
[32m- join hint[0m
[32mUnsupportedOperationsSuite:[0m
[32m- batch plan - local relation: supported[0m
[32m- batch plan - streaming source: not supported[0m
[32m- batch plan - select on streaming source: not supported[0m
[32m- streaming plan - no streaming source[0m
[32m- streaming plan - commmands: not supported[0m
[32m- streaming plan - aggregate - multiple batch aggregations: supported[0m
[32m- streaming plan - aggregate - multiple aggregations but only one streaming aggregation: supported[0m
[32m- streaming plan - aggregate - multiple streaming aggregations: not supported[0m
[32m- streaming plan - aggregate - streaming aggregations in update mode: supported[0m
[32m- streaming plan - aggregate - streaming aggregations in complete mode: supported[0m
[32m- streaming plan - aggregate - streaming aggregations with watermark in append mode: supported[0m
[32m- streaming plan - aggregate - streaming aggregations without watermark in append mode: not supported[0m
[32m- streaming plan - distinct aggregate - aggregate on batch relation: supported[0m
[32m- streaming plan - distinct aggregate - aggregate on streaming relation: not supported[0m
[32m- batch plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on batch relation: supported[0m
[32m- batch plan - flatMapGroupsWithState - multiple flatMapGroupsWithState(Append)s on batch relation: supported[0m
[32m- batch plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on batch relation: supported[0m
[32m- batch plan - flatMapGroupsWithState - multiple flatMapGroupsWithState(Update)s on batch relation: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation without aggregation in update mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation without aggregation in append mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation without aggregation in complete mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Append mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Update mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Complete mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation without aggregation in append mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation without aggregation in update mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation before aggregation in Append mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation before aggregation in Update mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation before aggregation in Complete mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation after aggregation in Append mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on streaming relation after aggregation in Update mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation in complete mode: not supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on batch relation inside streaming relation in Append output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Append) on batch relation inside streaming relation in Update output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on batch relation inside streaming relation in Append output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on batch relation inside streaming relation in Update output mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState - multiple flatMapGroupsWithStates on streaming relation and all are in append mode: supported[0m
[32m- streaming plan - flatMapGroupsWithState -  multiple flatMapGroupsWithStates on s streaming relation but some are not in append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation without aggregation in append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation without aggregation in complete mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation with aggregation in Append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation with aggregation in Update mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState on streaming relation with aggregation in Complete mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - multiple mapGroupsWithStates on streaming relation and all are in append mode: not supported[0m
[32m- streaming plan - mapGroupsWithState - mixing mapGroupsWithStates and flatMapGroupsWithStates on streaming relation: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState with event time timeout without watermark: not supported[0m
[32m- streaming plan - mapGroupsWithState - mapGroupsWithState with event time timeout with watermark: supported[0m
[32m- streaming plan - Deduplicate - Deduplicate on streaming relation before aggregation: supported[0m
[32m- streaming plan - Deduplicate - Deduplicate on streaming relation after aggregation: not supported[0m
[32m- streaming plan - Deduplicate - Deduplicate on batch relation inside a streaming query: supported[0m
[32m- streaming plan - single inner join in append mode with stream-stream relations: supported[0m
[32m- streaming plan - single inner join in append mode with stream-batch relations: supported[0m
[32m- streaming plan - single inner join in append mode with batch-stream relations: supported[0m
[32m- streaming plan - single inner join in append mode with batch-batch relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with stream-stream relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with stream-batch relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with batch-stream relations: supported[0m
[32m- streaming plan - multiple inner joins in append mode with batch-batch relations: supported[0m
[32m- streaming plan - inner join in update mode with stream-stream relations: not supported[0m
[32m- streaming plan - inner join in update mode with stream-batch relations: supported[0m
[32m- streaming plan - inner join in update mode with batch-stream relations: supported[0m
[32m- streaming plan - inner join in update mode with batch-batch relations: supported[0m
[32m- streaming plan - full outer join with stream-stream relations: not supported[0m
[32m- streaming plan - full outer join with stream-batch relations: not supported[0m
[32m- streaming plan - full outer join with batch-stream relations: not supported[0m
[32m- streaming plan - full outer join with batch-batch relations: supported[0m
[32m- streaming plan - left outer join with stream-stream relations: not supported[0m
[32m- streaming plan - left outer join with stream-batch relations: supported[0m
[32m- streaming plan - left outer join with batch-stream relations: not supported[0m
[32m- streaming plan - left outer join with batch-batch relations: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and update mode: not supported[0m
[32m- streaming plan - left outer join with stream-stream relations and complete mode: not supported[0m
[32m- streaming plan - left outer join with stream-stream relations and join on attribute with left watermark: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and join on attribute with right watermark: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and join on non-watermarked attribute: not supported[0m
[32m- streaming plan - left outer join with stream-stream relations and state value watermark: supported[0m
[32m- streaming plan - left outer join with stream-stream relations and state value watermark: not supported[0m
[32m- streaming plan - left semi join with stream-stream relations: not supported[0m
[32m- streaming plan - left semi join with stream-batch relations: supported[0m
[32m- streaming plan - left semi join with batch-stream relations: not supported[0m
[32m- streaming plan - left semi join with batch-batch relations: supported[0m
[32m- streaming plan - left anti join with stream-stream relations: not supported[0m
[32m- streaming plan - left anti join with stream-batch relations: supported[0m
[32m- streaming plan - left anti join with batch-stream relations: not supported[0m
[32m- streaming plan - left anti join with batch-batch relations: supported[0m
[32m- streaming plan - right outer join with stream-stream relations: not supported[0m
[32m- streaming plan - right outer join with stream-batch relations: not supported[0m
[32m- streaming plan - right outer join with batch-stream relations: supported[0m
[32m- streaming plan - right outer join with batch-batch relations: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and join on attribute with left watermark: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and join on attribute with right watermark: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and join on non-watermarked attribute: not supported[0m
[32m- streaming plan - right outer join with stream-stream relations and state value watermark: supported[0m
[32m- streaming plan - right outer join with stream-stream relations and state value watermark: not supported[0m
[32m- streaming plan - cogroup with stream-stream relations: not supported[0m
[32m- streaming plan - cogroup with stream-batch relations: not supported[0m
[32m- streaming plan - cogroup with batch-stream relations: not supported[0m
[32m- streaming plan - cogroup with batch-batch relations: supported[0m
[32m- streaming plan - union with stream-stream relations: supported[0m
[32m- streaming plan - union with stream-batch relations: not supported[0m
[32m- streaming plan - union with batch-stream relations: not supported[0m
[32m- streaming plan - union with batch-batch relations: supported[0m
[32m- streaming plan - except with stream-stream relations: not supported[0m
[32m- streaming plan - except with stream-batch relations: supported[0m
[32m- streaming plan - except with batch-stream relations: not supported[0m
[32m- streaming plan - except with batch-batch relations: supported[0m
[32m- streaming plan - intersect with stream-stream relations: not supported[0m
[32m- streaming plan - intersect with stream-batch relations: supported[0m
[32m- streaming plan - intersect with batch-stream relations: supported[0m
[32m- streaming plan - intersect with batch-batch relations: supported[0m
[32m- streaming plan - sort with stream relation: not supported[0m
[32m- streaming plan - sort with batch relation: supported[0m
[32m- streaming plan - sort - sort after aggregation in Complete output mode: supported[0m
[32m- streaming plan - sort - sort before aggregation in Complete output mode: not supported[0m
[32m- streaming plan - sort - sort over aggregated data in Update output mode: not supported[0m
[32m- streaming plan - sample with stream relation: not supported[0m
[32m- streaming plan - sample with batch relation: supported[0m
[32m- streaming plan - window with stream relation: not supported[0m
[32m- streaming plan - window with batch relation: supported[0m
[32m- streaming plan - Append output mode - aggregation: not supported[0m
[32m- streaming plan - Append output mode - no aggregation: supported[0m
[32m- streaming plan - Update output mode - aggregation: supported[0m
[32m- streaming plan - Update output mode - no aggregation: supported[0m
[32m- streaming plan - Complete output mode - aggregation: supported[0m
[32m- streaming plan - Complete output mode - no aggregation: not supported[0m
[32m- streaming plan - MonotonicallyIncreasingID: not supported[0m
[32m- continuous processing - TypedFilter: supported[0m
[32m- Global watermark limit - single streaming aggregation in Append mode[0m
[32m- Global watermark limit - chained streaming aggregations in Append mode[0m
[32m- Global watermark limit - Inner join after streaming aggregation in Append mode[0m
[32m- Global watermark limit - LeftOuter join after streaming aggregation in Append mode[0m
[32m- Global watermark limit - RightOuter join after streaming aggregation in Append mode[0m
[32m- Global watermark limit - deduplicate after streaming aggregation in Append mode[0m
[32m- Global watermark limit - FlatMapGroupsWithState after streaming aggregation in Append mode[0m
[32m- Global watermark limit - single Inner join in Append mode[0m
[32m- Global watermark limit - streaming aggregation after stream-stream Inner join in Append mode[0m
[32m- Global watermark limit - streaming-stream Inner after stream-stream Inner join in Append mode[0m
[32m- Global watermark limit - streaming-stream LeftOuter after stream-stream Inner join in Append mode[0m
[32m- Global watermark limit - streaming-stream RightOuter after stream-stream Inner join in Append mode[0m
[32m- Global watermark limit - FlatMapGroupsWithState after stream-stream Inner join in Append mode[0m
[32m- Global watermark limit - deduplicate after stream-stream Inner join in Append mode[0m
[32m- Global watermark limit - single LeftOuter join in Append mode[0m
[32m- Global watermark limit - streaming aggregation after stream-stream LeftOuter join in Append mode[0m
[32m- Global watermark limit - streaming-stream Inner after stream-stream LeftOuter join in Append mode[0m
[32m- Global watermark limit - streaming-stream LeftOuter after stream-stream LeftOuter join in Append mode[0m
[32m- Global watermark limit - streaming-stream RightOuter after stream-stream LeftOuter join in Append mode[0m
[32m- Global watermark limit - FlatMapGroupsWithState after stream-stream LeftOuter join in Append mode[0m
[32m- Global watermark limit - deduplicate after stream-stream LeftOuter join in Append mode[0m
[32m- Global watermark limit - single RightOuter join in Append mode[0m
[32m- Global watermark limit - streaming aggregation after stream-stream RightOuter join in Append mode[0m
[32m- Global watermark limit - streaming-stream Inner after stream-stream RightOuter join in Append mode[0m
[32m- Global watermark limit - streaming-stream LeftOuter after stream-stream RightOuter join in Append mode[0m
[32m- Global watermark limit - streaming-stream RightOuter after stream-stream RightOuter join in Append mode[0m
[32m- Global watermark limit - FlatMapGroupsWithState after stream-stream RightOuter join in Append mode[0m
[32m- Global watermark limit - deduplicate after stream-stream RightOuter join in Append mode[0m
[32m- Global watermark limit - single FlatMapGroupsWithState in Append mode[0m
[32m- Global watermark limit - streaming aggregation after FlatMapGroupsWithState in Append mode[0m
[32m- Global watermark limit - stream-stream Inner after FlatMapGroupsWithState in Append mode[0m
[32m- Global watermark limit - stream-stream LeftOuter after FlatMapGroupsWithState in Append mode[0m
[32m- Global watermark limit - stream-stream RightOuter after FlatMapGroupsWithState in Append mode[0m
[32m- Global watermark limit - FlatMapGroupsWithState after FlatMapGroupsWithState in Append mode[0m
[32m- Global watermark limit - deduplicate after FlatMapGroupsWithState in Append mode[0m
[32m- Global watermark limit - streaming aggregation after deduplicate in Append mode[0m
[32m- Global watermark limit - Inner join after deduplicate in Append mode[0m
[32m- Global watermark limit - LeftOuter join after deduplicate in Append mode[0m
[32m- Global watermark limit - RightOuter join after deduplicate in Append mode[0m
[32m- Global watermark limit - FlatMapGroupsWithState after deduplicate in Append mode[0m
[32mV2OverwriteByExpressionANSIAnalysisSuite:[0m
[32m- byName: basic behavior[0m
[32m- byName: does not match by position[0m
[32m- byName: case sensitive column resolution[0m
[32m- byName: case insensitive column resolution[0m
[32m- byName: data columns are reordered by name[0m
[32m- byName: fail nullable data written to required columns[0m
[32m- byName: allow required data written to nullable columns[0m
[32m- byName: missing required columns cause failure and are identified by name[0m
[32m- byName: missing optional columns cause failure and are identified by name[0m
[32m- byName: insert safe cast[0m
[32m- byName: fail extra data fields[0m
[32m- byPosition: basic behavior[0m
[32m- byPosition: data columns are not reordered[0m
[32m- byPosition: fail nullable data written to required columns[0m
[32m- byPosition: allow required data written to nullable columns[0m
[32m- byPosition: missing required columns cause failure[0m
[32m- byPosition: missing optional columns cause failure[0m
[32m- byPosition: insert safe cast[0m
[32m- byPosition: fail extra data fields[0m
[32m- bypass output column resolution[0m
[32m- check fields of struct type column[0m
[32m- delete expression is resolved using table fields[0m
[32m- delete expression is not resolved using query fields[0m
[32mStarJoinReorderSuite:[0m
[32m- Test 1: Selective star-join on all dimensions[0m
[32m- Test 2: Star join on a subset of dimensions due to inequality joins[0m
[32m- Test 3:  Star join on a subset of dimensions since join column is not unique[0m
[32m- Test 4: Star join on a subset of dimensions since join column is nullable[0m
[32m- Test 5: Table stats not available for some of the joined tables[0m
[32m- Test 6: Join with complex plans[0m
[32m- Test 7: Comparable fact table sizes[0m
[32m- Test 8: No RI joins[0m
[32m- Test 9: Complex join predicates[0m
[32m- Test 10: Less than two dimensions[0m
[32m- Test 11: Expanding star join[0m
[32m- Test 12: Non selective star join[0m
[32mCodeFormatterSuite:[0m
[32m- removing overlapping comments[0m
[32m- removing extra new lines and comments[0m
[32m- basic example[0m
[32m- nested example[0m
[32m- single line[0m
[32m- if else on the same line[0m
[32m- function calls[0m
[32m- function calls with maxLines=0[0m
[32m- function calls with maxLines=2[0m
[32m- single line comments[0m
[32m- single line comments /* */ [0m
[32m- multi-line comments[0m
[32m- reduce empty lines[0m
[32m- comment place holder[0m
[32mMathExpressionsSuite:[0m
[32m- conv[0m
[32m- e[0m
[32m- pi[0m
[32m- sin[0m
[32m- asin[0m
[32m- sinh[0m
[32m- asinh[0m
[32m- cos[0m
[32m- acos[0m
[32m- cosh[0m
[32m- acosh[0m
[32m- tan[0m
[32m- cot[0m
[32m- atan[0m
[32m- tanh[0m
[32m- atanh[0m
[32m- toDegrees[0m
[32m- toRadians[0m
[32m- cbrt[0m
[32m- ceil[0m
[32m- floor[0m
[32m- factorial[0m
[32m- rint[0m
[32m- exp[0m
[32m- expm1[0m
[32m- signum[0m
[32m- log[0m
[32m- log10[0m
[32m- log1p[0m
[32m- bin[0m
[32m- log2[0m
[32m- sqrt[0m
[32m- pow[0m
[32m- shift left[0m
[32m- shift right[0m
[32m- shift right unsigned[0m
[32m- hex[0m
[32m- unhex[0m
[32m- hypot[0m
[32m- atan2[0m
[32m- binary log[0m
[32m- round/bround[0m
[32mCheckCartesianProductsSuite:[0m
[32m- CheckCartesianProducts doesn't throw an exception if cross joins are enabled)[0m
[32m- CheckCartesianProducts throws an exception for join types that require a join condition[0m
[32m- CheckCartesianProducts doesn't throw an exception if a join condition is present[0m
[32m- CheckCartesianProducts doesn't throw an exception if join types don't require conditions[0m
[32mLookupCatalogSuite:[0m
[32m- catalog and identifier[0m
[32m- table identifier[0m
[32mProjectEstimationSuite:[0m
[32m- project with alias[0m
[32m- project on empty table[0m
[32m- test row size estimation[0m
[32mCSVExprUtilsSuite:[0m
[32m- Can parse escaped characters[0m
[32m- Does not accept delimiter larger than one character[0m
[32m- Throws exception for unsupported escaped characters[0m
[32m- string with one backward slash is prohibited[0m
[32m- output proper error message for empty string[0m
[32m- should correctly produce separator strings, or exceptions, from input[0m
[32mArrowUtilsSuite:[0m
[32m- simple[0m
[32m- timestamp[0m
[32m- array[0m
[32m- struct[0m
[32mEncoderResolutionSuite:[0m
[32m- real type doesn't match encoder schema but they are compatible: product[0m
[32m- real type doesn't match encoder schema but they are compatible: nested product[0m
[32m- real type doesn't match encoder schema but they are compatible: tupled encoder[0m
[32m- real type doesn't match encoder schema but they are compatible: primitive array[0m
[32m- the real type is not compatible with encoder schema: primitive array[0m
[32m- real type doesn't match encoder schema but they are compatible: array[0m
[32m- real type doesn't match encoder schema but they are compatible: nested array[0m
[32m- the real type is not compatible with encoder schema: non-array field[0m
[32m- the real type is not compatible with encoder schema: array element type[0m
[32m- the real type is not compatible with encoder schema: nested array element type[0m
[32m- nullability of array type element should not fail analysis[0m
[32m- the real number of fields doesn't match encoder schema: tuple encoder[0m
[32m- the real number of fields doesn't match encoder schema: nested tuple encoder[0m
[32m- nested case class can have different number of fields from the real schema[0m
[32m- SPARK-28497: complex type is not compatible with string encoder schema[0m
[32m- throw exception if real type is not compatible with encoder schema[0m
[32m- cast from int to Long should success[0m
[32m- cast from date to java.sql.Timestamp should success[0m
[32m- cast from bigint to String should success[0m
[32m- cast from int to java.math.BigDecimal should success[0m
[32m- cast from bigint to java.math.BigDecimal should success[0m
[32m- cast from bigint to Int should fail[0m
[32m- cast from timestamp to java.sql.Date should fail[0m
[32m- cast from decimal(38,18) to Double should fail[0m
[32m- cast from double to java.math.BigDecimal should fail[0m
[32m- cast from decimal(38,18) to Int should fail[0m
[32m- cast from string to Long should fail[0m
[32mIntervalUtilsSuite:[0m
[32m- string to interval: basic[0m
[32m- string to interval: multiple units[0m
[32m- string to interval: special cases[0m
[32m- string to interval: whitespaces[0m
[32m- string to interval: seconds with fractional part[0m
[32m- from year-month string[0m
[32m- from day-time string - legacy[0m
[32m- interval duration[0m
[32m- negative interval[0m
[32m- negate[0m
[32m- subtract one interval by another[0m
[32m- add two intervals[0m
[32m- multiply by num[0m
[32m- divide by num[0m
[32m- justify days[0m
[32m- justify hours[0m
[32m- justify interval[0m
[32m- to ansi sql standard string[0m
[32m- to iso 8601 string[0m
[32m- to multi units string[0m
[32m- from day-time string[0m
[32mCollectionExpressionsSuite:[0m
[32m- Array and Map Size - legacy[0m
[32m- Array and Map Size[0m
[32m- MapKeys/MapValues[0m
[32m- MapEntries[0m
[32m- Map Concat[0m
[32m- MapFromEntries[0m
[32m- Sort Array[0m
[32m- Array contains[0m
[32m- ArraysOverlap[0m
[32m- Slice[0m
[32m- ArrayJoin[0m
[32m- ArraysZip[0m
[32m- Array Min[0m
[32m- Array max[0m
[32m- Sequence of numbers[0m
[32m- Sequence of timestamps[0m
[32m- Sequence on DST boundaries[0m
[32m- Sequence of dates[0m
[32m- Sequence with default step[0m
[32m- Reverse[0m
[32m- Array Position[0m
[32m- elementAt[0m
[32m- correctly handles ElementAt nullability for arrays[0m
[32m- Concat[0m
[32m- Flatten[0m
[32m- ArrayRepeat[0m
[32m- Array remove[0m
[32m- Array Distinct[0m
[32m- Array Union[0m
[32m- Shuffle[0m
[32m- Array Except[0m
[32m- Array Except - null handling[0m
[32m- Array Intersect[0m
[32m- Array Intersect - null handling[0m
[32mQueryPlanSuite:[0m
[32m- origin remains the same after mapExpressions (SPARK-23823)[0m
[32m- collectInPlanAndSubqueries[0m
[32mUDFXPathUtilSuite:[0m
[32m- illegal arguments[0m
[32m- generic eval[0m
[32m- boolean eval[0m
[32m- string eval[0m
[32m- embedFailure[0m
[32m- number eval[0m
[32m- node eval[0m
[32m- node list eval[0m
[32mJsonInferSchemaSuite:[0m
[32m- inferring timestamp type[0m
[32m- prefer decimals over timestamps[0m
[32m- skip decimal type inferring[0m
[32m- fallback to string type[0m
[32m- disable timestamp inferring[0m
[32mPullOutNondeterministicSuite:[0m
[32m- no-op on filter[0m
[32m- sort[0m
[32m- aggregate[0m
[32mStarJoinCostBasedReorderSuite:[0m
[32m- Test 1: Star query with two dimensions and two regular tables[0m
[32m- Test 2: Star with a linear branch[0m
[32m- Test 3: Star with derived branches[0m
[32m- Test 4: Star with several branches[0m
[32m- Test 5: RI star only[0m
[32m- Test 6: No RI star[0m
[32mV2OverwritePartitionsDynamicANSIAnalysisSuite:[0m
[32m- byName: basic behavior[0m
[32m- byName: does not match by position[0m
[32m- byName: case sensitive column resolution[0m
[32m- byName: case insensitive column resolution[0m
[32m- byName: data columns are reordered by name[0m
[32m- byName: fail nullable data written to required columns[0m
[32m- byName: allow required data written to nullable columns[0m
[32m- byName: missing required columns cause failure and are identified by name[0m
[32m- byName: missing optional columns cause failure and are identified by name[0m
[32m- byName: insert safe cast[0m
[32m- byName: fail extra data fields[0m
[32m- byPosition: basic behavior[0m
[32m- byPosition: data columns are not reordered[0m
[32m- byPosition: fail nullable data written to required columns[0m
[32m- byPosition: allow required data written to nullable columns[0m
[32m- byPosition: missing required columns cause failure[0m
[32m- byPosition: missing optional columns cause failure[0m
[32m- byPosition: insert safe cast[0m
[32m- byPosition: fail extra data fields[0m
[32m- bypass output column resolution[0m
[32m- check fields of struct type column[0m
[32mOptimizerStructuralIntegrityCheckerSuite:[0m
[32m- check for invalid plan after execution of rule - unresolved attribute[0m
[32m- check for invalid plan after execution of rule - special expression in wrong operator[0m
[32m- check for invalid plan before execution of any rule[0m
[32mOuterJoinEliminationSuite:[0m
[32m- joins: full outer to inner[0m
[32m- joins: full outer to right[0m
[32m- joins: full outer to left[0m
[32m- joins: right to inner[0m
[32m- joins: left to inner[0m
[32m- joins: left to inner with complicated filter predicates #1[0m
[32m- joins: left to inner with complicated filter predicates #2[0m
[32m- joins: left to inner with complicated filter predicates #3[0m
[32m- joins: left to inner with complicated filter predicates #4[0m
[32m- joins: no outer join elimination if the filter is not NULL eliminated[0m
[32m- joins: no outer join elimination if the filter's constraints are not NULL eliminated[0m
[32m- no outer join elimination if constraint propagation is disabled[0m
[32mObjectExpressionsSuite:[0m
[32m- SPARK-16622: The returned value of the called method in Invoke can be null[0m
[32m- MapObjects should make copies of unsafe-backed data[0m
[32m- SPARK-23582: StaticInvoke should support interpreted execution[0m
[32m- SPARK-23583: Invoke should support interpreted execution[0m
[32m- SPARK-23593: InitializeJavaBean should support interpreted execution[0m
[32m- InitializeJavaBean doesn't call setters if input in null[0m
[32m- SPARK-23585: UnwrapOption should support interpreted execution[0m
[32m- SPARK-23586: WrapOption should support interpreted execution[0m
[32m- SPARK-23590: CreateExternalRow should support interpreted execution[0m
[32m- SPARK-23594 GetExternalRowField should support interpreted execution[0m
[32m- SPARK-23591: EncodeUsingSerializer should support interpreted execution[0m
[32m- SPARK-23587: MapObjects should support interpreted execution[0m
[32m- SPARK-23592: DecodeUsingSerializer should support interpreted execution[0m
[32m- SPARK-23584 NewInstance should support interpreted execution[0m
[32m- LambdaVariable should support interpreted execution[0m
[32m- SPARK-23588 CatalystToExternalMap should support interpreted execution[0m
[32m- SPARK-23595 ValidateExternalType should support interpreted execution[0m
[32m- SPARK-23589 ExternalMapToCatalyst should support interpreted execution[0m
[32mIntervalExpressionsSuite:[0m
[32m- millenniums[0m
[32m- centuries[0m
[32m- decades[0m
[32m- years[0m
[32m- quarters[0m
[32m- months[0m
[32m- days[0m
[32m- hours[0m
[32m- minutes[0m
[32m- seconds[0m
[32m- milliseconds[0m
[32m- microseconds[0m
[32m- epoch[0m
[32m- multiply[0m
[32m- divide[0m
[32m- make interval[0m
[32mAnalysisSuite:[0m
[32m- union project *[0m
[32m- check project's resolved[0m
[32m- analyze project[0m
[32m- resolve sort references - filter/limit[0m
[32m- resolve sort references - join[0m
[32m- resolve sort references - aggregate[0m
[32m- resolve relations[0m
[32m- divide should be casted into fractional types[0m
[32m- pull out nondeterministic expressions from RepartitionByExpression[0m
[32m- pull out nondeterministic expressions from Sort[0m
[32m- SPARK-9634: cleanup unnecessary Aliases in LogicalPlan[0m
[32m- Analysis may leave unnecessary aliases[0m
[32m- SPARK-10534: resolve attribute references in order by clause[0m
[32m- self intersect should resolve duplicate expression IDs[0m
[32m- SPARK-8654: invalid CAST in NULL IN(...) expression[0m
[32m- SPARK-8654: different types in inlist but can be converted to a common type[0m
[32m- SPARK-8654: check type compatibility error[0m
[32m- SPARK-11725: correctly handle null inputs for ScalaUDF[0m
[32m- SPARK-24891 Fix HandleNullInputsForUDF rule[0m
[32m- SPARK-11863 mixture of aliases and real columns in order by clause - tpcds 19,55,71[0m
[32m- Eliminate the unnecessary union[0m
[32m- SPARK-12102: Ignore nullablity when comparing two sides of case[0m
[32m- Keep attribute qualifiers after dedup[0m
[32m- SPARK-15776: test whether Divide expression's data type can be deduced correctly by analyzer[0m
[32m- SPARK-18058: union and set operations shall not care about the nullability when comparing column types[0m
[32m- resolve as with an already existed alias[0m
[32m- SPARK-20311 range(N) as alias[0m
[32m- SPARK-20841 Support table column aliases in FROM clause[0m
[32m- SPARK-20962 Support subquery column aliases in FROM clause[0m
[32m- SPARK-20963 Support aliases for join relations in FROM clause[0m
[32m- SPARK-22614 RepartitionByExpression partitioning[0m
[32m- SPARK-24208: analysis fails on self-join with FlatMapGroupsInPandas[0m
[32m- SPARK-24488 Generator with multiple aliases[0m
[32m- SPARK-24151: CURRENT_DATE, CURRENT_TIMESTAMP should be case insensitive[0m
[32m- SPARK-25691: AliasViewChild with different nullabilities[0m
[32m- CTE with non-existing column alias[0m
[32m- CTE with non-matching column alias[0m
[32m- SPARK-28251: Insert into non-existing table error message is user friendly[0m
[32m- check CollectMetrics resolved[0m
[32m- check CollectMetrics duplicates[0m
[32mNondeterministicSuite:[0m
[32m- MonotonicallyIncreasingID[0m
[32m- SparkPartitionID[0m
[32m- InputFileName[0m
[32mV2AppendDataStrictAnalysisSuite:[0m
[32m- byName: basic behavior[0m
[32m- byName: does not match by position[0m
[32m- byName: case sensitive column resolution[0m
[32m- byName: case insensitive column resolution[0m
[32m- byName: data columns are reordered by name[0m
[32m- byName: fail nullable data written to required columns[0m
[32m- byName: allow required data written to nullable columns[0m
[32m- byName: missing required columns cause failure and are identified by name[0m
[32m- byName: missing optional columns cause failure and are identified by name[0m
[32m- byName: insert safe cast[0m
[32m- byName: fail extra data fields[0m
[32m- byPosition: basic behavior[0m
[32m- byPosition: data columns are not reordered[0m
[32m- byPosition: fail nullable data written to required columns[0m
[32m- byPosition: allow required data written to nullable columns[0m
[32m- byPosition: missing required columns cause failure[0m
[32m- byPosition: missing optional columns cause failure[0m
[32m- byPosition: insert safe cast[0m
[32m- byPosition: fail extra data fields[0m
[32m- bypass output column resolution[0m
[32m- check fields of struct type column[0m
[32m- byName: fail canWrite check[0m
[32m- byName: multiple field errors are reported[0m
[32m- byPosition: fail canWrite check[0m
[32m- byPosition: multiple field errors are reported[0m
[32mExpressionParserSuite:[0m
[32m- star expressions[0m
[32m- named expressions[0m
[32m- binary logical expressions[0m
[32m- long binary logical expressions[0m
[32m- not expressions[0m
[32m- exists expression[0m
[32m- comparison expressions[0m
[32m- between expressions[0m
[32m- in expressions[0m
[32m- in sub-query[0m
[32m- like expressions[0m
[32m- like escape expressions[0m
[32m- like expressions with ESCAPED_STRING_LITERALS = true[0m
[32m- is null expressions[0m
[32m- is distinct expressions[0m
[32m- binary arithmetic expressions[0m
[32m- unary arithmetic expressions[0m
[32m- cast expressions[0m
[32m- function expressions[0m
[32m- lambda functions[0m
[32m- window function expressions[0m
[32m- range/rows window function expressions[0m
[32m- row constructor[0m
[32m- scalar sub-query[0m
[32m- case when[0m
[32m- dereference[0m
[32m- reference[0m
[32m- subscript[0m
[32m- parenthesis[0m
[32m- type constructors[0m
[32m- literals[0m
[32m- SPARK-29956: scientific decimal should be parsed as Decimal in legacy mode[0m
[32m- strings[0m
[32m- intervals[0m
[32m- SPARK-23264 Interval Compatibility tests[0m
[32m- composed expressions[0m
[32m- SPARK-17364, fully qualified column name which starts with number[0m
[32m- SPARK-17832 function identifier contains backtick[0m
[32m- SPARK-19526 Support ignore nulls keywords for first and last[0m
[32m- Support respect nulls keywords for first_value and last_value[0m
[32m- timestamp literals[0m
[32m- date literals[0m
[32m- current date/timestamp braceless expressions[0m
[32mFilterEstimationSuite:[0m
[32m- true[0m
[32m- false[0m
[32m- null[0m
[32m- Not(null)[0m
[32m- Not(Not(null))[0m
[32m- cint < 3 AND null[0m
[32m- cint < 3 OR null[0m
[32m- Not(cint < 3 AND null)[0m
[32m- Not(cint < 3 OR null)[0m
[32m- Not(cint < 3 AND Not(null))[0m
[32m- cint = 2[0m
[32m- cint <=> 2[0m
[32m- cint = 0[0m
[32m- cint < 3[0m
[32m- cint < 0[0m
[32m- cint <= 3[0m
[32m- cint > 6[0m
[32m- cint > 10[0m
[32m- cint >= 6[0m
[32m- cint IS NULL[0m
[32m- cint IS NOT NULL[0m
[32m- cint IS NOT NULL && null[0m
[32m- cint > 3 AND cint <= 6[0m
[32m- cint = 3 OR cint = 6[0m
[32m- Not(cint > 3 AND cint <= 6)[0m
[32m- Not(cint <= 3 OR cint > 6)[0m
[32m- Not(cint = 3 AND cstring < 'A8')[0m
[32m- Not(cint = 3 OR cstring < 'A8')[0m
[32m- cint IN (3, 4, 5)[0m
[32m- evaluateInSet with all zeros[0m
[32m- evaluateInSet with string[0m
[32m- cint NOT IN (3, 4, 5)[0m
[32m- cbool IN (true)[0m
[32m- cbool = true[0m
[32m- cbool > false[0m
[32m- cdate = cast('2017-01-02' AS DATE)[0m
[32m- cdate < cast('2017-01-03' AS DATE)[0m
[32m- cdate IN ( cast('2017-01-03' AS DATE),[0m
[32m      cast('2017-01-04' AS DATE), cast('2017-01-05' AS DATE) )[0m
[32m- cdecimal = 0.400000000000000000[0m
[32m- cdecimal < 0.60 [0m
[32m- cdouble < 3.0[0m
[32m- cstring = 'A2'[0m
[32m- cstring < 'A2' - unsupported condition[0m
[32m- cint IN (1, 2, 3, 4, 5)[0m
[32m- don't estimate IsNull or IsNotNull if the child is a non-leaf node[0m
[32m- cint = cint2[0m
[32m- cint > cint2[0m
[32m- cint < cint2[0m
[32m- cint = cint4[0m
[32m- cint < cint4[0m
[32m- cint = cint3[0m
[32m- cint < cint3[0m
[32m- cint > cint3[0m
[32m- update ndv for columns based on overall selectivity[0m
[32m- Not(cintHgm < 3 AND null)[0m
[32m- cintHgm = 5[0m
[32m- cintHgm = 0[0m
[32m- cintHgm < 3[0m
[32m- cintHgm < 0[0m
[32m- cintHgm <= 3[0m
[32m- cintHgm > 6[0m
[32m- cintHgm > 10[0m
[32m- cintHgm >= 6[0m
[32m- cintHgm > 3 AND cintHgm <= 6[0m
[32m- cintHgm = 3 OR cintHgm = 6[0m
[32m- Not(cintSkewHgm < 3 AND null)[0m
[32m- cintSkewHgm = 5[0m
[32m- cintSkewHgm = 0[0m
[32m- cintSkewHgm < 3[0m
[32m- cintSkewHgm < 0[0m
[32m- cintSkewHgm <= 3[0m
[32m- cintSkewHgm > 6[0m
[32m- cintSkewHgm > 10[0m
[32m- cintSkewHgm >= 6[0m
[32m- cintSkewHgm > 3 AND cintSkewHgm <= 6[0m
[32m- cintSkewHgm = 3 OR cintSkewHgm = 6[0m
[32m- ColumnStatsMap tests[0m
[32mDataTypeSuite:[0m
[32m- construct an ArrayType[0m
[32m- construct an MapType[0m
[32m- construct with add[0m
[32m- construct with add from StructField[0m
[32m- construct with add from StructField with comments[0m
[32m- construct with String DataType[0m
[32m- extract fields from a StructType[0m
[32m- extract field index from a StructType[0m
[32m- fieldsMap returns map of name to StructField[0m
[32m- fieldNames and names returns field names[0m
[32m- merge where right contains type conflict[0m
[32m- existsRecursively[0m
[32m- from Json - NullType[0m
[32m- from Json - BooleanType[0m
[32m- from DDL - BooleanType[0m
[32m- from Json - ByteType[0m
[32m- from DDL - ByteType[0m
[32m- from Json - ShortType[0m
[32m- from DDL - ShortType[0m
[32m- from Json - IntegerType[0m
[32m- from DDL - IntegerType[0m
[32m- from Json - LongType[0m
[32m- from DDL - LongType[0m
[32m- from Json - FloatType[0m
[32m- from DDL - FloatType[0m
[32m- from Json - DoubleType[0m
[32m- from DDL - DoubleType[0m
[32m- from Json - DecimalType(10,5)[0m
[32m- from DDL - DecimalType(10,5)[0m
[32m- from Json - DecimalType(38,18)[0m
[32m- from DDL - DecimalType(38,18)[0m
[32m- from Json - DateType[0m
[32m- from DDL - DateType[0m
[32m- from Json - TimestampType[0m
[32m- from DDL - TimestampType[0m
[32m- from Json - StringType[0m
[32m- from DDL - StringType[0m
[32m- from Json - BinaryType[0m
[32m- from DDL - BinaryType[0m
[32m- from Json - ArrayType(DoubleType,true)[0m
[32m- from DDL - ArrayType(DoubleType,true)[0m
[32m- from Json - ArrayType(StringType,false)[0m
[32m- from DDL - ArrayType(StringType,false)[0m
[32m- from Json - MapType(IntegerType,StringType,true)[0m
[32m- from DDL - MapType(IntegerType,StringType,true)[0m
[32m- from Json - MapType(IntegerType,ArrayType(DoubleType,true),false)[0m
[32m- from DDL - MapType(IntegerType,ArrayType(DoubleType,true),false)[0m
[32m- from Json - StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))[0m
[32m- from DDL - StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))[0m
[32m- fromJson throws an exception when given type string is invalid[0m
[32m- Check the default size of NullType[0m
[32m- Check the default size of BooleanType[0m
[32m- Check the default size of ByteType[0m
[32m- Check the default size of ShortType[0m
[32m- Check the default size of IntegerType[0m
[32m- Check the default size of LongType[0m
[32m- Check the default size of FloatType[0m
[32m- Check the default size of DoubleType[0m
[32m- Check the default size of DecimalType(10,5)[0m
[32m- Check the default size of DecimalType(38,18)[0m
[32m- Check the default size of DateType[0m
[32m- Check the default size of TimestampType[0m
[32m- Check the default size of StringType[0m
[32m- Check the default size of BinaryType[0m
[32m- Check the default size of ArrayType(DoubleType,true)[0m
[32m- Check the default size of ArrayType(StringType,false)[0m
[32m- Check the default size of MapType(IntegerType,StringType,true)[0m
[32m- Check the default size of MapType(IntegerType,ArrayType(DoubleType,true),false)[0m
[32m- Check the default size of StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,true), to: ArrayType(DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,true), to: ArrayType(DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(StringType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,true), to: MapType(StringType,DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,false), to: MapType(StringType,DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,false), to: MapType(StringType,DoubleType,true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,true), to: MapType(StringType,DoubleType,false))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,ArrayType(IntegerType,true),true), to: MapType(StringType,ArrayType(IntegerType,false),true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: MapType(StringType,ArrayType(IntegerType,false),true), to: MapType(StringType,ArrayType(IntegerType,true),true))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,true)), to: StructType(StructField(a,StringType,true)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false)), to: StructType(StructField(a,StringType,false)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false)), to: StructType(StructField(a,StringType,true)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,true)), to: StructType(StructField(a,StringType,false)))[0m
[32m- equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false), StructField(b,StringType,true)), to: StructType(StructField(a,StringType,false), StructField(b,StringType,false)))[0m
[32m- catalogString: BooleanType[0m
[32m- catalogString: ByteType[0m
[32m- catalogString: ShortType[0m
[32m- catalogString: IntegerType[0m
[32m- catalogString: LongType[0m
[32m- catalogString: FloatType[0m
[32m- catalogString: DoubleType[0m
[32m- catalogString: DecimalType(10,5)[0m
[32m- catalogString: BinaryType[0m
[32m- catalogString: StringType[0m
[32m- catalogString: DateType[0m
[32m- catalogString: TimestampType[0m
[32m- catalogString: StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))[0m
[32m- catalogString: StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true))[0m
[32m- catalogString: ArrayType(IntegerType,true)[0m
[32m- catalogString: ArrayType(StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true)),true)[0m
[32m- catalogString: MapType(IntegerType,StringType,true)[0m
[32m- catalogString: MapType(IntegerType,StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true)),true)[0m
[32m- equalsStructurally: (from: BooleanType, to: BooleanType)[0m
[32m- equalsStructurally: (from: IntegerType, to: IntegerType)[0m
[32m- equalsStructurally: (from: IntegerType, to: LongType)[0m
[32m- equalsStructurally: (from: ArrayType(IntegerType,true), to: ArrayType(IntegerType,true))[0m
[32m- equalsStructurally: (from: ArrayType(IntegerType,true), to: ArrayType(IntegerType,false))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true)), to: StructType(StructField(f2,IntegerType,true)))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true)), to: StructType(StructField(f2,IntegerType,false)))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true), StructField(f,StructType(StructField(f2,StringType,true)),true)), to: StructType(StructField(f2,IntegerType,true), StructField(g,StructType(StructField(f1,StringType,true)),true)))[0m
[32m- equalsStructurally: (from: StructType(StructField(f1,IntegerType,true), StructField(f,StructType(StructField(f2,StringType,false)),true)), to: StructType(StructField(f2,IntegerType,true), StructField(g,StructType(StructField(f1,StringType,true)),true)))[0m
[32m- SPARK-25031: MapType should produce current formatted string for complex types[0m
[32mQueryPlanningTrackerSuite:[0m
[32m- phases[0m
[32m- multiple measurePhase call[0m
[32m- rules[0m
[32m- topRulesByTime[0m
[32mFoldablePropagationSuite:[0m
[32m- Propagate from subquery[0m
[32m- Propagate to select clause[0m
[32m- Propagate to where clause[0m
[32m- Propagate to orderBy clause[0m
[32m- Propagate to groupBy clause[0m
[32m- Propagate in a complex query[0m
[32m- Propagate in subqueries of Union queries[0m
[32m- Propagate in inner join[0m
[32m- Propagate in expand[0m
[32m- Propagate above outer join[0m
[32mInMemoryCatalogSuite:[0m
[32m- basic create and list databases[0m
[32m- get database when a database exists[0m
[32m- get database should throw exception when the database does not exist[0m
[32m- list databases without pattern[0m
[32m- list databases with pattern[0m
[32m- drop database[0m
[32m- drop database when the database is not empty[0m
[32m- drop database when the database does not exist[0m
[32m- alter database[0m
[32m- alter database should throw exception when the database does not exist[0m
[32m- the table type of an external table should be EXTERNAL_TABLE[0m
[32m- create table when the table already exists[0m
[32m- drop table[0m
[32m- drop table when database/table does not exist[0m
[32m- rename table[0m
[32m- rename table when database/table does not exist[0m
[32m- rename table when destination table already exists[0m
[32m- alter table[0m
[32m- alter table when database/table does not exist[0m
[32m- alter table schema[0m
[32m- alter table stats[0m
[32m- get table[0m
[32m- get table when database/table does not exist[0m
[32m- get tables by name[0m
[32m- get tables by name when some tables do not exists[0m
[32m- get tables by name when contains invalid name[0m
[32m- get tables by name when empty table list[0m
[32m- list tables without pattern[0m
[32m- list tables with pattern[0m
[32m- column names should be case-preserving and column nullability should be retained[0m
[32m- basic create and list partitions[0m
[32m- create partitions when database/table does not exist[0m
[32m- create partitions that already exist[0m
[32m- create partitions without location[0m
[32m- create/drop partitions in managed tables with location[0m
[32m- list partition names[0m
[32m- list partition names with partial partition spec[0m
[32m- list partitions with partial partition spec[0m
[32m- SPARK-21457: list partitions with special chars[0m
[32m- list partitions by filter[0m
[32m- drop partitions[0m
[32m- drop partitions when database/table does not exist[0m
[32m- drop partitions that do not exist[0m
[32m- get partition[0m
[32m- get partition when database/table does not exist[0m
[32m- rename partitions[0m
[32m- rename partitions should update the location for managed table[0m
[32m- rename partitions when database/table does not exist[0m
[32m- rename partitions when the new partition already exists[0m
[32m- alter partitions[0m
[32m- alter partitions when database/table does not exist[0m
[32m- basic create and list functions[0m
[32m- create function when database does not exist[0m
[32m- create function that already exists[0m
[32m- drop function[0m
[32m- drop function when database does not exist[0m
[32m- drop function that does not exist[0m
[32m- get function[0m
[32m- get function when database does not exist[0m
[32m- rename function[0m
[32m- rename function when database does not exist[0m
[32m- rename function when new function already exists[0m
[32m- alter function[0m
[32m- list functions[0m
[32m- create/drop database should create/delete the directory[0m
[32m- create/drop/rename table should create/delete/rename the directory[0m
[32m- create/drop/rename partitions should create/delete/rename the directory[0m
[32m- drop partition from external table should not delete the directory[0m
[32mNestedColumnAliasingSuite:[0m
[32m- Pushing a single nested field projection[0m
[32m- Pushing multiple nested field projection[0m
[32m- function with nested field inputs[0m
[32m- multi-level nested field[0m
[32m- Push original case-sensitive names[0m
[32m- Pushing a single nested field projection - negative[0m
[32m- Pushing a single nested field projection through filters - negative[0m
[32m- Do not optimize when parent field is used[0m
[32m- Some nested column means the whole structure[0m
[32m- nested field pruning for getting struct field in array of struct[0m
[32m- nested field pruning for getting struct field in map[0m
[32mTypedFilterOptimizationSuite:[0m
[32m- filter after serialize with the same object type[0m
[32m- filter after serialize with different object types[0m
[32m- filter before deserialize with the same object type[0m
[32m- filter before deserialize with different object types[0m
[32m- back to back filter with the same object type[0m
[32m- back to back filter with different object types[0m
[32m- back to back FilterFunction with the same object type[0m
[32m- back to back FilterFunction with different object types[0m
[32m- FilterFunction and filter with the same object type[0m
[32m- FilterFunction and filter with different object types[0m
[32m- filter and FilterFunction with the same object type[0m
[32m- filter and FilterFunction with different object types[0m
[32mSchemaPruningSuite:[0m
[32m- prune schema by the requested fields[0m
[32mReorderAssociativeOperatorSuite:[0m
[32m- Reorder associative operators[0m
[32m- nested expression with aggregate operator[0m
[32mCombiningLimitsSuite:[0m
[32m- limits: combines two limits[0m
[32m- limits: combines three limits[0m
[32m- limits: combines two limits after ColumnPruning[0m
[32mResolveHintsSuite:[0m
[32m- invalid hints should be ignored[0m
[32m- case-sensitive or insensitive parameters[0m
[32m- multiple broadcast hint aliases[0m
[32m- do not traverse past existing broadcast hints[0m
[32m- should work for subqueries[0m
[32m- do not traverse past subquery alias[0m
[32m- should work for CTE[0m
[32m- should not traverse down CTE[0m
[32m- coalesce and repartition hint[0m
[32m- log warnings for invalid hints[0m
[32m- SPARK-30003: Do not throw stack overflow exception in non-root unknown hint resolution[0m
[32mPruneFiltersSuite:[0m
[32m- Constraints of isNull + LeftOuter[0m
[32m- Constraints of unionall[0m
[32m- Pruning multiple constraints in the same run[0m
[32m- Partial pruning[0m
[32m- No predicate is pruned[0m
[32m- Nondeterministic predicate is not pruned[0m
[32m- No pruning when constraint propagation is disabled[0m
[32mInMemorySessionCatalogSuite:[0m
[32m- basic create and list databases[0m
[32m- create databases using invalid names[0m
[32m- get database when a database exists[0m
[32m- get database should throw exception when the database does not exist[0m
[32m- list databases without pattern[0m
[32m- list databases with pattern[0m
[32m- drop database[0m
[32m- drop database when the database is not empty[0m
[32m- drop database when the database does not exist[0m
[32m- drop current database and drop default database[0m
[32m- alter database[0m
[32m- alter database should throw exception when the database does not exist[0m
[32m- get/set current database[0m
[32m- create table[0m
[32m- create tables using invalid names[0m
[32m- create table when database does not exist[0m
[32m- create temp view[0m
[32m- drop table[0m
[32m- drop table when database/table does not exist[0m
[32m- drop temp table[0m
[32m- rename table[0m
[32m- rename tables to an invalid name[0m
[32m- rename table when database/table does not exist[0m
[32m- rename temp table[0m
[32m- alter table[0m
[32m- alter table when database/table does not exist[0m
[32m- alter table stats[0m
[32m- alter table add columns[0m
[32m- alter table drop columns[0m
[32m- get table[0m
[32m- get table when database/table does not exist[0m
[32m- get tables by name[0m
[32m- get tables by name when some tables do not exist[0m
[32m- get tables by name when contains invalid name[0m
[32m- get tables by name when empty[0m
[32m- get tables by name when tables belong to different databases[0m
[32m- lookup table relation[0m
[32m- look up view relation[0m
[32m- table exists[0m
[32m- getTempViewOrPermanentTableMetadata on temporary views[0m
[32m- list tables without pattern[0m
[32m- list tables with pattern[0m
[32m- list tables with pattern and includeLocalTempViews[0m
[32m- list temporary view with pattern[0m
[32m- list global temporary view and local temporary view with pattern[0m
[32m- basic create and list partitions[0m
[32m- create partitions when database/table does not exist[0m
[32m- create partitions that already exist[0m
[32m- create partitions with invalid part spec[0m
[32m- drop partitions[0m
[32m- drop partitions when database/table does not exist[0m
[32m- drop partitions that do not exist[0m
[32m- drop partitions with invalid partition spec[0m
[32m- get partition[0m
[32m- get partition when database/table does not exist[0m
[32m- get partition with invalid partition spec[0m
[32m- rename partitions[0m
[32m- rename partitions when database/table does not exist[0m
[32m- rename partition with invalid partition spec[0m
[32m- alter partitions[0m
[32m- alter partitions when database/table does not exist[0m
[32m- alter partition with invalid partition spec[0m
[32m- list partition names[0m
[32m- list partition names with partial partition spec[0m
[32m- list partition names with invalid partial partition spec[0m
[32m- list partitions[0m
[32m- list partitions with partial partition spec[0m
[32m- list partitions with invalid partial partition spec[0m
[32m- list partitions when database/table does not exist[0m
[32m- basic create and list functions[0m
[32m- create function when database does not exist[0m
[32m- create function that already exists[0m
[32m- create temp function[0m
[32m- isTemporaryFunction[0m
[32m- isRegisteredFunction[0m
[32m- isPersistentFunction[0m
[32m- drop function[0m
[32m- drop function when database/function does not exist[0m
[32m- drop temp function[0m
[32m- get function[0m
[32m- get function when database/function does not exist[0m
[32m- lookup temp function[0m
[32m- list functions[0m
[32m- list functions when database does not exist[0m
[32m- copy SessionCatalog state - temp views[0m
[32m- copy SessionCatalog state - current db[0m
[32m- SPARK-19737: detect undefined functions without triggering relation resolution[0m
[32m- SPARK-24544: test print actual failure cause when look up function failed[0m
[32mComplexTypeSuite:[0m
[32m- GetArrayItem[0m
[32m- SPARK-26637 handles GetArrayItem nullability correctly when input array size is constant[0m
[32m- GetMapValue[0m
[32m- GetStructField[0m
[32m- GetArrayStructFields[0m
[32m- CreateArray[0m
[32m- CreateMap[0m
[32m- MapFromArrays[0m
[32m- CreateStruct[0m
[32m- CreateNamedStruct[0m
[32m- test dsl for complex type[0m
[32m- error message of ExtractValue[0m
[32m- ensure to preserve metadata[0m
[32m- StringToMap[0m
[32m- SPARK-22693: CreateNamedStruct should not use global variables[0m
[32mCaseInsensitiveStringMapSuite:[0m
[32m- put and get[0m
[32m- clear[0m
[32m- key and value set[0m
[32m- getInt[0m
[32m- getBoolean[0m
[32m- getLong[0m
[32m- getDouble[0m
[32m- asCaseSensitiveMap[0m
[32mCollapseWindowSuite:[0m
[32m- collapse two adjacent windows with the same partition/order[0m
[32m- Don't collapse adjacent windows with different partitions or orders[0m
[32m- Don't collapse adjacent windows with dependent columns[0m
[32m- Skip windows with empty window expressions[0m
[32mAnsiCastSuite:[0m
[32m- null cast[0m
[32m- cast string to date[0m
[32m- cast string to timestamp[0m
[32m- cast from boolean[0m
[32m- cast from float[0m
[32m- cast from double[0m
[32m- cast from string[0m
[32m- data type casting[0m
[32m- cast and add[0m
[32m- from decimal[0m
[32m- cast from date[0m
[32m- cast from timestamp[0m
[32m- cast from array[0m
[32m- cast from map[0m
[32m- cast from struct[0m
[32m- cast struct with a timestamp field[0m
[32m- complex casting[0m
[32m- cast between string and interval[0m
[32m- cast string to boolean[0m
[32m- SPARK-16729 type checking for casting to date type[0m
[32m- SPARK-20302 cast with same structure[0m
[32m- SPARK-22500: cast for struct should not generate codes beyond 64KB[0m
[32m- SPARK-22570: Cast should not create a lot of global variables[0m
[32m- SPARK-22825 Cast array to string[0m
[32m- SPARK-22973 Cast map to string[0m
[32m- SPARK-22981 Cast struct to string[0m
[32m- up-cast[0m
[32m- SPARK-27671: cast from nested null type in struct[0m
[32m- Throw exception on casting out-of-range value to decimal type[0m
[32m- Process Infinity, -Infinity, NaN in case insensitive manner[0m
[32m- Throw exception on casting out-of-range value to byte type[0m
[32m- Throw exception on casting out-of-range value to short type[0m
[32m- Throw exception on casting out-of-range value to int type[0m
[32m- Throw exception on casting out-of-range value to long type[0m
[32mCallMethodViaReflectionSuite:[0m
[32m- findMethod via reflection for static methods[0m
[32m- findMethod for a JDK library[0m
[32m- class not found[0m
[32m- method not found because name does not match[0m
[32m- method not found because there is no static method[0m
[32m- input type checking[0m
[32m- unsupported type checking[0m
[32m- invoking methods using acceptable types[0m
[32mPercentileSuite:[0m
[32m- serialize and de-serialize[0m
[32m- class Percentile, high level interface, update, merge, eval...[0m
[32m- class Percentile, low level interface, update, merge, eval...[0m
[32m- fail analysis if childExpression is invalid[0m
[32m- fails analysis if percentage(s) are invalid[0m
[32m- null handling[0m
[32m- negatives frequency column handling[0m
[32mExprIdSuite:[0m
[32m- hashcode independent of jvmId[0m
[32m- equality should depend on both id and jvmId[0m
[32mResolveGroupingAnalyticsSuite:[0m
[32m- rollupExprs[0m
[32m- cubeExprs[0m
[32m- grouping sets[0m
[32m- grouping sets with no explicit group by expressions[0m
[32m- cube[0m
[32m- rollup[0m
[32m- grouping function[0m
[32m- grouping_id[0m
[32m- filter with grouping function[0m
[32m- sort with grouping function[0m
[32mDecimalSuite:[0m
[32m- creating decimals[0m
[32m- creating decimals with negative scale[0m
[32m- double and long values[0m
[32m- small decimals represented as unscaled long[0m
[32m- hash code[0m
[32m- equals[0m
[32m- isZero[0m
[32m- arithmetic[0m
[32m- accurate precision after multiplication[0m
[32m- fix non-terminating decimal expansion problem[0m
[32m- fix loss of precision/scale when doing division operation[0m
[32m- set/setOrNull[0m
[32m- changePrecision/toPrecision on compact decimal should respect rounding mode[0m
[32m- SPARK-20341: support BigInt's value does not fit in long value range[0m
[32m- SPARK-26038: toScalaBigInt/toJavaBigInteger[0m
[32mArrayDataIndexedSeqSuite:[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BooleanType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BooleanType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BooleanType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BooleanType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ByteType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ByteType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ByteType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ByteType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ShortType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ShortType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(ShortType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(ShortType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(IntegerType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(IntegerType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(IntegerType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(IntegerType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(LongType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(LongType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(LongType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(LongType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(FloatType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(FloatType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(FloatType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(FloatType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DoubleType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DoubleType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DoubleType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DoubleType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DecimalType(10,0),false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DecimalType(10,0),false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DecimalType(10,0),true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DecimalType(10,0),true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(StringType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(StringType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(StringType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(StringType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BinaryType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BinaryType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(BinaryType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(BinaryType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DateType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DateType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(DateType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(DateType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(TimestampType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(TimestampType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(TimestampType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(TimestampType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(CalendarIntervalType,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(CalendarIntervalType,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(CalendarIntervalType,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(CalendarIntervalType,true)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@3c9e19de,false)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@3c9e19de,false)[0m
[32m- ArrayDataIndexedSeq - UnsafeArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@3c9e19de,true)[0m
[32m- ArrayDataIndexedSeq - GenericArrayData - ArrayType(org.apache.spark.sql.catalyst.encoders.ExamplePointUDT@3c9e19de,true)[0m
[32mNumberConverterSuite:[0m
[32m- convert[0m
[32m- byte to binary[0m
[32m- short to binary[0m
[32m- integer to binary[0m
[32m- long to binary[0m
[32mColumnPruningSuite:[0m
[32m- Column pruning for Generate when Generate.unrequiredChildIndex = child.output[0m
[32m- Fill Generate.unrequiredChildIndex if possible[0m
[32m- Another fill Generate.unrequiredChildIndex if possible[0m
[32m- Nested column pruning for Generate[0m
[32m- Column pruning for Project on Sort[0m
[32m- Column pruning for Expand[0m
[32m- Column pruning for ScriptTransformation[0m
[32m- Column pruning on Filter[0m
[32m- Column pruning on except/intersect/distinct[0m
[32m- Column pruning on Project[0m
[32m- Eliminate the Project with an empty projectList[0m
[32m- column pruning for group[0m
[32m- column pruning for group with alias[0m
[32m- column pruning for Project(ne, Limit)[0m
[32m- push down project past sort[0m
[32m- Column pruning on Window with useless aggregate functions[0m
[32m- Column pruning on Window with selected agg expressions[0m
[32m- Column pruning on Window in select[0m
[32m- Column pruning on Union[0m
[32m- Remove redundant projects in column pruning rule[0m
[32m- Column pruning on MapPartitions[0m
[32m- push project down into sample[0m
[32m- SPARK-24696 ColumnPruning rule fails to remove extra Project[0m
[32mOptimizerLoggingSuite:[0m
[32m- test log level[0m
[32m- test invalid log level conf[0m
[32m- test log rules[0m
[32m- test log batches which change the plan[0m
[32m- test log batches which do not change the plan[0m
[32mStringUtilsSuite:[0m
[32m- escapeLikeRegex[0m
[32m- filter pattern[0m
[32m- string concatenation[0m
[32m- string concatenation with limit[0m
[32m- string concatenation return value[0m
[32mPropagateEmptyRelationSuite:[0m
[32m- propagate empty relation through Union[0m
[32m- propagate empty relation through Join[0m
[32m- propagate empty relation through UnaryNode[0m
[32m- propagate empty streaming relation through multiple UnaryNode[0m
[32m- don't propagate empty streaming relation through agg[0m
[32m- don't propagate non-empty local relation[0m
[32m- propagate empty relation through Aggregate with grouping expressions[0m
[32m- don't propagate empty relation through Aggregate without grouping expressions[0m
[32m- propagate empty relation keeps the plan resolved[0m
[32mConstantPropagationSuite:[0m
[32m- basic test[0m
[32m- with combination of AND and OR predicates[0m
[32m- equality predicates outside a `NOT` can be propagated within a `NOT`[0m
[32m- equality predicates inside a `NOT` should not be picked for propagation[0m
[32m- equality predicates outside a `OR` can be propagated within a `OR`[0m
[32m- equality predicates inside a `OR` should not be picked for propagation[0m
[32m- equality operator not immediate child of root `AND` should not be used for propagation[0m
[32m- conflicting equality predicates[0m
[32mBinaryComparisonSimplificationSuite:[0m
[32m- Preserve nullable exprs in general[0m
[32m- Preserve non-deterministic exprs[0m
[32m- Nullable Simplification Primitive: <=>[0m
[32m- Non-Nullable Simplification Primitive[0m
[32m- Expression Normalization[0m
[32m- SPARK-26402: accessing nested fields with different cases in case insensitive mode[0m
[32mHyperLogLogPlusPlusSuite:[0m
[32m- test invalid parameter relativeSD[0m
[32m- add nulls[0m
[32m- deterministic cardinality estimation[0m
[32m- random cardinality estimation[0m
[32m- merging HLL instances[0m
[32mLastTestSuite:[0m
[32m- empty buffer[0m
[32m- update[0m
[32m- update - ignore nulls[0m
[32m- merge[0m
[32m- merge - ignore nulls[0m
[32m- eval[0m
[32m- eval - ignore nulls[0m
[32mQuantileSummariesSuite:[0m
[32m- Extremas with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Extremas with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=increasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Extremas with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Extremas with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=increasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Extremas with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Extremas with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=decreasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Extremas with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Extremas with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=decreasing, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Extremas with epsi=0.1 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=random, compression=1000[0m
[32m- Extremas with epsi=0.1 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=0.1 and seq=random, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=0.1 and seq=random, compression=10[0m
[32m- Extremas with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=1000 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Extremas with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Some quantile values with epsi=1.0E-4 and seq=random, compression=10 (interleaved)[0m
[32m- Tests on empty data with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Merging ordered lists with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Merging unbalanced interleaved lists with epsi=0.1 and seq=increasing, compression=1000[0m
[32m- Merging ordered lists with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Merging unbalanced interleaved lists with epsi=0.1 and seq=increasing, compression=10[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Merging unbalanced interleaved lists with epsi=1.0E-4 and seq=increasing, compression=1000[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Merging unbalanced interleaved lists with epsi=1.0E-4 and seq=increasing, compression=10[0m
[32m- Merging ordered lists with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Merging unbalanced interleaved lists with epsi=0.1 and seq=decreasing, compression=1000[0m
[32m- Merging ordered lists with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Merging unbalanced interleaved lists with epsi=0.1 and seq=decreasing, compression=10[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Merging unbalanced interleaved lists with epsi=1.0E-4 and seq=decreasing, compression=1000[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Merging unbalanced interleaved lists with epsi=1.0E-4 and seq=decreasing, compression=10[0m
[32m- Merging ordered lists with epsi=0.1 and seq=random, compression=1000[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=random, compression=1000[0m
[32m- Merging unbalanced interleaved lists with epsi=0.1 and seq=random, compression=1000[0m
[32m- Merging ordered lists with epsi=0.1 and seq=random, compression=10[0m
[32m- Merging interleaved lists with epsi=0.1 and seq=random, compression=10[0m
[32m- Merging unbalanced interleaved lists with epsi=0.1 and seq=random, compression=10[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Merging unbalanced interleaved lists with epsi=1.0E-4 and seq=random, compression=1000[0m
[32m- Merging ordered lists with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Merging interleaved lists with epsi=1.0E-4 and seq=random, compression=10[0m
[32m- Merging unbalanced interleaved lists with epsi=1.0E-4 and seq=random, compression=10[0m
[32mTransformExtractorSuite:[0m
[32m- Identity extractor[0m
[32m- Years extractor[0m
[32m- Months extractor[0m
[32m- Days extractor[0m
[32m- Hours extractor[0m
[32m- Bucket extractor[0m
[32mStringExpressionsSuite:[0m
[32m- concat[0m
[32m- SPARK-22498: Concat should not generate codes beyond 64KB[0m
[32m- SPARK-22771 Check Concat.checkInputDataTypes results[0m
[32m- concat_ws[0m
[32m- SPARK-22549: ConcatWs should not generate codes beyond 64KB[0m
[32m- elt[0m
[32m- SPARK-22550: Elt should not generate codes beyond 64KB[0m
[32m- StringComparison[0m
[32m- Substring[0m
[32m- string substring_index function[0m
[32m- ascii for string[0m
[32m- string for ascii[0m
[32m- base64/unbase64 for string[0m
[32m- encode/decode for string[0m
[32m- initcap unit test[0m
[32m- Levenshtein distance[0m
[32m- soundex unit test[0m
[32m- replace[0m
[32m- overlay for string[0m
[32m- overlay for byte array[0m
[32m- Check Overlay.checkInputDataTypes results[0m
[32m- translate[0m
[32m- TRIM[0m
[32m- LTRIM[0m
[32m- RTRIM[0m
[32m- FORMAT[0m
[32m- SPARK-22603: FormatString should not generate codes beyond 64KB[0m
[32m- INSTR[0m
[32m- LOCATE[0m
[32m- LPAD/RPAD[0m
[32m- REPEAT[0m
[32m- REVERSE[0m
[32m- SPACE[0m
[32m- length for string / binary[0m
[32m- format_number / FormatNumber[0m
[32m- find in set[0m
[32m- ParseUrl[0m
[32m- Sentences[0m
[32mEncoderErrorMessageSuite:[0m
[32m- primitive types in encoders using Kryo serialization[0m
[32m- primitive types in encoders using Java serialization[0m
[32m- nice error message for missing encoder[0m
[32mScanOperationSuite:[0m
[32m- Project with a non-deterministic field and a deterministic child Filter[0m
[32m- Project with all deterministic fields but a non-deterministic child Filter[0m
[32m- Project which has the same non-deterministic expression with its child Project[0m
[32m- Project which has different non-deterministic expressions with its child Project[0m
[32m- Filter which has the same non-deterministic expression with its child Project[0m
[32m- Deterministic filter with a child Project with a non-deterministic expression[0m
[32m- Filter which has different non-deterministic expressions with its child Project[0m
[32m- Deterministic filter which has a non-deterministic child Filter[0m
[32mOrderingSuite:[0m
[32m- compare two arrays: a = List(), b = List()[0m
[32m- compare two arrays: a = List(1), b = List(1)[0m
[32m- compare two arrays: a = List(1, 2), b = List(1, 2)[0m
[32m- compare two arrays: a = List(1, 2, 2), b = List(1, 2, 3)[0m
[32m- compare two arrays: a = List(), b = List(1)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, 4)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, 2)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 2, 2)[0m
[32m- compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, null)[0m
[32m- compare two arrays: a = List(), b = List(null)[0m
[32m- compare two arrays: a = List(null), b = List(null)[0m
[32m- compare two arrays: a = List(null, null), b = List(null, null)[0m
[32m- compare two arrays: a = List(null), b = List(null, null)[0m
[32m- compare two arrays: a = List(null), b = List(1)[0m
[32m- compare two arrays: a = List(null), b = List(null, 1)[0m
[32m- compare two arrays: a = List(null, 1), b = List(1, 1)[0m
[32m- compare two arrays: a = List(1, null, 1), b = List(1, null, 1)[0m
[32m- compare two arrays: a = List(1, null, 1), b = List(1, null, 2)[0m
[32m- GenerateOrdering with StringType[0m
[32m- GenerateOrdering with NullType[0m
[32m- GenerateOrdering with ArrayType(IntegerType,true)[0m
[32m- GenerateOrdering with LongType[0m
[32m- GenerateOrdering with IntegerType[0m
[32m- GenerateOrdering with DecimalType(20,5)[0m
[32m- GenerateOrdering with TimestampType[0m
[32m- GenerateOrdering with DoubleType[0m
[32m- GenerateOrdering with DateType[0m
[32m- GenerateOrdering with StructType(StructField(f1,FloatType,true), StructField(f2,ArrayType(BooleanType,true),true))[0m
[32m- GenerateOrdering with ArrayType(StructType(StructField(f1,FloatType,true), StructField(f2,ArrayType(BooleanType,true),true)),true)[0m
[32m- GenerateOrdering with DecimalType(10,0)[0m
[32m- GenerateOrdering with BinaryType[0m
[32m- GenerateOrdering with BooleanType[0m
[32m- GenerateOrdering with DecimalType(38,18)[0m
[32m- GenerateOrdering with ByteType[0m
[32m- GenerateOrdering with FloatType[0m
[32m- GenerateOrdering with ShortType[0m
[32m- SPARK-16845: GeneratedClass$SpecificOrdering grows beyond 64 KiB[0m
[32m- SPARK-21344: BinaryType comparison does signed byte array comparison[0m
[32m- SPARK-22591: GenerateOrdering shouldn't change ctx.INPUT_ROW[0m
[32mGenerateUnsafeProjectionSuite:[0m
[32m- Test unsafe projection string access pattern[0m
[32m- Test unsafe projection for array/map/struct[0m
[32mGeneratorExpressionSuite:[0m
[32m- explode[0m
[32m- posexplode[0m
[32m- inline[0m
[32m- stack[0m
[32mResolveNaturalJoinSuite:[0m
[32m- natural/using inner join[0m
[32m- natural/using left join[0m
[32m- natural/using right join[0m
[32m- natural/using full outer join[0m
[32m- natural/using inner join with no nullability[0m
[32m- natural/using left join with no nullability[0m
[32m- natural/using right join with no nullability[0m
[32m- natural/using full outer join with no nullability[0m
[32m- using unresolved attribute[0m
[32m- using join with a case sensitive analyzer[0m
[32m- using join on nested fields[0m
[32m- using join with a case insensitive analyzer[0m
[32mAnalysisErrorSuite:[0m
[32m- scalar subquery with 2 columns[0m
[32m- scalar subquery with no column[0m
[32m- single invalid type, single arg[0m
[32m- single invalid type, second arg[0m
[32m- multiple invalid type[0m
[32m- invalid window function[0m
[32m- distinct aggregate function in window[0m
[32m- distinct function[0m
[32m- distinct window function[0m
[32m- nested aggregate functions[0m
[32m- offset window function[0m
[32m- too many generators[0m
[32m- unresolved attributes[0m
[32m- unresolved attributes with a generated name[0m
[32m- unresolved star expansion in max[0m
[32m- sorting by unsupported column types[0m
[32m- sorting by attributes are not from grouping expressions[0m
[32m- non-boolean filters[0m
[32m- non-boolean join conditions[0m
[32m- missing group by[0m
[32m- ambiguous field[0m
[32m- ambiguous field due to case insensitivity[0m
[32m- missing field[0m
[32m- catch all unresolved plan[0m
[32m- union with unequal number of columns[0m
[32m- intersect with unequal number of columns[0m
[32m- except with unequal number of columns[0m
[32m- union with incompatible column types[0m
[32m- union with a incompatible column type and compatible column types[0m
[32m- intersect with incompatible column types[0m
[32m- intersect with a incompatible column type and compatible column types[0m
[32m- except with incompatible column types[0m
[32m- except with a incompatible column type and compatible column types[0m
[32m- SPARK-9955: correct error message for aggregate[0m
[32m- slide duration greater than window in time window[0m
[32m- start time greater than slide duration in time window[0m
[32m- start time equal to slide duration in time window[0m
[32m- SPARK-21590: absolute value of start time greater than slide duration in time window[0m
[32m- SPARK-21590: absolute value of start time equal to slide duration in time window[0m
[32m- negative window duration in time window[0m
[32m- zero window duration in time window[0m
[32m- negative slide duration in time window[0m
[32m- zero slide duration in time window[0m
[32m- generator nested in expressions[0m
[32m- generator appears in operator which is not Project[0m
[32m- an evaluated limit class must not be null[0m
[32m- num_rows in limit clause must be equal to or greater than 0[0m
[32m- more than one generators in SELECT[0m
[32m- SPARK-6452 regression test[0m
[32m- error test for self-join[0m
[32m- check grouping expression data types[0m
[32m- we should fail analysis when we find nested aggregate functions[0m
[32m- Join can work on binary types but can't work on map types[0m
[32m- PredicateSubQuery is used outside of a filter[0m
[32m- PredicateSubQuery is used is a nested condition[0m
[32m- PredicateSubQuery correlated predicate is nested in an illegal plan[0m
[32m- Error on filter condition containing aggregate expressions[0m
[32mTableIdentifierParserSuite:[0m
[32m- table identifier[0m
[32m- quoted identifiers[0m
[32m- table identifier - reserved/non-reserved keywords if ANSI mode enabled[0m
[32m- table identifier - strict keywords[0m
[32m- table identifier - non reserved keywords[0m
[32m- SPARK-17364 table identifier - contains number[0m
[32m- SPARK-17832 table identifier - contains backtick[0m
[32mJoinEstimationSuite:[0m
[32m- equi-height histograms: a bin is contained by another one[0m
[32m- equi-height histograms: a bin has only one value after trimming[0m
[32m- equi-height histograms: skew distribution (some bins have only one value)[0m
[32m- equi-height histograms: skew distribution (histograms have different skewed values[0m
[32m- equi-height histograms: skew distribution (both histograms have the same skewed value[0m
[32m- cross join[0m
[32m- disjoint inner join[0m
[32m- disjoint left outer join[0m
[32m- disjoint right outer join[0m
[32m- disjoint full outer join[0m
[32m- inner join[0m
[32m- inner join with multiple equi-join keys[0m
[32m- left outer join[0m
[32m- right outer join[0m
[32m- full outer join[0m
[32m- left semi/anti join[0m
[32m- test join keys of different types[0m
[32m- join with null column[0m
[32mBooleanSimplificationSuite:[0m
[32m- a && a => a[0m
[32m- a || a => a[0m
[32m- (a && b && c && ...) || (a && b && d && ...) || (a && b && e && ...) ...[0m
[32m- (a || b || c || ...) && (a || b || d || ...) && (a || b || e || ...) ...[0m
[32m- e && (!e || f) - not nullable[0m
[32m- e && (!e || f) - nullable[0m
[32m- a < 1 && (!(a < 1) || f) - not nullable[0m
[32m- a < 1 && ((a >= 1) || f) - not nullable[0m
[32m- DeMorgan's law[0m
[32m- (a && b) || (a && c) => a && (b || c) when case insensitive[0m
[32m- (a || b) && (a || c) => a || (b && c) when case insensitive[0m
[32m- Complementation Laws[0m
[32m- Complementation Laws - null handling[0m
[32m- Complementation Laws - negative case[0m
[32m- simplify NOT(IsNull(x)) and NOT(IsNotNull(x))[0m
[32m- filter reduction - positive cases[0m
[32mLikeSimplificationSuite:[0m
[32m- simplify Like into StartsWith[0m
[32m- simplify Like into EndsWith[0m
[32m- simplify Like into startsWith and EndsWith[0m
[32m- simplify Like into Contains[0m
[32m- simplify Like into EqualTo[0m
[32m- null pattern[0m
[32mRemoveRedundantAliasAndProjectSuite:[0m
[32m- all expressions in project list are aliased child output[0m
[32m- all expressions in project list are aliased child output but with different order[0m
[32m- some expressions in project list are aliased child output[0m
[32m- some expressions in project list are aliased child output but with different order[0m
[32m- some expressions in project list are not Alias or Attribute[0m
[32m- some expressions in project list are aliased child output but with metadata[0m
[32m- retain deduplicating alias in self-join[0m
[32m- alias removal should not break after push project through union[0m
[32m- remove redundant alias from aggregate[0m
[32m- remove redundant alias from window[0m
[32m- do not remove output attributes from a subquery[0m
[32mTypeUtilsSuite:[0m
[32m- checkForSameTypeInputExpr[0m
[32m- compareBinary[0m
[32mCaseInsensitiveMapSuite:[0m
[32m- Keys are case insensitive[0m
[32m- CaseInsensitiveMap should be serializable after '-' operator[0m
[32m- CaseInsensitiveMap should be serializable after '+' operator[0m
[32mScalaUDFSuite:[0m
[32m- basic[0m
[32m- better error message for NPE[0m
[32m- SPARK-22695: ScalaUDF should not use global variables[0m
[32m- SPARK-28369: honor nullOnOverflow config for ScalaUDF[0m
[32mStreamingJoinHelperSuite:[0m
[32m- extract watermark from time condition[0m
[32mUnsafeRowWriterSuite:[0m
[32m- SPARK-25538: zero-out all bits for decimals[0m
[32m- write and get calendar intervals through UnsafeRowWriter[0m
[32mV2AppendDataANSIAnalysisSuite:[0m
[32m- byName: basic behavior[0m
[32m- byName: does not match by position[0m
[32m- byName: case sensitive column resolution[0m
[32m- byName: case insensitive column resolution[0m
[32m- byName: data columns are reordered by name[0m
[32m- byName: fail nullable data written to required columns[0m
[32m- byName: allow required data written to nullable columns[0m
[32m- byName: missing required columns cause failure and are identified by name[0m
[32m- byName: missing optional columns cause failure and are identified by name[0m
[32m- byName: insert safe cast[0m
[32m- byName: fail extra data fields[0m
[32m- byPosition: basic behavior[0m
[32m- byPosition: data columns are not reordered[0m
[32m- byPosition: fail nullable data written to required columns[0m
[32m- byPosition: allow required data written to nullable columns[0m
[32m- byPosition: missing required columns cause failure[0m
[32m- byPosition: missing optional columns cause failure[0m
[32m- byPosition: insert safe cast[0m
[32m- byPosition: fail extra data fields[0m
[32m- bypass output column resolution[0m
[32m- check fields of struct type column[0m
[32mResolveLambdaVariablesSuite:[0m
[32m- resolution - no op[0m
[32m- resolution - simple[0m
[32m- resolution - nested[0m
[32m- resolution - hidden[0m
[32m- fail - name collisions[0m
[32m- fail - lambda arguments[0m
[32mOptimizeInSuite:[0m
[32m- OptimizedIn test: Remove deterministic repetitions[0m
[32m- OptimizedIn test: In clause not optimized to InSet when less than 10 items[0m
[32m- OptimizedIn test: In clause optimized to InSet when more than 10 items[0m
[32m- OptimizedIn test: In clause not optimized in case filter has attributes[0m
[32m- OptimizedIn test: NULL IN (expr1, ..., exprN) gets transformed to Filter(null)[0m
[32m- OptimizedIn test: NULL IN (subquery) gets transformed to Filter(null)[0m
[32m- OptimizedIn test: Inset optimization disabled as list expression contains attribute)[0m
[32m- OptimizedIn test: Inset optimization disabled as list expression contains attribute - select)[0m
[32m- OptimizedIn test: Setting the threshold for turning Set into InSet.[0m
[32m- OptimizedIn test: one element in list gets transformed to EqualTo.[0m
[32m- OptimizedIn test: In empty list gets transformed to FalseLiteral when value is not nullable[0m
[32m- OptimizedIn test: In empty list gets transformed to `If` expression when value is nullable[0m
[32mNormalizeFloatingPointNumbersSuite:[0m
[32m- normalize floating points in window function expressions[0m
[32m- normalize floating points in window function expressions - idempotence[0m
[32m- normalize floating points in join keys[0m
[32m- normalize floating points in join keys - idempotence[0m
[32m- normalize floating points in join keys (equal null safe) - idempotence[0m
[32mV2OverwritePartitionsDynamicStrictAnalysisSuite:[0m
[32m- byName: basic behavior[0m
[32m- byName: does not match by position[0m
[32m- byName: case sensitive column resolution[0m
[32m- byName: case insensitive column resolution[0m
[32m- byName: data columns are reordered by name[0m
[32m- byName: fail nullable data written to required columns[0m
[32m- byName: allow required data written to nullable columns[0m
[32m- byName: missing required columns cause failure and are identified by name[0m
[32m- byName: missing optional columns cause failure and are identified by name[0m
[32m- byName: insert safe cast[0m
[32m- byName: fail extra data fields[0m
[32m- byPosition: basic behavior[0m
[32m- byPosition: data columns are not reordered[0m
[32m- byPosition: fail nullable data written to required columns[0m
[32m- byPosition: allow required data written to nullable columns[0m
[32m- byPosition: missing required columns cause failure[0m
[32m- byPosition: missing optional columns cause failure[0m
[32m- byPosition: insert safe cast[0m
[32m- byPosition: fail extra data fields[0m
[32m- bypass output column resolution[0m
[32m- check fields of struct type column[0m
[32m- byName: fail canWrite check[0m
[32m- byName: multiple field errors are reported[0m
[32m- byPosition: fail canWrite check[0m
[32m- byPosition: multiple field errors are reported[0m
[32mSimplifyStringCaseConversionSuite:[0m
[32m- simplify UPPER(UPPER(str))[0m
[32m- simplify UPPER(LOWER(str))[0m
[32m- simplify LOWER(UPPER(str))[0m
[32m- simplify LOWER(LOWER(str))[0m
[32mSimplifyCastsSuite:[0m
[32m- non-nullable element array to nullable element array cast[0m
[32m- nullable element to non-nullable element array cast[0m
[32m- non-nullable value map to nullable value map cast[0m
[32m- nullable value map to non-nullable value map cast[0m
[32mEliminateSerializationSuite:[0m
[32m- back to back serialization[0m
[32m- back to back serialization with object change[0m
[32m- back to back serialization in AppendColumns[0m
[32m- back to back serialization in AppendColumns with object change[0m
[32mRowJsonSuite:[0m
[32m- IntegerType null[0m
[32m- FloatType null[0m
[32m- ArrayType(DoubleType,true) null[0m
[32m- BooleanType true[0m
[32m- BooleanType false[0m
[32m- ByteType 23[0m
[32m- ByteType -126[0m
[32m- ShortType 20281[0m
[32m- ShortType -8752[0m
[32m- IntegerType 1078231987[0m
[32m- IntegerType -10[0m
[32m- LongType 139289832109874199[0m
[32m- LongType -7873748239973488[0m
[32m- FloatType 1.02319997E11[0m
[32m- FloatType 9.7E-13[0m
[32m- DoubleType 3.891E98[0m
[32m- DoubleType -780000.0[0m
[32m- DecimalType(10,2) 1092.88[0m
[32m- DecimalType(7,4) 782.0003[0m
[32m- DecimalType(4,2) -77.89[0m
[32m- StringType hello world[0m
[32m- BinaryType[0m
[32m- DateType 2019-04-22[0m
[32m- DateType 2018-05-14[0m
[32m- TimestampType 2017-01-06 10:22:03.0[0m
[32m- TimestampType 2017-05-30T02:22:03Z[0m
[32m- ArrayType(LongType,true)[0m
[32m- ArrayType(IntegerType,false) List(1, -2, 3)[0m
[32m- MapType(StringType,StringType,true) Map(a -> b, c -> d, e -> null)[0m
[32m- MapType(IntegerType,StringType,true) Map(1 -> b, 2 -> d, 3 -> null)[0m
[32m- StructType(StructField(c1,StringType,true), StructField(c2,IntegerType,true)) [1,2][0m
[32m- UDT[0m
[32m- no schema[0m
[32m- unsupported type[0m
[32mComplexDataSuite:[0m
[32m- inequality tests for MapData[0m
[32m- GenericInternalRow.copy return a new instance that is independent from the old one[0m
[32m- SpecificMutableRow.copy return a new instance that is independent from the old one[0m
[32m- GenericArrayData.copy return a new instance that is independent from the old one[0m
[32m- copy on nested complex type[0m
[32m- SPARK-24659: GenericArrayData.equals should respect element type differences[0m
[32mRandomDataGeneratorSuite:[0m
[32m- StringType (nullable=true)[0m
[32m- StringType (nullable=false)[0m
[32m- LongType (nullable=true)[0m
[32m- LongType (nullable=false)[0m
[32m- IntegerType (nullable=true)[0m
[32m- IntegerType (nullable=false)[0m
[32m- TimestampType (nullable=true)[0m
[32m- TimestampType (nullable=false)[0m
[32m- DoubleType (nullable=true)[0m
[32m- DoubleType (nullable=false)[0m
[32m- DateType (nullable=true)[0m
[32m- DateType (nullable=false)[0m
[32m- BinaryType (nullable=true)[0m
[32m- BinaryType (nullable=false)[0m
[32m- BooleanType (nullable=true)[0m
[32m- BooleanType (nullable=false)[0m
[32m- ByteType (nullable=true)[0m
[32m- ByteType (nullable=false)[0m
[32m- FloatType (nullable=true)[0m
[32m- FloatType (nullable=false)[0m
[32m- ShortType (nullable=true)[0m
[32m- ShortType (nullable=false)[0m
[32m- ArrayType(FloatType,true)[0m
[32m- ArrayType(LongType,true)[0m
[32m- ArrayType(IntegerType,true)[0m
[32m- ArrayType(TimestampType,true)[0m
[32m- ArrayType(ByteType,true)[0m
[32m- ArrayType(ShortType,true)[0m
[32m- ArrayType(DecimalType(20,5),true)[0m
[32m- ArrayType(DateType,true)[0m
[32m- ArrayType(BooleanType,true)[0m
[32m- ArrayType(BinaryType,true)[0m
[32m- ArrayType(DecimalType(10,0),true)[0m
[32m- ArrayType(StringType,true)[0m
[32m- ArrayType(DoubleType,true)[0m
[32m- ArrayType(DecimalType(38,18),true)[0m
[32m- MapType(StringType,StringType,true)[0m
[32m- MapType(StringType,LongType,true)[0m
[32m- MapType(StringType,IntegerType,true)[0m
[32m- MapType(StringType,DecimalType(20,5),true)[0m
[32m- MapType(StringType,TimestampType,true)[0m
[32m- MapType(StringType,DoubleType,true)[0m
[32m- MapType(StringType,DateType,true)[0m
[32m- MapType(StringType,DecimalType(10,0),true)[0m
[32m- MapType(StringType,BinaryType,true)[0m
[32m- MapType(StringType,BooleanType,true)[0m
[32m- MapType(StringType,DecimalType(38,18),true)[0m
[32m- MapType(StringType,ByteType,true)[0m
[32m- MapType(StringType,FloatType,true)[0m
[32m- MapType(StringType,ShortType,true)[0m
[32m- MapType(LongType,StringType,true)[0m
[32m- MapType(LongType,LongType,true)[0m
[32m- MapType(LongType,IntegerType,true)[0m
[32m- MapType(LongType,DecimalType(20,5),true)[0m
[32m- MapType(LongType,TimestampType,true)[0m
[32m- MapType(LongType,DoubleType,true)[0m
[32m- MapType(LongType,DateType,true)[0m
[32m- MapType(LongType,DecimalType(10,0),true)[0m
[32m- MapType(LongType,BinaryType,true)[0m
[32m- MapType(LongType,BooleanType,true)[0m
[32m- MapType(LongType,DecimalType(38,18),true)[0m
[32m- MapType(LongType,ByteType,true)[0m
[32m- MapType(LongType,FloatType,true)[0m
[32m- MapType(LongType,ShortType,true)[0m
[32m- MapType(IntegerType,StringType,true)[0m
[32m- MapType(IntegerType,LongType,true)[0m
[32m- MapType(IntegerType,IntegerType,true)[0m
[32m- MapType(IntegerType,DecimalType(20,5),true)[0m
[32m- MapType(IntegerType,TimestampType,true)[0m
[32m- MapType(IntegerType,DoubleType,true)[0m
[32m- MapType(IntegerType,DateType,true)[0m
[32m- MapType(IntegerType,DecimalType(10,0),true)[0m
[32m- MapType(IntegerType,BinaryType,true)[0m
[32m- MapType(IntegerType,BooleanType,true)[0m
[32m- MapType(IntegerType,DecimalType(38,18),true)[0m
[32m- MapType(IntegerType,ByteType,true)[0m
[32m- MapType(IntegerType,FloatType,true)[0m
[32m- MapType(IntegerType,ShortType,true)[0m
[32m- MapType(TimestampType,StringType,true)[0m
[32m- MapType(TimestampType,LongType,true)[0m
[32m- MapType(TimestampType,IntegerType,true)[0m
[32m- MapType(TimestampType,DecimalType(20,5),true)[0m
[32m- MapType(TimestampType,TimestampType,true)[0m
[32m- MapType(TimestampType,DoubleType,true)[0m
[32m- MapType(TimestampType,DateType,true)[0m
[32m- MapType(TimestampType,DecimalType(10,0),true)[0m
[32m- MapType(TimestampType,BinaryType,true)[0m
[32m- MapType(TimestampType,BooleanType,true)[0m
[32m- MapType(TimestampType,DecimalType(38,18),true)[0m
[32m- MapType(TimestampType,ByteType,true)[0m
[32m- MapType(TimestampType,FloatType,true)[0m
[32m- MapType(TimestampType,ShortType,true)[0m
[32m- MapType(DoubleType,StringType,true)[0m
[32m- MapType(DoubleType,LongType,true)[0m
[32m- MapType(DoubleType,IntegerType,true)[0m
[32m- MapType(DoubleType,DecimalType(20,5),true)[0m
[32m- MapType(DoubleType,TimestampType,true)[0m
[32m- MapType(DoubleType,DoubleType,true)[0m
[32m- MapType(DoubleType,DateType,true)[0m
[32m- MapType(DoubleType,DecimalType(10,0),true)[0m
[32m- MapType(DoubleType,BinaryType,true)[0m
[32m- MapType(DoubleType,BooleanType,true)[0m
[32m- MapType(DoubleType,DecimalType(38,18),true)[0m
[32m- MapType(DoubleType,ByteType,true)[0m
[32m- MapType(DoubleType,FloatType,true)[0m
[32m- MapType(DoubleType,ShortType,true)[0m
[32m- MapType(DateType,StringType,true)[0m
[32m- MapType(DateType,LongType,true)[0m
[32m- MapType(DateType,IntegerType,true)[0m
[32m- MapType(DateType,DecimalType(20,5),true)[0m
[32m- MapType(DateType,TimestampType,true)[0m
[32m- MapType(DateType,DoubleType,true)[0m
[32m- MapType(DateType,DateType,true)[0m
[32m- MapType(DateType,DecimalType(10,0),true)[0m
[32m- MapType(DateType,BinaryType,true)[0m
[32m- MapType(DateType,BooleanType,true)[0m
[32m- MapType(DateType,DecimalType(38,18),true)[0m
[32m- MapType(DateType,ByteType,true)[0m
[32m- MapType(DateType,FloatType,true)[0m
[32m- MapType(DateType,ShortType,true)[0m
[32m- MapType(BinaryType,StringType,true)[0m
[32m- MapType(BinaryType,LongType,true)[0m
[32m- MapType(BinaryType,IntegerType,true)[0m
[32m- MapType(BinaryType,DecimalType(20,5),true)[0m
[32m- MapType(BinaryType,TimestampType,true)[0m
[32m- MapType(BinaryType,DoubleType,true)[0m
[32m- MapType(BinaryType,DateType,true)[0m
[32m- MapType(BinaryType,DecimalType(10,0),true)[0m
[32m- MapType(BinaryType,BinaryType,true)[0m
[32m- MapType(BinaryType,BooleanType,true)[0m
[32m- MapType(BinaryType,DecimalType(38,18),true)[0m
[32m- MapType(BinaryType,ByteType,true)[0m
[32m- MapType(BinaryType,FloatType,true)[0m
[32m- MapType(BinaryType,ShortType,true)[0m
[32m- MapType(BooleanType,StringType,true)[0m
[32m- MapType(BooleanType,LongType,true)[0m
[32m- MapType(BooleanType,IntegerType,true)[0m
[32m- MapType(BooleanType,DecimalType(20,5),true)[0m
[32m- MapType(BooleanType,TimestampType,true)[0m
[32m- MapType(BooleanType,DoubleType,true)[0m
[32m- MapType(BooleanType,DateType,true)[0m
[32m- MapType(BooleanType,DecimalType(10,0),true)[0m
[32m- MapType(BooleanType,BinaryType,true)[0m
[32m- MapType(BooleanType,BooleanType,true)[0m
[32m- MapType(BooleanType,DecimalType(38,18),true)[0m
[32m- MapType(BooleanType,ByteType,true)[0m
[32m- MapType(BooleanType,FloatType,true)[0m
[32m- MapType(BooleanType,ShortType,true)[0m
[32m- MapType(ByteType,StringType,true)[0m
[32m- MapType(ByteType,LongType,true)[0m
[32m- MapType(ByteType,IntegerType,true)[0m
[32m- MapType(ByteType,DecimalType(20,5),true)[0m
[32m- MapType(ByteType,TimestampType,true)[0m
[32m- MapType(ByteType,DoubleType,true)[0m
[32m- MapType(ByteType,DateType,true)[0m
[32m- MapType(ByteType,DecimalType(10,0),true)[0m
[32m- MapType(ByteType,BinaryType,true)[0m
[32m- MapType(ByteType,BooleanType,true)[0m
[32m- MapType(ByteType,DecimalType(38,18),true)[0m
[32m- MapType(ByteType,ByteType,true)[0m
[32m- MapType(ByteType,FloatType,true)[0m
[32m- MapType(ByteType,ShortType,true)[0m
[32m- MapType(FloatType,StringType,true)[0m
[32m- MapType(FloatType,LongType,true)[0m
[32m- MapType(FloatType,IntegerType,true)[0m
[32m- MapType(FloatType,DecimalType(20,5),true)[0m
[32m- MapType(FloatType,TimestampType,true)[0m
[32m- MapType(FloatType,DoubleType,true)[0m
[32m- MapType(FloatType,DateType,true)[0m
[32m- MapType(FloatType,DecimalType(10,0),true)[0m
[32m- MapType(FloatType,BinaryType,true)[0m
[32m- MapType(FloatType,BooleanType,true)[0m
[32m- MapType(FloatType,DecimalType(38,18),true)[0m
[32m- MapType(FloatType,ByteType,true)[0m
[32m- MapType(FloatType,FloatType,true)[0m
[32m- MapType(FloatType,ShortType,true)[0m
[32m- MapType(ShortType,StringType,true)[0m
[32m- MapType(ShortType,LongType,true)[0m
[32m- MapType(ShortType,IntegerType,true)[0m
[32m- MapType(ShortType,DecimalType(20,5),true)[0m
[32m- MapType(ShortType,TimestampType,true)[0m
[32m- MapType(ShortType,DoubleType,true)[0m
[32m- MapType(ShortType,DateType,true)[0m
[32m- MapType(ShortType,DecimalType(10,0),true)[0m
[32m- MapType(ShortType,BinaryType,true)[0m
[32m- MapType(ShortType,BooleanType,true)[0m
[32m- MapType(ShortType,DecimalType(38,18),true)[0m
[32m- MapType(ShortType,ByteType,true)[0m
[32m- MapType(ShortType,FloatType,true)[0m
[32m- MapType(ShortType,ShortType,true)[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,StringType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,LongType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,IntegerType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DecimalType(20,5),true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,TimestampType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DoubleType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DateType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DecimalType(10,0),true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,BinaryType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,BooleanType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,DecimalType(38,18),true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,ByteType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,FloatType,true), StructField(b,ShortType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,StringType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,LongType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,IntegerType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DecimalType(20,5),true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,TimestampType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DoubleType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DateType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DecimalType(10,0),true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,BinaryType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,BooleanType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,DecimalType(38,18),true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,ByteType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,FloatType,true))[0m
[32m- StructType(StructField(a,ShortType,true), StructField(b,ShortType,true))[0m
[32m- check size of generated map[0m
[32m- Use Float.NaN for all NaN values[0m
[32m- Use Double.NaN for all NaN values[0m
[32mOptimizeLimitZeroSuite:[0m
[32m- Limit 0: return empty local relation[0m
[32m- Limit 0: individual LocalLimit 0 node[0m
[32m- Limit 0: individual GlobalLimit 0 node[0m
[32m- Limit 0: for join type Inner[0m
[32m- Limit 0: for join type LeftOuter[0m
[32m- Limit 0: for join type RightOuter[0m
[32m- Limit 0: for join type FullOuter[0m
[32m- Limit 0: 3-way join[0m
[32m- Limit 0: intersect[0m
[32mSelectedFieldSuite:[0m
[32m- SelectedField should not match an attribute reference[0m
[32m- SELECT "col2.field2", "col2.field2[0] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field2: array (nullable = true)[0m
[32m    |    |    |-- element: integer (containsNull = false)[0m

[32m- SELECT "col2.field9", "col2.field9['foo'] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field9: map (nullable = true)[0m
[32m    |    |    |-- key: string[0m
[32m    |    |    |-- value: integer (valueContainsNull = false)[0m

[32m- SELECT "col2.field3.subfield3", "col2.field3[0].subfield3 as foo", "col2.field3.subfield3[0] as foo", "col2.field3[0].subfield3[0] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field3: array (nullable = false)[0m
[32m    |    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |    |-- subfield3: array (nullable = true)[0m
[32m    |    |    |    |    |-- element: integer (containsNull = true)[0m

[32m- SELECT "col2.field3.subfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field3: array (nullable = false)[0m
[32m    |    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |    |-- subfield1: integer (nullable = true)[0m

[32m- SELECT "col2.field4['foo'].subfield1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field4: map (nullable = true)[0m
[32m    |    |    |-- key: string[0m
[32m    |    |    |-- value: struct (valueContainsNull = false)[0m
[32m    |    |    |    |-- subfield1: integer (nullable = true)[0m

[32m- SELECT "col2.field4['foo'].subfield2 as foo", "col2.field4['foo'].subfield2[0] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field4: map (nullable = true)[0m
[32m    |    |    |-- key: string[0m
[32m    |    |    |-- value: struct (valueContainsNull = false)[0m
[32m    |    |    |    |-- subfield2: array (nullable = true)[0m
[32m    |    |    |    |    |-- element: integer (containsNull = false)[0m

[32m- SELECT "col2.field5.subfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field5: array (nullable = false)[0m
[32m    |    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |    |-- subfield1: struct (nullable = false)[0m
[32m    |    |    |    |    |-- subsubfield1: integer (nullable = true)[0m
[32m    |    |    |    |    |-- subsubfield2: integer (nullable = true)[0m

[32m- SELECT "col2.field5.subfield1.subsubfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field5: array (nullable = false)[0m
[32m    |    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |    |-- subfield1: struct (nullable = false)[0m
[32m    |    |    |    |    |-- subsubfield1: integer (nullable = true)[0m

[32m- SELECT "col2.field5.subfield2.subsubfield1.subsubsubfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field5: array (nullable = false)[0m
[32m    |    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |    |-- subfield2: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subsubfield1: struct (nullable = true)[0m
[32m    |    |    |    |    |    |-- subsubsubfield1: string (nullable = true)[0m

[32m- SELECT "col2.field8['foo'][0].subfield1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field8: map (nullable = true)[0m
[32m    |    |    |-- key: string[0m
[32m    |    |    |-- value: array (valueContainsNull = false)[0m
[32m    |    |    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = true)[0m

[32m- SELECT "col2.field1" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field1: integer (nullable = true)[0m

[32m- SELECT "col2.field6" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field6: struct (nullable = true)[0m
[32m    |    |    |-- subfield1: string (nullable = false)[0m
[32m    |    |    |-- subfield2: string (nullable = true)[0m

[32m- SELECT "col2.field6.subfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field6: struct (nullable = true)[0m
[32m    |    |    |-- subfield1: string (nullable = false)[0m

[32m- SELECT "col2.field7.subfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col2: struct (nullable = true)[0m
[32m    |    |-- field7: struct (nullable = true)[0m
[32m    |    |    |-- subfield1: struct (nullable = true)[0m
[32m    |    |    |    |-- subsubfield1: integer (nullable = true)[0m
[32m    |    |    |    |-- subsubfield2: integer (nullable = true)[0m

[32m- SELECT "col3.field1.subfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col3: array (nullable = false)[0m
[32m    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |-- field1: struct (nullable = true)[0m
[32m    |    |    |    |-- subfield1: integer (nullable = false)[0m

[32m- SELECT "col3.field2['foo'] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col3: array (nullable = false)[0m
[32m    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |-- field2: map (nullable = true)[0m
[32m    |    |    |    |-- key: string[0m
[32m    |    |    |    |-- value: integer (valueContainsNull = false)[0m

[32m- SELECT "col4['foo'].field1.subfield1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col4: map (nullable = false)[0m
[32m    |    |-- key: string[0m
[32m    |    |-- value: struct (valueContainsNull = false)[0m
[32m    |    |    |-- field1: struct (nullable = true)[0m
[32m    |    |    |    |-- subfield1: integer (nullable = false)[0m

[32m- SELECT "col4['foo'].field2['bar'] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col4: map (nullable = false)[0m
[32m    |    |-- key: string[0m
[32m    |    |-- value: struct (valueContainsNull = false)[0m
[32m    |    |    |-- field2: map (nullable = true)[0m
[32m    |    |    |    |-- key: string[0m
[32m    |    |    |    |-- value: integer (valueContainsNull = false)[0m

[32m- SELECT "col5[0]['foo'].field1.subfield1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col5: array (nullable = true)[0m
[32m    |    |-- element: map (containsNull = true)[0m
[32m    |    |    |-- key: string[0m
[32m    |    |    |-- value: struct (valueContainsNull = false)[0m
[32m    |    |    |    |-- field1: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = true)[0m

[32m- SELECT "map_values(col5[0]).field1.subfield1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col5: array (nullable = true)[0m
[32m    |    |-- element: map (containsNull = true)[0m
[32m    |    |    |-- key: string[0m
[32m    |    |    |-- value: struct (valueContainsNull = false)[0m
[32m    |    |    |    |-- field1: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = true)[0m

[32m- SELECT "map_values(col5[0]).field1.subfield2 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col5: array (nullable = true)[0m
[32m    |    |-- element: map (containsNull = true)[0m
[32m    |    |    |-- key: string[0m
[32m    |    |    |-- value: struct (valueContainsNull = false)[0m
[32m    |    |    |    |-- field1: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield2: integer (nullable = true)[0m

[32m- SELECT "col6['foo'][0].field1.subfield1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col6: map (nullable = true)[0m
[32m    |    |-- key: string[0m
[32m    |    |-- value: array (valueContainsNull = true)[0m
[32m    |    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |    |-- field1: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = true)[0m

[32m- SELECT "col7.field1", "col7[0].field1 as foo", "col7.field1[0] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col7: array (nullable = true)[0m
[32m    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |-- field1: integer (nullable = false)[0m

[32m- SELECT "col7.field2.subfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col7: array (nullable = true)[0m
[32m    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |-- field2: struct (nullable = true)[0m
[32m    |    |    |    |-- subfield1: integer (nullable = false)[0m

[32m- SELECT "col7.field3.subfield1" should select the schema[0m
[32m   root[0m
[32m    |-- col7: array (nullable = true)[0m
[32m    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |-- field3: array (nullable = true)[0m
[32m    |    |    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = false)[0m

[32m- SELECT "col8.field1", "col8[0].field1 as foo", "col8.field1[0] as foo", "col8[0].field1[0] as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col8: array (nullable = true)[0m
[32m    |    |-- element: struct (containsNull = true)[0m
[32m    |    |    |-- field1: array (nullable = false)[0m
[32m    |    |    |    |-- element: integer (containsNull = false)[0m

[32m- SELECT "map_keys(col2).field1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: map (nullable = true)[0m
[32m    |    |-- key: struct[0m
[32m    |    |    |-- field1: string (nullable = true)[0m
[32m    |    |-- value: array (valueContainsNull = true)[0m
[32m    |    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |    |-- field3: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = true)[0m
[32m    |    |    |    |    |-- subfield2: integer (nullable = true)[0m

[32m- SELECT "map_keys(col2).field2 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: map (nullable = true)[0m
[32m    |    |-- key: struct[0m
[32m    |    |    |-- field2: integer (nullable = true)[0m
[32m    |    |-- value: array (valueContainsNull = true)[0m
[32m    |    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |    |-- field3: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = true)[0m
[32m    |    |    |    |    |-- subfield2: integer (nullable = true)[0m

[32m- SELECT "map_keys(col2)[0].field1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: map (nullable = true)[0m
[32m    |    |-- key: array[0m
[32m    |    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |    |-- field1: string (nullable = true)[0m
[32m    |    |-- value: array (valueContainsNull = true)[0m
[32m    |    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |    |-- field3: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield3: integer (nullable = true)[0m
[32m    |    |    |    |    |-- subfield4: integer (nullable = true)[0m

[32m- SELECT "map_keys(col2)[0].field2.subfield1 as foo" should select the schema[0m
[32m   root[0m
[32m    |-- col2: map (nullable = true)[0m
[32m    |    |-- key: array[0m
[32m    |    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |    |-- field2: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield1: integer (nullable = true)[0m
[32m    |    |-- value: array (valueContainsNull = true)[0m
[32m    |    |    |-- element: struct (containsNull = false)[0m
[32m    |    |    |    |-- field3: struct (nullable = true)[0m
[32m    |    |    |    |    |-- subfield3: integer (nullable = true)[0m
[32m    |    |    |    |    |-- subfield4: integer (nullable = true)[0m

[32mDecimalPrecisionSuite:[0m
[32m- basic operations[0m
[32m- Comparison operations[0m
[32m- decimal precision for union[0m
[32m- bringing in primitive types[0m
[32m- maximum decimals[0m
[32m- DecimalType.isWiderThan[0m
[32m- strength reduction for integer/decimal comparisons - basic test[0m
[32m- strength reduction for integer/decimal comparisons - overflow test[0m
[32m- SPARK-24468: operations on decimals with negative scale[0m
[32mUnsafeArraySuite:[0m
[32m- read array[0m
[32m- from primitive array[0m
[32m- to primitive array[0m
[32m- unsafe java serialization[0m
[32m- unsafe Kryo serialization[0m
[32mTimestampFormatterSuite:[0m
[32m- parsing timestamps using time zones[0m
[32m- format timestamps using time zones[0m
[32m- roundtrip micros -> timestamp -> micros using timezones[0m
[32m- roundtrip timestamp -> micros -> timestamp using timezones[0m
[32m-  case insensitive parsing of am and pm[0m
[32m- format fraction of second[0m
[32m- formatting negative years with default pattern[0m
[32m- special timestamp values[0m
[32m- parsing timestamp strings with various seconds fractions[0m
[32m- formatting timestamp strings up to microsecond precision[0m
[32mExternalCatalogEventSuite:[0m
[32m- database[0m
[32m- table[0m
[32m- function[0m
[32mExpressionTypeCheckingSuite:[0m
[32m- check types for unary arithmetic[0m
[32m- check types for binary arithmetic[0m
[32m- check types for predicates[0m
[32m- check types for aggregates[0m
[32m- check types for others[0m
[32m- check types for CreateNamedStruct[0m
[32m- check types for CreateMap[0m
[32m- check types for ROUND/BROUND[0m
[32m- check types for Greatest/Least[0m
[32mInternalOutputModesSuite:[0m
[32m- supported strings[0m
[32m- unsupported strings[0m
[32mEliminateSubqueryAliasesSuite:[0m
[32m- eliminate top level subquery[0m
[32m- eliminate mid-tree subquery[0m
[32m- eliminate multiple subqueries[0m
[32mJoinTypesTest:[0m
[32m- construct an Inner type[0m
[32m- construct a FullOuter type[0m
[32m- construct a LeftOuter type[0m
[32m- construct a RightOuter type[0m
[32m- construct a LeftSemi type[0m
[32m- construct a LeftAnti type[0m
[32m- construct a Cross type[0m
[32mLeftSemiPushdownSuite:[0m
[32m- Project: LeftSemiAnti join pushdown[0m
[32m- Project: LeftSemiAnti join no pushdown because of non-deterministic proj exprs[0m
[32m- Project: LeftSemiAnti join non correlated scalar subq[0m
[32m- Project: LeftSemiAnti join no pushdown - correlated scalar subq in projection list[0m
[32m- Aggregate: LeftSemiAnti join pushdown[0m
[32m- Aggregate: LeftSemiAnti join no pushdown due to non-deterministic aggr expressions[0m
[32m- Aggregate: LeftSemi join partial pushdown[0m
[32m- Aggregate: LeftAnti join no pushdown[0m
[32m- LeftSemiAnti join over aggregate - no pushdown[0m
[32m- Aggregate: LeftSemiAnti join non-correlated scalar subq aggr exprs[0m
[32m- LeftSemiAnti join over Window[0m
[32m- Window: LeftSemi partial pushdown[0m
[32m- Window: LeftAnti no pushdown[0m
[32m- Union: LeftSemiAnti join pushdown[0m
[32m- Union: LeftSemiAnti join no pushdown in self join scenario[0m
[32m- Unary: LeftSemiAnti join pushdown[0m
[32m- Unary: LeftSemiAnti join pushdown - empty join condition[0m
[32m- Unary: LeftSemi join pushdown - partial pushdown[0m
[32m- Unary: LeftAnti join pushdown - no pushdown[0m
[32m- Unary: LeftSemiAnti join pushdown - no pushdown[0m
[32m- LeftSemi pushdown empty join cond join type Inner join cond Some(('d = 'e))[0m
[32m- LeftSemi pushdown empty join cond join type LeftOuter join cond Some(('d = 'e))[0m
[32m- LeftSemi pushdown empty join cond join type Cross join cond Some(('d = 'e))[0m
[32m- LeftSemi pushdown empty join cond join type RightOuter join cond Some(('d = 'e))[0m
[32m- LeftAnti pushdown empty join cond join type Inner join cond Some(('d = 'e))[0m
[32m- LeftAnti pushdown empty join cond join type LeftOuter join cond Some(('d = 'e))[0m
[32m- LeftAnti pushdown empty join cond join type Cross join cond Some(('d = 'e))[0m
[32m- LeftAnti pushdown empty join cond join type RightOuter join cond Some(('d = 'e))[0m
[32m- LeftSemi pushdown empty join cond join type Inner join cond None[0m
[32m- LeftSemi pushdown empty join cond join type LeftOuter join cond None[0m
[32m- LeftSemi pushdown empty join cond join type Cross join cond None[0m
[32m- LeftSemi pushdown empty join cond join type RightOuter join cond None[0m
[32m- LeftAnti pushdown empty join cond join type Inner join cond None[0m
[32m- LeftAnti pushdown empty join cond join type LeftOuter join cond None[0m
[32m- LeftAnti pushdown empty join cond join type Cross join cond None[0m
[32m- LeftAnti pushdown empty join cond join type RightOuter join cond None[0m
[32m- LeftSemi pushdown to left of join type: Inner join condition Some(('d = 'e))[0m
[32m- LeftSemi pushdown to left of join type: LeftOuter join condition Some(('d = 'e))[0m
[32m- LeftSemi pushdown to left of join type: Cross join condition Some(('d = 'e))[0m
[32m- LeftAnti pushdown to left of join type: Inner join condition Some(('d = 'e))[0m
[32m- LeftAnti pushdown to left of join type: LeftOuter join condition Some(('d = 'e))[0m
[32m- LeftAnti pushdown to left of join type: Cross join condition Some(('d = 'e))[0m
[32m- LeftSemi pushdown to left of join type: Inner join condition None[0m
[32m- LeftSemi pushdown to left of join type: LeftOuter join condition None[0m
[32m- LeftSemi pushdown to left of join type: Cross join condition None[0m
[32m- LeftAnti pushdown to left of join type: Inner join condition None[0m
[32m- LeftAnti pushdown to left of join type: LeftOuter join condition None[0m
[32m- LeftAnti pushdown to left of join type: Cross join condition None[0m
[32m- LeftSemi pushdown to right of join type: Inner join condition Some(('e = 'd))[0m
[32m- LeftSemi pushdown to right of join type: RightOuter join condition Some(('e = 'd))[0m
[32m- LeftSemi pushdown to right of join type: Cross join condition Some(('e = 'd))[0m
[32m- LeftAnti pushdown to right of join type: Inner join condition Some(('e = 'd))[0m
[32m- LeftAnti pushdown to right of join type: RightOuter join condition Some(('e = 'd))[0m
[32m- LeftAnti pushdown to right of join type: Cross join condition Some(('e = 'd))[0m
[32m- LeftSemi pushdown to right of join type: Inner join condition None[0m
[32m- LeftSemi pushdown to right of join type: RightOuter join condition None[0m
[32m- LeftSemi pushdown to right of join type: Cross join condition None[0m
[32m- LeftAnti pushdown to right of join type: Inner join condition None[0m
[32m- LeftAnti pushdown to right of join type: RightOuter join condition None[0m
[32m- LeftAnti pushdown to right of join type: Cross join condition None[0m
[32m- LeftSemi no pushdown - join condition refers left leg - join type for RightOuter[0m
[32m- LeftAnti no pushdown - join condition refers left leg - join type for RightOuter[0m
[32m- LeftSemi no pushdown - join condition refers right leg - join type for LeftOuter[0m
[32m- LeftAnti no pushdown - join condition refers right leg - join type for LeftOuter[0m
[32m- LeftSemi no pushdown - join condition refers both leg - join type Inner[0m
[32m- LeftSemi no pushdown - join condition refers both leg - join type LeftOuter[0m
[32m- LeftSemi no pushdown - join condition refers both leg - join type RightOuter[0m
[32m- LeftSemi no pushdown - join condition refers both leg - join type Cross[0m
[32m- LeftAnti no pushdown - join condition refers both leg - join type Inner[0m
[32m- LeftAnti no pushdown - join condition refers both leg - join type LeftOuter[0m
[32m- LeftAnti no pushdown - join condition refers both leg - join type RightOuter[0m
[32m- LeftAnti no pushdown - join condition refers both leg - join type Cross[0m
[32m- LeftSemi no pushdown - join condition refers none of the leg - join type Inner[0m
[32m- LeftSemi no pushdown - join condition refers none of the leg - join type LeftOuter[0m
[32m- LeftSemi no pushdown - join condition refers none of the leg - join type RightOuter[0m
[32m- LeftSemi no pushdown - join condition refers none of the leg - join type Cross[0m
[32m- LeftAnti no pushdown - join condition refers none of the leg - join type Inner[0m
[32m- LeftAnti no pushdown - join condition refers none of the leg - join type LeftOuter[0m
[32m- LeftAnti no pushdown - join condition refers none of the leg - join type RightOuter[0m
[32m- LeftAnti no pushdown - join condition refers none of the leg - join type Cross[0m
[32m- LeftSemi no pushdown when child join type is FullOuter[0m
[32m- LeftAnti no pushdown when child join type is FullOuter[0m
[32mDateExpressionsSuite:[0m
[32m- datetime function current_date[0m
[32m- datetime function current_timestamp[0m
[32m- DayOfYear[0m
[32m- Year[0m
[32m- Quarter[0m
[32m- Month[0m
[32m- Day / DayOfMonth[0m
[32m- Seconds[0m
[32m- DayOfWeek[0m
[32m- WeekDay[0m
[32m- WeekOfYear[0m
[32m- DateFormat[0m
[32m- Hour[0m
[32m- Minute[0m
[32m- date_add[0m
[32m- date_sub[0m
[32m- time_add[0m
[32m- time_sub[0m
[32m- add_months[0m
[32m- months_between[0m
[32m- last_day[0m
[32m- next_day[0m
[32m- TruncDate[0m
[32m- TruncTimestamp[0m
[32m- from_unixtime[0m
[32m- unix_timestamp[0m
[32m- to_unix_timestamp[0m
[32m- datediff[0m
[32m- to_utc_timestamp[0m
[32m- to_utc_timestamp - invalid time zone id[0m
[32m- from_utc_timestamp[0m
[32m- from_utc_timestamp - invalid time zone id[0m
[32m- creating values of DateType via make_date[0m
[32m- creating values of TimestampType via make_timestamp[0m
[32m- millennium[0m
[32m- century[0m
[32m- decade[0m
[32m- milliseconds and microseconds[0m
[32m- epoch[0m
[32m- ISO 8601 week-numbering year[0m
[32m- extract the seconds part with fraction from timestamps[0m
[32m- timestamps difference[0m
[32m- subtract dates[0m
[32mReplaceNullWithFalseInPredicateSuite:[0m
[32m- replace null inside filter and join conditions[0m
[32m- Not expected type - replaceNullWithFalse[0m
[32m- replace null in branches of If[0m
[32m- replace nulls in nested expressions in branches of If[0m
[32m- replace null in elseValue of CaseWhen[0m
[32m- replace null in branch values of CaseWhen[0m
[32m- replace null in branches of If inside CaseWhen[0m
[32m- replace null in complex CaseWhen expressions[0m
[32m- replace null in Or[0m
[32m- replace null in And[0m
[32m- replace nulls in nested And/Or expressions[0m
[32m- replace null in And inside branches of If[0m
[32m- replace null in branches of If inside And[0m
[32m- replace null in branches of If inside another If[0m
[32m- replace null in CaseWhen inside another CaseWhen[0m
[32m- inability to replace null in non-boolean branches of If[0m
[32m- inability to replace null in non-boolean values of CaseWhen[0m
[32m- inability to replace null in non-boolean branches of If inside another If[0m
[32m- replace null in If used as a join condition[0m
[32m- replace null in CaseWhen used as a join condition[0m
[32m- inability to replace null in CaseWhen inside EqualTo used as a join condition[0m
[32m- replace null in predicates of If[0m
[32m- replace null in predicates of If inside another If[0m
[32m- inability to replace null in non-boolean expressions inside If predicates[0m
[32m- replace null in conditions of CaseWhen[0m
[32m- replace null in conditions of CaseWhen inside another CaseWhen[0m
[32m- inability to replace null in non-boolean exprs inside CaseWhen conditions[0m
[32m- replace nulls in lambda function of ArrayFilter[0m
[32m- replace nulls in lambda function of ArrayExists[0m
[32m- replace nulls in lambda function of MapFilter[0m
[32m- inability to replace nulls in arbitrary higher-order function[0m
[32mDecimalAggregatesSuite:[0m
[32m- Decimal Sum Aggregation: Optimized[0m
[32m- Decimal Sum Aggregation: Not Optimized[0m
[32m- Decimal Average Aggregation: Optimized[0m
[32m- Decimal Average Aggregation: Not Optimized[0m
[32m- Decimal Sum Aggregation over Window: Optimized[0m
[32m- Decimal Sum Aggregation over Window: Not Optimized[0m
[32m- Decimal Average Aggregation over Window: Optimized[0m
[32m- Decimal Average Aggregation over Window: Not Optimized[0m
[32mDSLHintSuite:[0m
[32m- various hint parameters[0m
[32mExpressionEncoderSuite:[0m
[32m- encode/decode for primitive boolean: false (codegen path)[0m
[32m- encode/decode for primitive boolean: false (interpreted path)[0m
[32m- encode/decode for primitive byte: -3 (codegen path)[0m
[32m- encode/decode for primitive byte: -3 (interpreted path)[0m
[32m- encode/decode for primitive short: -3 (codegen path)[0m
[32m- encode/decode for primitive short: -3 (interpreted path)[0m
[32m- encode/decode for primitive int: -3 (codegen path)[0m
[32m- encode/decode for primitive int: -3 (interpreted path)[0m
[32m- encode/decode for primitive long: -3 (codegen path)[0m
[32m- encode/decode for primitive long: -3 (interpreted path)[0m
[32m- encode/decode for primitive float: -3.7 (codegen path)[0m
[32m- encode/decode for primitive float: -3.7 (interpreted path)[0m
[32m- encode/decode for primitive double: -3.7 (codegen path)[0m
[32m- encode/decode for primitive double: -3.7 (interpreted path)[0m
[32m- encode/decode for boxed boolean: false (codegen path)[0m
[32m- encode/decode for boxed boolean: false (interpreted path)[0m
[32m- encode/decode for boxed byte: -3 (codegen path)[0m
[32m- encode/decode for boxed byte: -3 (interpreted path)[0m
[32m- encode/decode for boxed short: -3 (codegen path)[0m
[32m- encode/decode for boxed short: -3 (interpreted path)[0m
[32m- encode/decode for boxed int: -3 (codegen path)[0m
[32m- encode/decode for boxed int: -3 (interpreted path)[0m
[32m- encode/decode for boxed long: -3 (codegen path)[0m
[32m- encode/decode for boxed long: -3 (interpreted path)[0m
[32m- encode/decode for boxed float: -3.7 (codegen path)[0m
[32m- encode/decode for boxed float: -3.7 (interpreted path)[0m
[32m- encode/decode for boxed double: -3.7 (codegen path)[0m
[32m- encode/decode for boxed double: -3.7 (interpreted path)[0m
[32m- encode/decode for scala decimal: 32131413.211321313 (codegen path)[0m
[32m- encode/decode for scala decimal: 32131413.211321313 (interpreted path)[0m
[32m- encode/decode for java decimal: 231341.23123 (codegen path)[0m
[32m- encode/decode for java decimal: 231341.23123 (interpreted path)[0m
[32m- encode/decode for scala biginteger: 23134123123 (codegen path)[0m
[32m- encode/decode for scala biginteger: 23134123123 (interpreted path)[0m
[32m- encode/decode for java BigInteger: 23134123123 (codegen path)[0m
[32m- encode/decode for java BigInteger: 23134123123 (interpreted path)[0m
[32m- encode/decode for catalyst decimal: 32131413.211321313 (codegen path)[0m
[32m- encode/decode for catalyst decimal: 32131413.211321313 (interpreted path)[0m
[32m- encode/decode for string: hello (codegen path)[0m
[32m- encode/decode for string: hello (interpreted path)[0m
[32m- encode/decode for date: 2012-12-23 (codegen path)[0m
[32m- encode/decode for date: 2012-12-23 (interpreted path)[0m
[32m- encode/decode for timestamp: 2016-01-29 10:00:00.0 (codegen path)[0m
[32m- encode/decode for timestamp: 2016-01-29 10:00:00.0 (interpreted path)[0m
[32m- encode/decode for array of timestamp: [Ljava.sql.Timestamp;@7c871ce4 (codegen path)[0m
[32m- encode/decode for array of timestamp: [Ljava.sql.Timestamp;@7c871ce4 (interpreted path)[0m
[32m- encode/decode for binary: [B@2c4be45f (codegen path)[0m
[32m- encode/decode for binary: [B@2c4be45f (interpreted path)[0m
[32m- encode/decode for seq of int: List(31, -123, 4) (codegen path)[0m
[32m- encode/decode for seq of int: List(31, -123, 4) (interpreted path)[0m
[32m- encode/decode for seq of string: List(abc, xyz) (codegen path)[0m
[32m- encode/decode for seq of string: List(abc, xyz) (interpreted path)[0m
[32m- encode/decode for seq of string with null: List(abc, null, xyz) (codegen path)[0m
[32m- encode/decode for seq of string with null: List(abc, null, xyz) (interpreted path)[0m
[32m- encode/decode for empty seq of int: List() (codegen path)[0m
[32m- encode/decode for empty seq of int: List() (interpreted path)[0m
[32m- encode/decode for empty seq of string: List() (codegen path)[0m
[32m- encode/decode for empty seq of string: List() (interpreted path)[0m
[32m- encode/decode for seq of seq of int: List(List(31, -123), null, List(4, 67)) (codegen path)[0m
[32m- encode/decode for seq of seq of int: List(List(31, -123), null, List(4, 67)) (interpreted path)[0m
[32m- encode/decode for seq of seq of string: List(List(abc, xyz), List(null), null, List(1, null, 2)) (codegen path)[0m
[32m- encode/decode for seq of seq of string: List(List(abc, xyz), List(null), null, List(1, null, 2)) (interpreted path)[0m
[32m- encode/decode for array of int: [I@7857cb1d (codegen path)[0m
[32m- encode/decode for array of int: [I@7857cb1d (interpreted path)[0m
[32m- encode/decode for array of string: [Ljava.lang.String;@42536da6 (codegen path)[0m
[32m- encode/decode for array of string: [Ljava.lang.String;@42536da6 (interpreted path)[0m
[32m- encode/decode for array of string with null: [Ljava.lang.String;@1e592ef2 (codegen path)[0m
[32m- encode/decode for array of string with null: [Ljava.lang.String;@1e592ef2 (interpreted path)[0m
[32m- encode/decode for empty array of int: [I@7a51dc38 (codegen path)[0m
[32m- encode/decode for empty array of int: [I@7a51dc38 (interpreted path)[0m
[32m- encode/decode for empty array of string: [Ljava.lang.String;@43ca96a0 (codegen path)[0m
[32m- encode/decode for empty array of string: [Ljava.lang.String;@43ca96a0 (interpreted path)[0m
[32m- encode/decode for array of array of int: [[I@56e78538 (codegen path)[0m
[32m- encode/decode for array of array of int: [[I@56e78538 (interpreted path)[0m
[32m- encode/decode for array of array of string: [[Ljava.lang.String;@67671db1 (codegen path)[0m
[32m- encode/decode for array of array of string: [[Ljava.lang.String;@67671db1 (interpreted path)[0m
[32m- encode/decode for map: Map(1 -> a, 2 -> b) (codegen path)[0m
[32m- encode/decode for map: Map(1 -> a, 2 -> b) (interpreted path)[0m
[32m- encode/decode for map with null: Map(1 -> a, 2 -> null) (codegen path)[0m
[32m- encode/decode for map with null: Map(1 -> a, 2 -> null) (interpreted path)[0m
[32m- encode/decode for map of map: Map(1 -> Map(a -> 1), 2 -> Map(b -> 2)) (codegen path)[0m
[32m- encode/decode for map of map: Map(1 -> Map(a -> 1), 2 -> Map(b -> 2)) (interpreted path)[0m
[32m- encode/decode for null seq in tuple: (null) (codegen path)[0m
[32m- encode/decode for null seq in tuple: (null) (interpreted path)[0m
[32m- encode/decode for null map in tuple: (null) (codegen path)[0m
[32m- encode/decode for null map in tuple: (null) (interpreted path)[0m
[32m- encode/decode for list of int: List(1, 2) (codegen path)[0m
[32m- encode/decode for list of int: List(1, 2) (interpreted path)[0m
[32m- encode/decode for list with String and null: List(a, null) (codegen path)[0m
[32m- encode/decode for list with String and null: List(a, null) (interpreted path)[0m
[32m- encode/decode for udt with case class: UDTCaseClass(http://spark.apache.org/) (codegen path)[0m
[32m- encode/decode for udt with case class: UDTCaseClass(http://spark.apache.org/) (interpreted path)[0m
[32m- encode/decode for kryo string: hello (codegen path)[0m
[32m- encode/decode for kryo string: hello (interpreted path)[0m
[32m- encode/decode for kryo object: org.apache.spark.sql.catalyst.encoders.KryoSerializable@f (codegen path)[0m
[32m- encode/decode for kryo object: org.apache.spark.sql.catalyst.encoders.KryoSerializable@f (interpreted path)[0m
[32m- encode/decode for java string: hello (codegen path)[0m
[32m- encode/decode for java string: hello (interpreted path)[0m
[32m- encode/decode for java object: org.apache.spark.sql.catalyst.encoders.JavaSerializable@f (codegen path)[0m
[32m- encode/decode for java object: org.apache.spark.sql.catalyst.encoders.JavaSerializable@f (interpreted path)[0m
[32m- encode/decode for InnerClass: InnerClass(1) (codegen path)[0m
[32m- encode/decode for InnerClass: InnerClass(1) (interpreted path)[0m
[32m- encode/decode for array of inner class: [Lorg.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$InnerClass;@1e4d93f7 (codegen path)[0m
[32m- encode/decode for array of inner class: [Lorg.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$InnerClass;@1e4d93f7 (interpreted path)[0m
[32m- encode/decode for array of optional inner class: [Lscala.Option;@31fc658f (codegen path)[0m
[32m- encode/decode for array of optional inner class: [Lscala.Option;@31fc658f (interpreted path)[0m
[32m- encode/decode for PrimitiveData: PrimitiveData(1,1,1.0,1.0,1,1,true) (codegen path)[0m
[32m- encode/decode for PrimitiveData: PrimitiveData(1,1,1.0,1.0,1,1,true) (interpreted path)[0m
[32m- encode/decode for OptionalData: OptionalData(Some(2),Some(2),Some(2.0),Some(2.0),Some(2),Some(2),Some(true),Some(PrimitiveData(1,1,1.0,1.0,1,1,true))) (codegen path)[0m
[32m- encode/decode for OptionalData: OptionalData(Some(2),Some(2),Some(2.0),Some(2.0),Some(2),Some(2),Some(true),Some(PrimitiveData(1,1,1.0,1.0,1,1,true))) (interpreted path)[0m
[32m- encode/decode for OptionalData: OptionalData(None,None,None,None,None,None,None,None) (codegen path)[0m
[32m- encode/decode for OptionalData: OptionalData(None,None,None,None,None,None,None,None) (interpreted path)[0m
[32m- encode/decode for Option in array: List(Some(1), None) (codegen path)[0m
[32m- encode/decode for Option in array: List(Some(1), None) (interpreted path)[0m
[32m- encode/decode for Option in map: Map(1 -> Some(10), 2 -> Some(20), 3 -> None) (codegen path)[0m
[32m- encode/decode for Option in map: Map(1 -> Some(10), 2 -> Some(20), 3 -> None) (interpreted path)[0m
[32m- encode/decode for BoxedData: BoxedData(1,1,1.0,1.0,1,1,true) (codegen path)[0m
[32m- encode/decode for BoxedData: BoxedData(1,1,1.0,1.0,1,1,true) (interpreted path)[0m
[32m- encode/decode for BoxedData: BoxedData(null,null,null,null,null,null,null) (codegen path)[0m
[32m- encode/decode for BoxedData: BoxedData(null,null,null,null,null,null,null) (interpreted path)[0m
[32m- encode/decode for RepeatedStruct: RepeatedStruct(List(PrimitiveData(1,1,1.0,1.0,1,1,true))) (codegen path)[0m
[32m- encode/decode for RepeatedStruct: RepeatedStruct(List(PrimitiveData(1,1,1.0,1.0,1,1,true))) (interpreted path)[0m
[32m- encode/decode for Tuple3: (1,test,PrimitiveData(1,1,1.0,1.0,1,1,true)) (codegen path)[0m
[32m- encode/decode for Tuple3: (1,test,PrimitiveData(1,1,1.0,1.0,1,1,true)) (interpreted path)[0m
[32m- encode/decode for RepeatedData: RepeatedData(List(1, 2),List(1, null, 2),Map(1 -> 2),Map(1 -> null),PrimitiveData(1,1,1.0,1.0,1,1,true)) (codegen path)[0m
[32m- encode/decode for RepeatedData: RepeatedData(List(1, 2),List(1, null, 2),Map(1 -> 2),Map(1 -> null),PrimitiveData(1,1,1.0,1.0,1,1,true)) (interpreted path)[0m
[32m- encode/decode for NestedArray: NestedArray([[I@47bbf44d) (codegen path)[0m
[32m- encode/decode for NestedArray: NestedArray([[I@47bbf44d) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(String, String)],List((a,b))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(String, String)],List((a,b))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Int, Int)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Int, Int)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Long, Long)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Long, Long)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Float, Float)],List((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Float, Float)],List((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Double, Double)],List((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Double, Double)],List((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Short, Short)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Short, Short)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Byte, Byte)],List((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Byte, Byte)],List((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[(Boolean, Boolean)],List((true,false))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[(Boolean, Boolean)],List((true,false))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(String, String)],ArrayBuffer((a,b))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(String, String)],ArrayBuffer((a,b))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Int, Int)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Int, Int)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Long, Long)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Long, Long)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Float, Float)],ArrayBuffer((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Float, Float)],ArrayBuffer((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Double, Double)],ArrayBuffer((1.0,2.0))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Double, Double)],ArrayBuffer((1.0,2.0))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Short, Short)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Short, Short)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Byte, Byte)],ArrayBuffer((1,2))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Byte, Byte)],ArrayBuffer((1,2))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Boolean, Boolean)],ArrayBuffer((true,false))) (codegen path)[0m
[32m- encode/decode for Tuple2: (ArrayBuffer[(Boolean, Boolean)],ArrayBuffer((true,false))) (interpreted path)[0m
[32m- encode/decode for Tuple2: (Seq[Seq[(Int, Int)]],List(List((1,2)))) (codegen path)[0m
[32m- encode/decode for Tuple2: (Seq[Seq[(Int, Int)]],List(List((1,2)))) (interpreted path)[0m
[32m- encode/decode for tuple with 2 flat encoders: (1,10) (codegen path)[0m
[32m- encode/decode for tuple with 2 flat encoders: (1,10) (interpreted path)[0m
[32m- encode/decode for tuple with 2 product encoders: (PrimitiveData(1,1,1.0,1.0,1,1,true),(3,30)) (codegen path)[0m
[32m- encode/decode for tuple with 2 product encoders: (PrimitiveData(1,1,1.0,1.0,1,1,true),(3,30)) (interpreted path)[0m
[32m- encode/decode for tuple with flat encoder and product encoder: (PrimitiveData(1,1,1.0,1.0,1,1,true),3) (codegen path)[0m
[32m- encode/decode for tuple with flat encoder and product encoder: (PrimitiveData(1,1,1.0,1.0,1,1,true),3) (interpreted path)[0m
[32m- encode/decode for tuple with product encoder and flat encoder: (3,PrimitiveData(1,1,1.0,1.0,1,1,true)) (codegen path)[0m
[32m- encode/decode for tuple with product encoder and flat encoder: (3,PrimitiveData(1,1,1.0,1.0,1,1,true)) (interpreted path)[0m
[32m- encode/decode for nested tuple encoder: (1,(10,100)) (codegen path)[0m
[32m- encode/decode for nested tuple encoder: (1,(10,100)) (interpreted path)[0m
[32m- encode/decode for primitive value class: PrimitiveValueClass(42) (codegen path)[0m
[32m- encode/decode for primitive value class: PrimitiveValueClass(42) (interpreted path)[0m
[32m- encode/decode for reference value class: ReferenceValueClass(Container(1)) (codegen path)[0m
[32m- encode/decode for reference value class: ReferenceValueClass(Container(1)) (interpreted path)[0m
[32m- encode/decode for option of int: Some(31) (codegen path)[0m
[32m- encode/decode for option of int: Some(31) (interpreted path)[0m
[32m- encode/decode for empty option of int: None (codegen path)[0m
[32m- encode/decode for empty option of int: None (interpreted path)[0m
[32m- encode/decode for option of string: Some(abc) (codegen path)[0m
[32m- encode/decode for option of string: Some(abc) (interpreted path)[0m
[32m- encode/decode for empty option of string: None (codegen path)[0m
[32m- encode/decode for empty option of string: None (interpreted path)[0m
[32m- encode/decode for Tuple2: (UDT,org.apache.spark.sql.catalyst.encoders.ExamplePoint@691) (codegen path)[0m
[32m- encode/decode for Tuple2: (UDT,org.apache.spark.sql.catalyst.encoders.ExamplePoint@691) (interpreted path)[0m
[32m- nullable of encoder schema (codegen path)[0m
[32m- nullable of encoder schema (interpreted path)[0m
[32m- nullable of encoder serializer (codegen path)[0m
[32m- nullable of encoder serializer (interpreted path)[0m
[32m- null check for map key: String (codegen path)[0m
[32m- null check for map key: String (interpreted path)[0m
[32m- null check for map key: Integer (codegen path)[0m
[32m- null check for map key: Integer (interpreted path)[0m
[32m- throw exception for tuples with more than 22 elements (codegen path)[0m
[32m- throw exception for tuples with more than 22 elements (interpreted path)[0m
[32m- encode/decode for scala decimal within precision/scale limit: 99999999999999999999.999999999999999999 (codegen path)[0m
[32m- encode/decode for scala decimal within precision/scale limit: 99999999999999999999.999999999999999999 (interpreted path)[0m
[32m- encode/decode for java decimal within precision/scale limit: 99999999999999999999.999999999999999999 (codegen path)[0m
[32m- encode/decode for java decimal within precision/scale limit: 99999999999999999999.999999999999999999 (interpreted path)[0m
[32m- encode/decode for negative scala decimal within precision/scale limit: -99999999999999999999.999999999999999999 (codegen path)[0m
[32m- encode/decode for negative scala decimal within precision/scale limit: -99999999999999999999.999999999999999999 (interpreted path)[0m
[32m- encode/decode for negative java decimal within precision/scale limit: -99999999999999999999.999999999999999999 (codegen path)[0m
[32m- encode/decode for negative java decimal within precision/scale limit: -99999999999999999999.999999999999999999 (interpreted path)[0m
[32m- overflowing scala big decimal, ansiEnabled=true (codegen path)[0m
[32m- overflowing scala big decimal, ansiEnabled=true (interpreted path)[0m
[32m- overflowing scala big decimal, ansiEnabled=false (codegen path)[0m
[32m- overflowing scala big decimal, ansiEnabled=false (interpreted path)[0m
[32m- overflowing java big decimal, ansiEnabled=true (codegen path)[0m
[32m- overflowing java big decimal, ansiEnabled=true (interpreted path)[0m
[32m- overflowing java big decimal, ansiEnabled=false (codegen path)[0m
[32m- overflowing java big decimal, ansiEnabled=false (interpreted path)[0m
[32m- overflowing negative scala big decimal, ansiEnabled=true (codegen path)[0m
[32m- overflowing negative scala big decimal, ansiEnabled=true (interpreted path)[0m
[32m- overflowing negative scala big decimal, ansiEnabled=false (codegen path)[0m
[32m- overflowing negative scala big decimal, ansiEnabled=false (interpreted path)[0m
[32m- overflowing negative java big decimal, ansiEnabled=true (codegen path)[0m
[32m- overflowing negative java big decimal, ansiEnabled=true (interpreted path)[0m
[32m- overflowing negative java big decimal, ansiEnabled=false (codegen path)[0m
[32m- overflowing negative java big decimal, ansiEnabled=false (interpreted path)[0m
[32m- overflowing scala big decimal with fractional part, ansiEnabled=true (codegen path)[0m
[32m- overflowing scala big decimal with fractional part, ansiEnabled=true (interpreted path)[0m
[32m- overflowing scala big decimal with fractional part, ansiEnabled=false (codegen path)[0m
[32m- overflowing scala big decimal with fractional part, ansiEnabled=false (interpreted path)[0m
[32m- overflowing java big decimal with fractional part, ansiEnabled=true (codegen path)[0m
[32m- overflowing java big decimal with fractional part, ansiEnabled=true (interpreted path)[0m
[32m- overflowing java big decimal with fractional part, ansiEnabled=false (codegen path)[0m
[32m- overflowing java big decimal with fractional part, ansiEnabled=false (interpreted path)[0m
[32m- overflowing scala big decimal with long fractional part, ansiEnabled=true (codegen path)[0m
[32m- overflowing scala big decimal with long fractional part, ansiEnabled=true (interpreted path)[0m
[32m- overflowing scala big decimal with long fractional part, ansiEnabled=false (codegen path)[0m
[32m- overflowing scala big decimal with long fractional part, ansiEnabled=false (interpreted path)[0m
[32m- overflowing java big decimal with long fractional part, ansiEnabled=true (codegen path)[0m
[32m- overflowing java big decimal with long fractional part, ansiEnabled=true (interpreted path)[0m
[32m- overflowing java big decimal with long fractional part, ansiEnabled=false (codegen path)[0m
[32m- overflowing java big decimal with long fractional part, ansiEnabled=false (interpreted path)[0m
[32m- encode/decode for scala big integer within precision limit: 99999999999999999999999999999999999999 (codegen path)[0m
[32m- encode/decode for scala big integer within precision limit: 99999999999999999999999999999999999999 (interpreted path)[0m
[32m- encode/decode for java big integer within precision limit: 99999999999999999999999999999999999999 (codegen path)[0m
[32m- encode/decode for java big integer within precision limit: 99999999999999999999999999999999999999 (interpreted path)[0m
[32m- encode/decode for negative scala big integer within precision limit: -99999999999999999999999999999999999999 (codegen path)[0m
[32m- encode/decode for negative scala big integer within precision limit: -99999999999999999999999999999999999999 (interpreted path)[0m
[32m- encode/decode for negative java big integer within precision limit: -99999999999999999999999999999999999999 (codegen path)[0m
[32m- encode/decode for negative java big integer within precision limit: -99999999999999999999999999999999999999 (interpreted path)[0m
[32m- overflowing scala big int, ansiEnabled=true (codegen path)[0m
[32m- overflowing scala big int, ansiEnabled=true (interpreted path)[0m
[32m- overflowing scala big int, ansiEnabled=false (codegen path)[0m
[32m- overflowing scala big int, ansiEnabled=false (interpreted path)[0m
[32m- overflowing java big integer, ansiEnabled=true (codegen path)[0m
[32m- overflowing java big integer, ansiEnabled=true (interpreted path)[0m
[32m- overflowing java big integer, ansiEnabled=false (codegen path)[0m
[32m- overflowing java big integer, ansiEnabled=false (interpreted path)[0m
[32m- overflowing negative scala big int, ansiEnabled=true (codegen path)[0m
[32m- overflowing negative scala big int, ansiEnabled=true (interpreted path)[0m
[32m- overflowing negative scala big int, ansiEnabled=false (codegen path)[0m
[32m- overflowing negative scala big int, ansiEnabled=false (interpreted path)[0m
[32m- overflowing negative java big integer, ansiEnabled=true (codegen path)[0m
[32m- overflowing negative java big integer, ansiEnabled=true (interpreted path)[0m
[32m- overflowing negative java big integer, ansiEnabled=false (codegen path)[0m
[32m- overflowing negative java big integer, ansiEnabled=false (interpreted path)[0m
[32m- overflowing scala very large big int, ansiEnabled=true (codegen path)[0m
[32m- overflowing scala very large big int, ansiEnabled=true (interpreted path)[0m
[32m- overflowing scala very large big int, ansiEnabled=false (codegen path)[0m
[32m- overflowing scala very large big int, ansiEnabled=false (interpreted path)[0m
[32m- overflowing java very big int, ansiEnabled=true (codegen path)[0m
[32m- overflowing java very big int, ansiEnabled=true (interpreted path)[0m
[32m- overflowing java very big int, ansiEnabled=false (codegen path)[0m
[32m- overflowing java very big int, ansiEnabled=false (interpreted path)[0m
[32m- encode/decode for makeCopy: (foo,1) (codegen path)[0m
[32m- encode/decode for makeCopy: (foo,1) (interpreted path)[0m
[32mSubstituteUnresolvedOrdinalsSuite:[0m
[32m- unresolved ordinal should not be unresolved[0m
[32m- order by ordinal[0m
[32m- group by ordinal[0m
[32mCollapseProjectSuite:[0m
[32m- collapse two deterministic, independent projects into one[0m
[32m- collapse two deterministic, dependent projects into one[0m
[32m- do not collapse nondeterministic projects[0m
[32m- collapse two nondeterministic, independent projects into one[0m
[32m- collapse one nondeterministic, one deterministic, independent projects into one[0m
[32m- collapse project into aggregate[0m
[32m- do not collapse common nondeterministic project and aggregate[0m
[32m- preserve top-level alias metadata while collapsing projects[0m
[32m- collapse redundant alias through limit[0m
[32m- collapse redundant alias through local limit[0m
[32m- collapse redundant alias through repartition[0m
[32m- collapse redundant alias through sample[0m
[32mCodeGeneratorWithInterpretedFallbackSuite:[0m
[32m- UnsafeProjection with codegen factory mode[0m
[32m- MutableProjection with codegen factory mode[0m
[32m- fallback to the interpreter mode[0m
[32m- codegen failures in the CODEGEN_ONLY mode[0m
[32m- SPARK-25358 Correctly handles NoOp in MutableProjection[0m
[32m- SPARK-25374 Correctly handles NoOp in SafeProjection[0m
[32mV2OverwriteByExpressionStrictAnalysisSuite:[0m
[32m- byName: basic behavior[0m
[32m- byName: does not match by position[0m
[32m- byName: case sensitive column resolution[0m
[32m- byName: case insensitive column resolution[0m
[32m- byName: data columns are reordered by name[0m
[32m- byName: fail nullable data written to required columns[0m
[32m- byName: allow required data written to nullable columns[0m
[32m- byName: missing required columns cause failure and are identified by name[0m
[32m- byName: missing optional columns cause failure and are identified by name[0m
[32m- byName: insert safe cast[0m
[32m- byName: fail extra data fields[0m
[32m- byPosition: basic behavior[0m
[32m- byPosition: data columns are not reordered[0m
[32m- byPosition: fail nullable data written to required columns[0m
[32m- byPosition: allow required data written to nullable columns[0m
[32m- byPosition: missing required columns cause failure[0m
[32m- byPosition: missing optional columns cause failure[0m
[32m- byPosition: insert safe cast[0m
[32m- byPosition: fail extra data fields[0m
[32m- bypass output column resolution[0m
[32m- check fields of struct type column[0m
[32m- byName: fail canWrite check[0m
[32m- byName: multiple field errors are reported[0m
[32m- byPosition: fail canWrite check[0m
[32m- byPosition: multiple field errors are reported[0m
[32m- delete expression is resolved using table fields[0m
[32m- delete expression is not resolved using query fields[0m
[32mPredicateSuite:[0m
[32m- 3VL Not[0m
[32m- AND, OR, EqualTo, EqualNullSafe consistency check[0m
[32m- 3VL AND[0m
[32m- 3VL OR[0m
[32m- 3VL =[0m
[32m- basic IN/INSET predicate test[0m
[32m- IN with different types[0m
[32m- switch statements in InSet for bytes, shorts, ints, dates[0m
[32m- SPARK-22501: In should not generate codes beyond 64KB[0m
[32m- SPARK-22705: In should use less global variables[0m
[32m- IN/INSET: binary[0m
[32m- IN/INSET: struct[0m
[32m- IN/INSET: array[0m
[32m- BinaryComparison consistency check[0m
[32m- BinaryComparison: lessThan[0m
[32m- BinaryComparison: LessThanOrEqual[0m
[32m- BinaryComparison: GreaterThan[0m
[32m- BinaryComparison: GreaterThanOrEqual[0m
[32m- BinaryComparison: EqualTo[0m
[32m- BinaryComparison: EqualNullSafe[0m
[32m- BinaryComparison: null test[0m
[32m- EqualTo on complex type[0m
[32m- EqualTo double/float infinity[0m
[32m- SPARK-22693: InSet should not use global variables[0m
[32m- SPARK-24007: EqualNullSafe for FloatType and DoubleType might generate a wrong result[0m
[32m- Interpreted Predicate should initialize nondeterministic expressions[0m
[32m- SPARK-24872: Replace taking the $symbol with $sqlOperator in BinaryOperator's toString method[0m
[32m- istrue and isnottrue[0m
[32m- isfalse and isnotfalse[0m
[32m- isunknown and isnotunknown[0m
[32m- SPARK-29100: InSet with empty input set[0m
[32mCombineConcatsSuite:[0m
[32m- combine nested Concat exprs[0m
[32m- combine string and binary exprs[0m
[32mComplexTypesSuite:[0m
[32m- explicit get from namedStruct[0m
[32m- explicit get from named_struct- expression maintains original deduced alias[0m
[32m- collapsed getStructField ontop of namedStruct[0m
[32m- collapse multiple CreateNamedStruct/GetStructField pairs[0m
[32m- collapsed2 - deduced names[0m
[32m- simplified array ops[0m
[32m- SPARK-22570: CreateArray should not create a lot of global variables[0m
[32m- SPARK-23208: Test code splitting for create array related methods[0m
[32m- simplify map ops[0m
[32m- simplify map ops, constant lookup, dynamic keys[0m
[32m- simplify map ops, dynamic lookup, dynamic keys, lookup is equivalent to one of the keys[0m
[32m- simplify map ops, no positive match[0m
[32m- simplify map ops, constant lookup, mixed keys, eliminated constants[0m
[32m- simplify map ops, potential dynamic match with null value + an absolute constant match[0m
[32m- SPARK-23500: Simplify array ops that are not at the top node[0m
[32m- SPARK-23500: Simplify map ops that are not top nodes[0m
[32m- SPARK-23500: Simplify complex ops that aren't at the plan root[0m
[32m- SPARK-23500: Ensure that aggregation expressions are not simplified[0m
[32m- SPARK-23500: namedStruct and getField in the same Project #1[0m
[32m- SPARK-23500: namedStruct and getField in the same Project #2[0m
[32m- SPARK-24313: support binary type as map keys in GetMapValue[0m
[32mCatalogSuite:[0m
[32m- desc table when owner is set to null[0m
[32mOptimizerRuleExclusionSuite:[0m
[32m- Exclude a single rule from multiple batches[0m
[32m- Exclude multiple rules from single or multiple batches[0m
[32m- Exclude non-existent rule with other valid rules[0m
[32m- Try to exclude some non-excludable rules[0m
[32m- Custom optimizer[0m
[32m- Verify optimized plan after excluding CombineUnions rule[0m
[32mCSVInferSchemaSuite:[0m
[32m- String fields types are inferred correctly from null types[0m
[32m- String fields types are inferred correctly from other types[0m
[32m- Timestamp field types are inferred correctly via custom data format[0m
[32m- Timestamp field types are inferred correctly from other types[0m
[32m- Boolean fields types are inferred correctly from other types[0m
[32m- Type arrays are merged to highest common type[0m
[32m- Null fields are handled properly when a nullValue is specified[0m
[32m- Merging Nulltypes should yield Nulltype.[0m
[32m- SPARK-18433: Improve DataSource option keys to be more case-insensitive[0m
[32m- SPARK-18877: `inferField` on DecimalType should find a common type with `typeSoFar`[0m
[32m- DoubleType should be inferred when user defined nan/inf are provided[0m
[32m- inferring the decimal type using locale[0m
[32mExpressionSetSuite:[0m
[32m- expect 1: (A#1 + 1), (a#1 + 1)[0m
[32m- expect 2: (A#1 + 1), (a#1 + 2)[0m
[32m- expect 2: (A#1 + 1), (a#3 + 1)[0m
[32m- expect 2: (A#1 + 1), (B#2 + 1)[0m
[32m- expect 1: (A#1 + a#1), (a#1 + A#1)[0m
[32m- expect 1: (A#1 + B#2), (B#2 + A#1)[0m
[32m- expect 1: ((A#1 + B#2) + 3), ((B#2 + 3) + A#1), ((B#2 + A#1) + 3), ((3 + A#1) + B#2)[0m
[32m- expect 1: ((A#1 * B#2) * 3), ((B#2 * 3) * A#1), ((B#2 * A#1) * 3), ((3 * A#1) * B#2)[0m
[32m- expect 1: (A#1 = B#2), (B#2 = A#1)[0m
[32m- expect 1: ((A#1 + 1) = B#2), (B#2 = (1 + A#1))[0m
[32m- expect 2: (A#1 - B#2), (B#2 - A#1)[0m
[32m- expect 1: (A#1 > B#2), (B#2 < A#1)[0m
[32m- expect 1: (A#1 >= B#2), (B#2 <= A#1)[0m
[32m- expect 1: NOT (none#4 > 1), (none#4 <= 1), NOT (1 < none#4), (1 >= none#4)[0m
[32m- expect 1: NOT (none#5 > 1), (none#5 <= 1), NOT (1 < none#5), (1 >= none#5)[0m
[32m- expect 1: NOT (none#4 < 1), (none#4 >= 1), NOT (1 > none#4), (1 <= none#4)[0m
[32m- expect 1: NOT (none#5 < 1), (none#5 >= 1), NOT (1 > none#5), (1 <= none#5)[0m
[32m- expect 1: NOT (none#4 >= 1), (none#4 < 1), NOT (1 <= none#4), (1 > none#4)[0m
[32m- expect 1: NOT (none#5 >= 1), (none#5 < 1), NOT (1 <= none#5), (1 > none#5)[0m
[32m- expect 1: NOT (none#4 <= 1), (none#4 > 1), NOT (1 >= none#4), (1 < none#4)[0m
[32m- expect 1: NOT (none#5 <= 1), (none#5 > 1), NOT (1 >= none#5), (1 < none#5)[0m
[32m- expect 1: ((A#1 > B#2) AND (A#1 <= 10)), ((A#1 <= 10) AND (A#1 > B#2))[0m
[32m- expect 1: (((A#1 > B#2) AND (B#2 > 100)) AND (A#1 <= 10)), (((B#2 > 100) AND (A#1 <= 10)) AND (A#1 > B#2))[0m
[32m- expect 1: ((A#1 > B#2) OR (A#1 <= 10)), ((A#1 <= 10) OR (A#1 > B#2))[0m
[32m- expect 1: (((A#1 > B#2) OR (B#2 > 100)) OR (A#1 <= 10)), (((B#2 > 100) OR (A#1 <= 10)) OR (A#1 > B#2))[0m
[32m- expect 1: (((A#1 <= 10) AND (A#1 > B#2)) OR (B#2 > 100)), ((B#2 > 100) OR ((A#1 <= 10) AND (A#1 > B#2)))[0m
[32m- expect 1: ((A#1 >= B#2) OR ((A#1 > 10) AND (B#2 < 10))), (((B#2 < 10) AND (A#1 > 10)) OR (A#1 >= B#2))[0m
[32m- expect 1: (((B#2 > 100) OR ((A#1 < 100) AND (B#2 <= A#1))) OR ((A#1 >= 10) AND (B#2 >= 50))), ((((A#1 >= 10) AND (B#2 >= 50)) OR (B#2 > 100)) OR ((A#1 < 100) AND (B#2 <= A#1))), ((((B#2 >= 50) AND (A#1 >= 10)) OR ((B#2 <= A#1) AND (A#1 < 100))) OR (B#2 > 100))[0m
[32m- expect 1: ((((B#2 > 100) AND (A#1 < 100)) AND (B#2 <= A#1)) OR ((A#1 >= 10) AND (B#2 >= 50))), (((A#1 >= 10) AND (B#2 >= 50)) OR (((A#1 < 100) AND (B#2 > 100)) AND (B#2 <= A#1))), (((B#2 >= 50) AND (A#1 >= 10)) OR (((B#2 <= A#1) AND (A#1 < 100)) AND (B#2 > 100)))[0m
[32m- expect 1: (((A#1 >= 10) OR (((B#2 <= 10) AND (A#1 = B#2)) AND (A#1 < 100))) OR (B#2 >= 100)), (((((A#1 = B#2) AND (A#1 < 100)) AND (B#2 <= 10)) OR (B#2 >= 100)) OR (A#1 >= 10)), (((((A#1 < 100) AND (B#2 <= 10)) AND (A#1 = B#2)) OR (A#1 >= 10)) OR (B#2 >= 100)), ((((B#2 <= 10) AND (A#1 = B#2)) AND (A#1 < 100)) OR ((A#1 >= 10) OR (B#2 >= 100)))[0m
[32m- expect 2: ((rand(1) > A#1) AND (A#1 <= 10)), ((A#1 <= 10) AND (rand(1) > A#1))[0m
[32m- expect 2: (((A#1 > B#2) AND (B#2 > 100)) AND (rand(1) > A#1)), (((B#2 > 100) AND (rand(1) > A#1)) AND (A#1 > B#2))[0m
[32m- expect 2: ((rand(1) > A#1) OR (A#1 <= 10)), ((A#1 <= 10) OR (rand(1) > A#1))[0m
[32m- expect 2: (((A#1 > B#2) OR (A#1 <= rand(1))) OR (A#1 <= 10)), (((A#1 <= rand(1)) OR (A#1 <= 10)) OR (A#1 > B#2))[0m
[32m- expect 2: rand(1), rand(1)[0m
[32m- expect 2: (((A#1 > B#2) OR (B#2 > 100)) AND (A#1 = rand(1))), (((B#2 > 100) OR (A#1 > B#2)) AND (A#1 = rand(1)))[0m
[32m- expect 2: (((rand(1) > A#1) OR ((A#1 <= rand(1)) AND (A#1 > B#2))) OR ((A#1 > 10) AND (B#2 > 10))), (((rand(1) > A#1) OR ((A#1 <= rand(1)) AND (A#1 > B#2))) OR ((B#2 > 10) AND (A#1 > 10)))[0m
[32m- expect 2: (((rand(1) > A#1) OR ((A#1 <= rand(1)) AND (A#1 > B#2))) OR ((A#1 > 10) AND (B#2 > 10))), (((rand(1) > A#1) OR ((A#1 > B#2) AND (A#1 <= rand(1)))) OR ((A#1 > 10) AND (B#2 > 10)))[0m
[32m- add to / remove from set[0m
[32m- add multiple elements to set[0m
[32m- add single element to set with non-deterministic expressions[0m
[32m- remove single element to set with non-deterministic expressions[0m
[32m- add multiple elements to set with non-deterministic expressions[0m
[32m- remove multiple elements to set with non-deterministic expressions[0m
[32mResolvedUuidExpressionsSuite:[0m
[32m- analyzed plan sets random seed for Uuid expression[0m
[32m- Uuid expressions should have different random seeds[0m
[32m- Different analyzed plans should have different random seeds in Uuids[0m
[32mExtractPythonUDFFromJoinConditionSuite:[0m
[32m- inner join condition with python udf[0m
[32m- unevaluable python udf and common condition[0m
[32m- unevaluable python udf or common condition[0m
[32m- pull out whole complex condition with multiple unevaluable python udf[0m
[32m- partial pull out complex condition with multiple unevaluable python udf[0m
[32m- pull out unevaluable python udf when it's mixed with evaluable one[0m
[32m- throw an exception for not supported join types[0m
[32mMetadataSuite:[0m
[32m- String Metadata[0m
[32m- Long Metadata[0m
[32m- Double Metadata[0m
[32m- Boolean Metadata[0m
[32m- Null Metadata[0m
[32mParserUtilsSuite:[0m
[32m- unescapeSQLString[0m
[32m- command[0m
[32m- operationNotAllowed[0m
[32m- checkDuplicateKeys[0m
[32m- source[0m
[32m- remainder[0m
[32m- string[0m
[32m- position[0m
[32m- validate[0m
[32m- withOrigin[0m
[32mExpressionEvalHelperSuite:[0m
[32m- SPARK-16489 checkEvaluation should fail if expression reuses variable names[0m
[32m- SPARK-25388: checkEvaluation should fail if nullable in DataType is incorrect[0m
[32mXPathExpressionSuite:[0m
[32m- xpath_boolean[0m
[32m- xpath_short[0m
[32m- xpath_int[0m
[32m- xpath_long[0m
[32m- xpath_float[0m
[32m- xpath_double[0m
[32m- xpath_string[0m
[32m- xpath[0m
[32m- accept only literal path[0m
[32mHashExpressionsSuite:[0m
[32m- md5[0m
[32m- sha1[0m
[32m- sha2[0m
[32m- crc32[0m
[32m- hive-hash for null[0m
[32m- hive-hash for boolean[0m
[32m- hive-hash for byte[0m
[32m- hive-hash for short[0m
[32m- hive-hash for int[0m
[32m- hive-hash for long[0m
[32m- hive-hash for float[0m
[32m- hive-hash for double[0m
[32m- hive-hash for string[0m
[32m- hive-hash for date type[0m
[32m- hive-hash for timestamp type[0m
[32m- hive-hash for CalendarInterval type[0m
[32m- hive-hash for array[0m
[32m- hive-hash for map[0m
[32m- hive-hash for struct[0m
[32m- murmur3/xxHash64/hive hash: struct<null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,bigDecimal:decimal(38,18),smallDecimal:decimal(10,0),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint>[0m
[32m- murmur3/xxHash64/hive hash: struct<arrayOfNull:array<null>,arrayOfString:array<string>,arrayOfArrayOfString:array<array<string>>,arrayOfArrayOfInt:array<array<int>>,arrayOfMap:array<map<string,string>>,arrayOfStruct:array<struct<str:string>>,arrayOfUDT:array<examplepoint>>[0m
[32m- murmur3/xxHash64/hive hash: struct<mapOfIntAndString:map<int,string>,mapOfStringAndArray:map<string,array<string>>,mapOfArrayAndInt:map<array<string>,int>,mapOfArray:map<array<string>,array<string>>,mapOfStringAndStruct:map<string,struct<str:string>>,mapOfStructAndString:map<struct<str:string>,string>,mapOfStruct:map<struct<str:string>,struct<str:string>>>[0m
[32m- murmur3/xxHash64/hive hash: struct<structOfString:struct<str:string>,structOfStructOfString:struct<struct:struct<str:string>>,structOfArray:struct<array:array<string>>,structOfMap:struct<map:map<string,string>>,structOfArrayAndMap:struct<array:array<string>,map:map<string,string>>,structOfUDT:struct<udt:examplepoint>>[0m
[32m- hive-hash for decimal[0m
[32m- SPARK-18207: Compute hash for a lot of expressions[0m
[32m- SPARK-22284: Compute hash for nested structs[0m
[32mSimplifyConditionalSuite:[0m
[32m- simplify if[0m
[32m- remove unnecessary if when the outputs are semantic equivalence[0m
[32m- remove unreachable branches[0m
[32m- remove entire CaseWhen if only the else branch is reachable[0m
[32m- remove entire CaseWhen if the first branch is always true[0m
[32m- simplify CaseWhen, prune branches following a definite true[0m
[32m- simplify CaseWhen if all the outputs are semantic equivalence[0m
[32mCreateTablePartitioningValidationSuite:[0m
[32m- CreateTableAsSelect: fail missing top-level column[0m
[32m- CreateTableAsSelect: fail missing top-level column nested reference[0m
[32m- CreateTableAsSelect: fail missing nested column[0m
[32m- CreateTableAsSelect: fail with multiple errors[0m
[32m- CreateTableAsSelect: success with top-level column[0m
[32m- CreateTableAsSelect: success using nested column[0m
[32m- CreateTableAsSelect: success using complex column[0m
[32mLogicalPlanSuite:[0m
[32m- transformUp runs on operators[0m
[32m- transformUp runs on operators recursively[0m
[32m- isStreaming[0m
[32m- transformExpressions works with a Stream[0m
[32mHigherOrderFunctionsSuite:[0m
[32m- ArrayTransform[0m
[32m- ArraySort[0m
[32m- MapFilter[0m
[32m- ArrayFilter[0m
[32m- ArrayExists[0m
[32m- ArrayForAll[0m
[32m- ArrayAggregate[0m
[32m- TransformKeys[0m
[32m- TransformValues[0m
[32m- MapZipWith[0m
[32m- ZipWith[0m
[32mDDLParserSuite:[0m
[32m- SPARK-30098: create table without provider should use default data source under non-legacy mode[0m
[32m- create/replace table using - schema[0m
[32m- create/replace table - with IF NOT EXISTS[0m
[32m- create/replace table - with partitioned by[0m
[32m- create/replace table - partitioned by transforms[0m
[32m- create/replace table - with bucket[0m
[32m- create/replace table - with comment[0m
[32m- create/replace table - with table properties[0m
[32m- create/replace table - with location[0m
[32m- create/replace table - byte length literal table name[0m
[32m- Duplicate clauses - create/replace table[0m
[32m- support for other types in OPTIONS[0m
[32m- Test CTAS against native tables[0m
[32m- drop table[0m
[32m- drop view[0m
[32m- alter view: alter view properties[0m
[32m- alter table: alter table properties[0m
[32m- alter table: add column[0m
[32m- alter table: add multiple columns[0m
[32m- alter table: add column with COLUMNS[0m
[32m- alter table: add column with COLUMNS (...)[0m
[32m- alter table: add column with COLUMNS (...) and COMMENT[0m
[32m- alter table: add column with COMMENT[0m
[32m- alter table: add column with position[0m
[32m- alter table: add column with nested column name[0m
[32m- alter table: add multiple columns with nested column name[0m
[32m- alter table: set location[0m
[32m- alter table: rename column[0m
[32m- alter table: update column type using ALTER[0m
[32m- alter table: update column type[0m
[32m- alter table: update column comment[0m
[32m- alter table: update column position[0m
[32m- alter table: update column type, comment and position[0m
[32m- alter table: drop column[0m
[32m- alter table: drop multiple columns[0m
[32m- alter table/view: rename table/view[0m
[32m- describe table column[0m
[32m- describe database[0m
[32m- SPARK-17328 Fix NPE with EXPLAIN DESCRIBE TABLE[0m
[32m- insert table: basic append[0m
[32m- insert table: append from another catalog[0m
[32m- insert table: append with partition[0m
[32m- insert table: overwrite[0m
[32m- insert table: overwrite with partition[0m
[32m- insert table: overwrite with partition if not exists[0m
[32m- insert table: if not exists with dynamic partition fails[0m
[32m- insert table: if not exists without overwrite fails[0m
[32m- delete from table: delete all[0m
[32m- delete from table: with alias and where clause[0m
[32m- delete from table: columns aliases is not allowed[0m
[32m- update table: basic[0m
[32m- update table: with alias and where clause[0m
[32m- update table: columns aliases is not allowed[0m
[32m- merge into table: basic[0m
[32m- merge into table: using subquery[0m
[32m- merge into table: cte[0m
[32m- merge into table: no additional condition[0m
[32m- merge into table: star[0m
[32m- merge into table: columns aliases are not allowed[0m
[32m- merge into table: at most two matched clauses[0m
[32m- merge into table: at most one not matched clause[0m
[32m- show tables[0m
[32m- show table extended[0m
[32m- create namespace -- backward compatibility with DATABASE/DBPROPERTIES[0m
[32m- create namespace -- check duplicates[0m
[32m- create namespace - property values must be set[0m
[32m- create namespace -- either PROPERTIES or DBPROPERTIES is allowed[0m
[32m- create namespace - support for other types in PROPERTIES[0m
[32m- drop namespace[0m
[32m- set namespace properties[0m
[32m- set namespace location[0m
[32m- show databases: basic[0m
[32m- show databases: FROM/IN operator is not allowed[0m
[32m- show namespaces[0m
[32m- analyze table statistics[0m
[32m- analyze table column statistics[0m
[32m- MSCK REPAIR TABLE[0m
[32m- LOAD DATA INTO table[0m
[32m- SHOW CREATE table[0m
[32m- CACHE TABLE[0m
[32m- UNCACHE TABLE[0m
[32m- TRUNCATE table[0m
[32m- SHOW PARTITIONS[0m
[32m- REFRESH TABLE[0m
[32m- show columns[0m
[32m- alter table: recover partitions[0m
[32m- alter table: add partition[0m
[32m- alter view: add partition (not supported)[0m
[32m- alter table: rename partition[0m
[32m- alter table: drop partition[0m
[32m- show current namespace[0m
[32m- alter table: SerDe properties[0m
[32m- alter view: AS Query[0m
[32m- create view -- basic[0m
[32m- create view - full[0m
[32m- create view -- partitioned view[0m
[32m- create view - duplicate clauses[0m
[32m- SHOW TBLPROPERTIES table[0m
[32m- DESCRIBE FUNCTION[0m
[32m- SHOW FUNCTIONS[0m
[32m- DROP FUNCTION[0m
[32mCastSuite:[0m
[32m- null cast[0m
[32m- cast string to date[0m
[32m- cast string to timestamp[0m
[32m- cast from boolean[0m
[32m- cast from float[0m
[32m- cast from double[0m
[32m- cast from string[0m
[32m- data type casting[0m
[32m- cast and add[0m
[32m- from decimal[0m
[32m- cast from date[0m
[32m- cast from timestamp[0m
[32m- cast from array[0m
[32m- cast from map[0m
[32m- cast from struct[0m
[32m- cast struct with a timestamp field[0m
[32m- complex casting[0m
[32m- cast between string and interval[0m
[32m- cast string to boolean[0m
[32m- SPARK-16729 type checking for casting to date type[0m
[32m- SPARK-20302 cast with same structure[0m
[32m- SPARK-22500: cast for struct should not generate codes beyond 64KB[0m
[32m- SPARK-22570: Cast should not create a lot of global variables[0m
[32m- SPARK-22825 Cast array to string[0m
[32m- SPARK-22973 Cast map to string[0m
[32m- SPARK-22981 Cast struct to string[0m
[32m- up-cast[0m
[32m- SPARK-27671: cast from nested null type in struct[0m
[32m- Throw exception on casting out-of-range value to decimal type[0m
[32m- Process Infinity, -Infinity, NaN in case insensitive manner[0m
[32m- Throw exception on casting out-of-range value to byte type[0m
[32m- Throw exception on casting out-of-range value to short type[0m
[32m- Throw exception on casting out-of-range value to int type[0m
[32m- Throw exception on casting out-of-range value to long type[0m
[32m- cast from int[0m
[32m- cast from long[0m
[32m- cast from int 2[0m
[32m- casting to fixed-precision decimals[0m
[32m- SPARK-28470: Cast should honor nullOnOverflow property[0m
[32m- collect_list/collect_set can cast to ArrayType not containsNull[0m
[32mANSIDataTypeWriteCompatibilitySuite:[0m
[32m- Check each type with itself[0m
[32m- Check atomic types: write allowed only when casting is safe[0m
[32m- Check struct types: missing required field[0m
[32m- Check struct types: missing starting field, matched by position[0m
[32m- Check struct types: missing middle field, matched by position[0m
[32m- Check struct types: generic colN names are ignored[0m
[32m- Check struct types: required field is optional[0m
[32m- Check struct types: data field would be dropped[0m
[32m- Check struct types: type promotion is allowed[0m
[32m- Check struct type: ignore field name mismatch with byPosition mode[0m
[33m- Check struct types: missing optional field is allowed !!! IGNORED !!![0m
[32m- Check array types: type promotion is allowed[0m
[32m- Check array types: cannot write optional to required elements[0m
[32m- Check array types: writing required to optional elements is allowed[0m
[32m- Check map value types: type promotion is allowed[0m
[32m- Check map value types: cannot write optional to required values[0m
[32m- Check map value types: writing required to optional values is allowed[0m
[32m- Check map key types: type promotion is allowed[0m
[32m- Check types with multiple errors[0m
[32m- Check map value types: unsafe casts are not allowed[0m
[32m- Check struct types: unsafe casts are not allowed[0m
[32m- Check array types: unsafe casts are not allowed[0m
[32m- Check map key types: unsafe casts are not allowed[0m
[32m- Conversions between timestamp and long are not allowed[0m
[32m- Check NullType is compatible with all other types[0m
[32mArrayBasedMapBuilderSuite:[0m
[32m- basic[0m
[32m- fail with null key[0m
[32m- remove duplicated keys with last wins policy[0m
[32m- binary type key[0m
[32m- struct type key[0m
[32m- array type key[0m
[32mInferFiltersFromConstraintsSuite:[0m
[32m- filter: filter out constraints in condition[0m
[32m- single inner join: filter out values on either side on equi-join keys[0m
[32m- single inner join: filter out nulls on either side on non equal keys[0m
[32m- single inner join with pre-existing filters: filter out values on either side[0m
[32m- single outer join: no null filters are generated[0m
[32m- multiple inner joins: filter out values on all sides on equi-join keys[0m
[32m- inner join with filter: filter out values on all sides on equi-join keys[0m
[32m- inner join with alias: alias contains multiple attributes[0m
[32m- inner join with alias: alias contains single attributes[0m
[32m- generate correct filters for alias that don't produce recursive constraints[0m
[32m- No inferred filter when constraint propagation is disabled[0m
[32m- constraints should be inferred from aliased literals[0m
[32m- SPARK-23405: left-semi equal-join should filter out null join keys on both sides[0m
[32m- SPARK-21479: Outer join after-join filters push down to null-supplying side[0m
[32m- SPARK-21479: Outer join pre-existing filters push down to null-supplying side[0m
[32m- SPARK-21479: Outer join no filter push down to preserved side[0m
[32m- SPARK-23564: left anti join should filter out null join keys on right side[0m
[32m- SPARK-23564: left outer join should filter out null join keys on right side[0m
[32m- SPARK-23564: right outer join should filter out null join keys on left side[0m
[32mBufferHolderSuite:[0m
[32m- SPARK-16071 Check the size limit to avoid integer overflow[0m
[32mRowTest:[0m
[32mRow (without schema)[0m
[32m- throws an exception when accessing by fieldName[0m
[32mRow (with schema)[0m
[32m- fieldIndex(name) returns field index[0m
[32m- getAs[T] retrieves a value by fieldname[0m
[32m- Accessing non existent field throws an exception[0m
[32m- getValuesMap() retrieves values of multiple fields as a Map(field -> value)[0m
[32m- getValuesMap() retrieves null value on non AnyVal Type[0m
[32m- getAs() on type extending AnyVal throws an exception when accessing field that is null[0m
[32m- getAs() on type extending AnyVal does not throw exception when value is null[0m
[32mrow equals[0m
[32m- equality check for external rows[0m
[32m- equality check for internal rows[0m
[32mrow immutability[0m
[32m- copy should return same ref for external rows[0m
[32m- toSeq should not expose internal state for external rows[0m
[32m- toSeq should not expose internal state for internal rows[0m
[32mPullupCorrelatedPredicatesSuite:[0m
[32m- PullupCorrelatedPredicates should not produce unresolved plan[0m
[32m- PullupCorrelatedPredicates in correlated subquery idempotency check[0m
[32m- PullupCorrelatedPredicates exists correlated subquery idempotency check[0m
[32m- PullupCorrelatedPredicates scalar correlated subquery idempotency check[0m
[32mSubexpressionEliminationSuite:[0m
[32m- Semantic equals and hash[0m
[32m- Expression Equivalence - basic[0m
[32m- Expression Equivalence - Trees[0m
[32m- Expression equivalence - non deterministic[0m
[32m- Children of CodegenFallback[0m
[32m- Children of conditional expressions[0m
[32mStrictDataTypeWriteCompatibilitySuite:[0m
[32m- Check each type with itself[0m
[32m- Check atomic types: write allowed only when casting is safe[0m
[32m- Check struct types: missing required field[0m
[32m- Check struct types: missing starting field, matched by position[0m
[32m- Check struct types: missing middle field, matched by position[0m
[32m- Check struct types: generic colN names are ignored[0m
[32m- Check struct types: required field is optional[0m
[32m- Check struct types: data field would be dropped[0m
[32m- Check struct types: type promotion is allowed[0m
[32m- Check struct type: ignore field name mismatch with byPosition mode[0m
[33m- Check struct types: missing optional field is allowed !!! IGNORED !!![0m
[32m- Check array types: type promotion is allowed[0m
[32m- Check array types: cannot write optional to required elements[0m
[32m- Check array types: writing required to optional elements is allowed[0m
[32m- Check map value types: type promotion is allowed[0m
[32m- Check map value types: cannot write optional to required values[0m
[32m- Check map value types: writing required to optional values is allowed[0m
[32m- Check map key types: type promotion is allowed[0m
[32m- Check types with multiple errors[0m
[32m- Check struct types: unsafe casts are not allowed[0m
[32m- Check array types: unsafe casts are not allowed[0m
[32m- Check map value types: casting Long to Integer is not allowed[0m
[32m- Check map key types: unsafe casts are not allowed[0m
[32m- Check NullType is incompatible with all other types[0m
[32mErrorParserSuite:[0m
[32m- no viable input[0m
[32m- extraneous input[0m
[32m- mismatched input[0m
[32m- semantic errors[0m
[32m- SPARK-21136: misleading error message due to problematic antlr grammar[0m
[32m- hyphen in identifier - DDL tests[0m
[32m- hyphen in identifier - DML tests[0m
[32mCatalystTypeConvertersSuite:[0m
[32m- null handling in rows[0m
[32m- null handling for individual values[0m
[32m- option handling in convertToCatalyst[0m
[32m- option handling in createToCatalystConverter[0m
[32m- primitive array handling[0m
[32m- An array with null handling[0m
[32m- converting a wrong value to the struct type[0m
[32m- converting a wrong value to the map type[0m
[32m- converting a wrong value to the array type[0m
[32m- converting a wrong value to the decimal type[0m
[32m- converting a wrong value to the string type[0m
[32m- SPARK-24571: convert Char to String[0m
[32m- converting java.time.Instant to TimestampType[0m
[32m- converting TimestampType to java.time.Instant[0m
[32m- converting java.time.LocalDate to DateType[0m
[32m- converting DateType to java.time.LocalDate[0m
[32mScalaReflectionSuite:[0m
[32m- isSubtype[0m
[32m- SQLUserDefinedType annotation on Scala structure[0m
[32m- primitive data[0m
[32m- nullable data[0m
[32m- optional data[0m
[32m- complex data[0m
[32m- generic data[0m
[32m- tuple data[0m
[32m- type-aliased data[0m
[32m- convert PrimitiveData to catalyst[0m
[32m- convert Option[Product] to catalyst[0m
[32m- infer schema from case class with multiple constructors[0m
[32m- SPARK-15062: Get correct serializer for List[_][0m
[32m- SPARK 16792: Get correct deserializer for List[_][0m
[32m- serialize and deserialize arbitrary sequence types[0m
[32m- serialize and deserialize arbitrary map types[0m
[32m- SPARK-22442: Generate correct field names for special characters[0m
[32m- SPARK-22472: add null check for top-level primitive values[0m
[32m- SPARK-23025: schemaFor should support Null type[0m
[32m- SPARK-23835: add null check to non-nullable types in Tuples[0m
[32m- SPARK-8288: schemaFor works for a class with only a companion object constructor[0m
[32m- SPARK-29026: schemaFor for trait without companion object throws exception [0m
[32m- SPARK-29026: schemaFor for trait with no-constructor companion throws exception [0m
[32m- SPARK-27625: annotated data types[0m
[32mApproximatePercentileSuite:[0m
[32m- serialize and de-serialize[0m
[32m- class PercentileDigest, basic operations[0m
[32m- class PercentileDigest, makes sure the memory foot print is bounded[0m
[32m- class ApproximatePercentile, high level interface, update, merge, eval...[0m
[32m- class ApproximatePercentile, low level interface, update, merge, eval...[0m
[32m- class ApproximatePercentile, sql string[0m
[32m- class ApproximatePercentile, fails analysis if percentage or accuracy is not a constant[0m
[32m- class ApproximatePercentile, fails analysis if parameters are invalid[0m
[32m- class ApproximatePercentile, automatically add type casting for parameters[0m
[32m- class ApproximatePercentile, null handling[0m
[32mCanonicalizeSuite:[0m
[32m- SPARK-24276: IN expression with different order are semantically equal[0m
[32m- SPARK-26402: accessing nested fields with different cases in case insensitive mode[0m
[32mGenerateUnsafeRowJoinerBitsetSuite:[0m
[32m- bitset concat: boundary size 0, 0[0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m  + num fields: 0 and 0 [0m
[32m- bitset concat: boundary size 0, 64[0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m  + num fields: 0 and 64 [0m
[32m- bitset concat: boundary size 64, 0[0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m  + num fields: 64 and 0 [0m
[32m- bitset concat: boundary size 64, 64[0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m  + num fields: 64 and 64 [0m
[32m- bitset concat: boundary size 0, 128[0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m  + num fields: 0 and 128 [0m
[32m- bitset concat: boundary size 128, 0[0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m  + num fields: 128 and 0 [0m
[32m- bitset concat: boundary size 128, 128[0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m  + num fields: 128 and 128 [0m
[32m- bitset concat: single word bitsets[0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m  + num fields: 10 and 5 [0m
[32m- bitset concat: first bitset larger than a word[0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m  + num fields: 67 and 5 [0m
[32m- bitset concat: second bitset larger than a word[0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m  + num fields: 6 and 67 [0m
[32m- bitset concat: no reduction in bitset size[0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m  + num fields: 33 and 34 [0m
[32m- bitset concat: two words[0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m  + num fields: 120 and 95 [0m
[32m- bitset concat: bitset 65, 128[0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m  + num fields: 65 and 128 [0m
[32m- bitset concat: randomized tests[0m
[32m  + num fields: 751 and 986 [0m
[32m  + num fields: 386 and 403 [0m
[32m  + num fields: 43 and 397 [0m
[32m  + num fields: 247 and 647 [0m
[32m  + num fields: 230 and 401 [0m
[32m  + num fields: 752 and 842 [0m
[32m  + num fields: 381 and 190 [0m
[32m  + num fields: 766 and 654 [0m
[32m  + num fields: 378 and 733 [0m
[32m  + num fields: 930 and 474 [0m
[32m  + num fields: 420 and 162 [0m
[32m  + num fields: 796 and 270 [0m
[32m  + num fields: 485 and 341 [0m
[32m  + num fields: 404 and 964 [0m
[32m  + num fields: 799 and 133 [0m
[32m  + num fields: 999 and 754 [0m
[32m  + num fields: 313 and 493 [0m
[32m  + num fields: 772 and 973 [0m
[32m  + num fields: 201 and 584 [0m
[32mNullExpressionsSuite:[0m
[32m- isnull and isnotnull[0m
[32m- AssertNotNUll[0m
[32m- IsNaN[0m
[32m- nanvl[0m
[32m- coalesce[0m
[32m- SPARK-16602 Nvl should support numeric-string cases[0m
[32m- AtLeastNNonNulls[0m
[32m- Coalesce should not throw 64KiB exception[0m
[32m- SPARK-22705: Coalesce should use less global variables[0m
[32m- AtLeastNNonNulls should not throw 64KiB exception[0m
[32mExpressionSQLBuilderSuite:[0m
[32m- literal[0m
[32m- attributes[0m
[32m- binary comparisons[0m
[32m- logical operators[0m
[32m- arithmetic expressions[0m
[32m- window specification[0m
[32m- interval arithmetic[0m
[32mLookupFunctionsSuite:[0m
[32m- SPARK-23486: the functionExists for the Persistent function check[0m
[32m- SPARK-23486: the functionExists for the Registered function check[0m
[32mEliminateDistinctSuite:[0m
[32m- Eliminate Distinct in Max[0m
[32m- Eliminate Distinct in Min[0m
[32mDateTimeUtilsSuite:[0m
[32m- nanoseconds truncation[0m
[32m- timestamp and us[0m
[32m- us and julian day[0m
[32m- SPARK-6785: java date conversion before and after epoch[0m
[32m- string to date[0m
[32m- string to timestamp[0m
[32m- SPARK-15379: special invalid date string[0m
[32m- hours[0m
[32m- minutes[0m
[32m- seconds[0m
[32m- hours / minutes / seconds[0m
[32m- get day in year[0m
[32m- day of year calculations for old years[0m
[32m- get year[0m
[32m- get quarter[0m
[32m- get month[0m
[32m- get day of month[0m
[32m- date add months[0m
[32m- timestamp add months[0m
[32m- timestamp add days[0m
[32m- monthsBetween[0m
[32m- from UTC timestamp[0m
[32m- to UTC timestamp[0m
[32m- trailing characters while converting string to timestamp[0m
[32m- truncTimestamp[0m
[32m- daysToMillis and millisToDays[0m
[32m- toMillis[0m
[32m- special timestamp values[0m
[32m- special date values[0m
[32mPlanParserSuite:[0m
[32m- case insensitive[0m
[32m- explain[0m
[32m- set operations[0m
[32m- common table expressions[0m
[32m- simple select query[0m
[32m- hive-style single-FROM statement[0m
[32m- multi select query[0m
[32m- query organization[0m
[32m- insert into[0m
[32m- aggregation[0m
[32m- limit[0m
[32m- window spec[0m
[32m- lateral view[0m
[32m- joins[0m
[32m- sampled relations[0m
[32m- sub-query[0m
[32m- scalar sub-query[0m
[32m- table reference[0m
[32m- table valued function[0m
[32m- SPARK-20311 range(N) as alias[0m
[32m- SPARK-20841 Support table column aliases in FROM clause[0m
[32m- SPARK-20962 Support subquery column aliases in FROM clause[0m
[32m- SPARK-20963 Support aliases for join relations in FROM clause[0m
[32m- inline table[0m
[32m- simple select query with !> and !<[0m
[32m- select hint syntax[0m
[32m- SPARK-20854: select hint syntax with expressions[0m
[32m- SPARK-20854: multiple hints[0m
[32m- TRIM function[0m
[32m- OVERLAY function[0m
[32m- precedence of set operations[0m
[32m- create/alter view as insert into table[0m
[32m- Invalid insert constructs in the query[0m
[32m- relation in v2 catalog[0m
[32m- CTE with column alias[0m
[32mMutableProjectionSuite:[0m
[32m- fixed-length types with CODEGEN_ONLY[0m
[32m- fixed-length types with NO_CODEGEN[0m
[32m- unsafe buffer with CODEGEN_ONLY[0m
[32m- unsafe buffer with NO_CODEGEN[0m
[32m- variable-length types with CODEGEN_ONLY[0m
[32m- variable-length types with NO_CODEGEN[0m
[32m- unsupported types for unsafe buffer[0m
[32mGeneratedProjectionSuite:[0m
[32m- generated projections on wider table[0m
[32m- SPARK-18016: generated projections on wider table requiring class-splitting[0m
[32m- generated unsafe projection with array of binary[0m
[32m- padding bytes should be zeroed out[0m
[32m- MutableProjection should not cache content from the input row[0m
[32m- SafeProjection should not cache content from the input row[0m
[32m- SPARK-22699: GenerateSafeProjection should not use global variables for struct[0m
[32m- SPARK-18016: generated projections on wider table requiring state compaction[0m
[32mTransposeWindowSuite:[0m
[32m- transpose two adjacent windows with compatible partitions[0m
[32m- transpose two adjacent windows with differently ordered compatible partitions[0m
[32m- don't transpose two adjacent windows with incompatible partitions[0m
[32m- don't transpose two adjacent windows with intersection of partition and output set[0m
[32m- don't transpose two adjacent windows with non-deterministic expressions[0m
[32mSetOperationSuite:[0m
[32m- union: combine unions into one unions[0m
[32m- union: filter to each side[0m
[32m- union: project to each side[0m
[32m- Remove unnecessary distincts in multiple unions[0m
[32m- Keep necessary distincts in multiple unions[0m
[32m- EXCEPT ALL rewrite[0m
[32m- INTERSECT ALL rewrite[0m
[32m- SPARK-23356 union: expressions with literal in project list are pushed down[0m
[32m- SPARK-23356 union: expressions in project list are pushed down[0m
[32m- SPARK-23356 union: no pushdown for non-deterministic expression[0m
[32mRuleExecutorSuite:[0m
[32m- idempotence[0m
[32m- only once[0m
[32m- to fixed point[0m
[32m- to maxIterations[0m
[32m- structural integrity checker - verify initial input[0m
[32m- structural integrity checker - verify rule execution result[0m
[32m- SPARK-27243: dumpTimeSpent when no rule has run[0m
[32mDistributionSuite:[0m
[32m- UnspecifiedDistribution and AllTuples[0m
[32m- SinglePartition is the output partitioning[0m
[32m- HashPartitioning is the output partitioning[0m
[32m- RangePartitioning is the output partitioning[0m
[32m- Partitioning.numPartitions must match Distribution.requiredNumPartitions to satisfy it[0m
[32mRewriteSubquerySuite:[0m
[32m- Column pruning after rewriting predicate subquery[0m
[32mJoinReorderSuite:[0m
[32m- reorder 3 tables[0m
[32m- put unjoinable item at the end and reorder 3 joinable tables[0m
[32m- reorder 3 tables with pure-attribute project[0m
[32m- reorder 3 tables - one of the leaf items is a project[0m
[32m- don't reorder if project contains non-attribute[0m
[32m- reorder 4 tables (bushy tree)[0m
[32m- keep the order of attributes in the final output[0m
[32m- SPARK-26352: join reordering should not change the order of attributes[0m
[32m- reorder recursively[0m
[32m- don't reorder if hints present[0m
[32m- reorder below and above the hint node[0m
[32mCodegenExpressionCachingSuite:[0m
[32m- GenerateUnsafeProjection should initialize expressions[0m
[32m- GenerateMutableProjection should initialize expressions[0m
[32m- GeneratePredicate should initialize expressions[0m
[32m- GenerateUnsafeProjection should not share expression instances[0m
[32m- GenerateMutableProjection should not share expression instances[0m
[32m- GeneratePredicate should not share expression instances[0m
[32mConditionalExpressionSuite:[0m
[32m- if[0m
[32m- case when[0m
[32m- if/case when - null flags of non-primitive types[0m
[32m- case key when[0m
[32m- case key when - internal pattern matching expects a List while apply takes a Seq[0m
[32m- SPARK-22705: case when should use less global variables[0m
[32m- SPARK-27551: informative error message of mismatched types for case when[0m
[32m- SPARK-27917 test semantic equals of CaseWhen[0m
[32mBasicStatsEstimationSuite:[0m
[32m- range[0m
[32m- windows[0m
[32m- limit estimation: limit < child's rowCount[0m
[32m- limit estimation: limit > child's rowCount[0m
[32m- limit estimation: limit = 0[0m
[32m- sample estimation[0m
[32m- estimate statistics when the conf changes[0m
[32mResolveSubquerySuite:[0m
[32m- SPARK-17251 Improve `OuterReference` to be `NamedExpression`[0m
[32m- SPARK-29145 Support subquery in join condition[0m
[32mComputeCurrentTimeSuite:[0m
[32m- analyzer should replace current_timestamp with literals[0m
[32m- analyzer should replace current_date with literals[0m
[32mFilterPushdownSuite:[0m
[32m- eliminate subqueries[0m
[32m- simple push down[0m
[32m- combine redundant filters[0m
[32m- do not combine non-deterministic filters even if they are identical[0m
[32m- SPARK-16164: Filter pushdown should keep the ordering in the logical plan[0m
[32m- SPARK-16994: filter should not be pushed through limit[0m
[32m- can't push without rewrite[0m
[32m- nondeterministic: can always push down filter through project with deterministic field[0m
[32m- nondeterministic: can't push down filter through project with nondeterministic field[0m
[32m- nondeterministic: can't push down filter through aggregate with nondeterministic field[0m
[32m- nondeterministic: push down part of filter through aggregate with deterministic field[0m
[32m- filters: combines filters[0m
[32m- joins: push to either side[0m
[32m- joins: push to one side[0m
[32m- joins: do not push down non-deterministic filters into join condition[0m
[32m- joins: push to one side after transformCondition[0m
[32m- joins: rewrite filter to push to either side[0m
[32m- joins: push down left semi join[0m
[32m- joins: push down left outer join #1[0m
[32m- joins: push down right outer join #1[0m
[32m- joins: push down left outer join #2[0m
[32m- joins: push down right outer join #2[0m
[32m- joins: push down left outer join #3[0m
[32m- joins: push down right outer join #3[0m
[32m- joins: push down left outer join #4[0m
[32m- joins: push down right outer join #4[0m
[32m- joins: push down left outer join #5[0m
[32m- joins: push down right outer join #5[0m
[32m- joins: can't push down[0m
[32m- joins: conjunctive predicates[0m
[32m- joins: conjunctive predicates #2[0m
[32m- joins: conjunctive predicates #3[0m
[32m- joins: push down where clause into left anti join[0m
[32m- joins: only push down join conditions to the right of a left anti join[0m
[32m- joins: only push down join conditions to the right of an existence join[0m
[32m- generate: predicate referenced no generated column[0m
[32m- generate: non-deterministic predicate referenced no generated column[0m
[32m- generate: part of conjuncts referenced generated column[0m
[32m- generate: all conjuncts referenced generated column[0m
[32m- aggregate: push down filter when filter on group by expression[0m
[32m- aggregate: don't push down filter when filter not on group by expression[0m
[32m- aggregate: push down filters partially which are subset of group by expressions[0m
[32m- aggregate: push down filters with alias[0m
[32m- aggregate: push down filters with literal[0m
[32m- aggregate: don't push down filters that are nondeterministic[0m
[32m- SPARK-17712: aggregate: don't push down filters that are data-independent[0m
[32m- aggregate: don't push filters if the aggregate has no grouping expressions[0m
[32m- union[0m
[32m- expand[0m
[32m- predicate subquery: push down simple[0m
[32m- predicate subquery: push down complex[0m
[32m- SPARK-20094: don't push predicate with IN subquery into join condition[0m
[32m- Window: predicate push down -- basic[0m
[32m- Window: predicate push down -- predicates with compound predicate using only one column[0m
[32m- Window: predicate push down -- multi window expressions with the same window spec[0m
[32m- Window: predicate push down -- multi window specification - 1[0m
[32m- Window: predicate push down -- multi window specification - 2[0m
[32m- Window: predicate push down -- predicates with multiple partitioning columns[0m
[33m- Window: predicate push down -- complex predicate with the same expressions !!! IGNORED !!![0m
[32m- Window: no predicate push down -- predicates are not from partitioning keys[0m
[32m- Window: no predicate push down -- partial compound partition key[0m
[32m- Window: no predicate push down -- complex predicates containing non partitioning columns[0m
[32m- Window: no predicate push down -- complex predicate with different expressions[0m
[32m- join condition pushdown: deterministic and non-deterministic[0m
[32m- watermark pushdown: no pushdown on watermark attribute #1[0m
[32m- watermark pushdown: no pushdown for nondeterministic filter[0m
[32m- watermark pushdown: full pushdown[0m
[32m- watermark pushdown: no pushdown on watermark attribute #2[0m
[32m- SPARK-28345: PythonUDF predicate should be able to pushdown to join[0m
[32mExprValueSuite:[0m
[32m- TrueLiteral and FalseLiteral should be LiteralValue[0m
[32mUtilSuite:[0m
[32m- truncatedString[0m
[32mObjectSerializerPruningSuite:[0m
[32m- collect struct types[0m
[32m- SPARK-26619: Prune the unused serializers from SerializeFromObject[0m
[32m- Prune nested serializers[0m
[32mBufferHolderSparkSubmitSuite:[0m
[31m- SPARK-22222: Buffer holder should be able to allocate memory larger than 1GB *** FAILED ***[0m
[31m  Process returned with exit code 127. See the log4j logs for more detail. (SparkSubmitSuite.scala:1435)[0m
[32mAttributeSetSuite:[0m
[32m- sanity check[0m
[32m- checks by id not name[0m
[32m- ++ preserves AttributeSet[0m
[32m- extracts all references [0m
[32m- dedups attributes[0m
[32m- subset[0m
[32m- equality[0m
[32m- SPARK-18394 keep a deterministic output order along with attribute names and exprIds[0m
[32mTypeCoercionSuite:[0m
[32m- implicit type cast - ByteType[0m
[32m- implicit type cast - ShortType[0m
[32m- implicit type cast - IntegerType[0m
[32m- implicit type cast - LongType[0m
[32m- implicit type cast - FloatType[0m
[32m- implicit type cast - DoubleType[0m
[32m- implicit type cast - DecimalType(10, 2)[0m
[32m- implicit type cast - BinaryType[0m
[32m- implicit type cast - BooleanType[0m
[32m- implicit type cast - StringType[0m
[32m- implicit type cast - DateType[0m
[32m- implicit type cast - TimestampType[0m
[32m- implicit type cast - ArrayType(StringType)[0m
[32m- implicit type cast between two Map types[0m
[32m- implicit type cast - StructType().add("a1", StringType)[0m
[32m- implicit type cast - NullType[0m
[32m- implicit type cast - CalendarIntervalType[0m
[32m- eligible implicit type cast - TypeCollection[0m
[32m- ineligible implicit type cast - TypeCollection[0m
[32m- tightest common bound for types[0m
[32m- wider common type for decimal and array[0m
[32m- cast NullType for expressions that implement ExpectsInputTypes[0m
[32m- cast NullType for binary operators[0m
[32m- coalesce casts[0m
[32m- CreateArray casts[0m
[32m- CreateMap casts[0m
[32m- greatest/least cast[0m
[32m- nanvl casts[0m
[32m- type coercion for If[0m
[32m- type coercion for CaseKeyWhen[0m
[32m- type coercion for Stack[0m
[32m- type coercion for Concat[0m
[32m- type coercion for Elt[0m
[32m- BooleanEquality type cast[0m
[32m- BooleanEquality simplification[0m
[32m- WidenSetOperationTypes for except and intersect[0m
[32m- WidenSetOperationTypes for union[0m
[32m- Transform Decimal precision/scale for union except and intersect[0m
[32m- make sure rules do not fire early[0m
[32m- SPARK-15776 Divide expression's dataType should be casted to Double or Decimal in aggregation function like sum[0m
[32m- SPARK-17117 null type coercion in divide[0m
[32m- binary comparison with string promotion[0m
[32m- cast WindowFrame boundaries to the type they operate upon[0m
[32m- SPARK-29000: skip to handle decimals in ImplicitTypeCasts[0m
[32mRandomUUIDGeneratorSuite:[0m
[32m- RandomUUIDGenerator should generate version 4, variant 2 UUIDs[0m
[32m- UUID from RandomUUIDGenerator should be deterministic[0m
[32m- Get UTF8String UUID[0m
[32mJoinOptimizationSuite:[0m
[32m- extract filters and joins[0m
[32m- reorder inner joins[0m
[32mAnalysisExternalCatalogSuite:[0m
[32m- query builtin functions don't call the external catalog[0m
[32m- check the existence of builtin functions don't call the external catalog[0m
[32mAggregateOptimizeSuite:[0m
[32m- remove literals in grouping expression[0m
[32m- do not remove all grouping expressions if they are all literals[0m
[32m- Remove aliased literals[0m
[32m- remove repetition in grouping expression[0m
[32mDateFormatterSuite:[0m
[32m- parsing dates[0m
[32m- format dates[0m
[32m- roundtrip date -> days -> date[0m
[32m- roundtrip days -> date -> days[0m
[32m- parsing date without explicit day[0m
[32m- formatting negative years with default pattern[0m
[32m- special date values[0m
[32mAggregateExpressionSuite:[0m
[32m- test references from unresolved aggregate functions[0m
[32mGenerateUnsafeRowJoinerSuite:[0m
[32m- simple fixed width types[0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m- rows with all empty strings[0m
[32m- rows with all empty int arrays[0m
[32m- alternating empty and non-empty strings[0m
[32m- randomized fix width types[0m
[32m  + schema size 41, 39 [0m
[32m  + schema size 65, 39 [0m
[32m  + schema size 31, 7 [0m
[32m  + schema size 84, 16 [0m
[32m  + schema size 29, 14 [0m
[32m  + schema size 77, 51 [0m
[32m  + schema size 92, 57 [0m
[32m  + schema size 32, 55 [0m
[32m  + schema size 51, 91 [0m
[32m  + schema size 93, 88 [0m
[32m  + schema size 54, 10 [0m
[32m  + schema size 95, 55 [0m
[32m  + schema size 37, 2 [0m
[32m  + schema size 79, 52 [0m
[32m  + schema size 51, 7 [0m
[32m  + schema size 41, 1 [0m
[32m  + schema size 79, 16 [0m
[32m  + schema size 61, 34 [0m
[32m  + schema size 66, 46 [0m
[32m  + schema size 67, 64 [0m
[32m- simple variable width types[0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 0 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 0, 1 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 1, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 64, 0 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 0, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m  + schema size 64, 64 [0m
[32m- randomized variable width types[0m
[32m  + schema size 47, 32 [0m
[32m  + schema size 97, 95 [0m
[32m  + schema size 73, 74 [0m
[32m  + schema size 2, 6 [0m
[32m  + schema size 23, 5 [0m
[32m  + schema size 40, 11 [0m
[32m  + schema size 18, 65 [0m
[32m  + schema size 10, 71 [0m
[32m  + schema size 55, 78 [0m
[32m  + schema size 40, 57 [0m
[32m- SPARK-22508: GenerateUnsafeRowJoiner.create should not generate codes beyond 64KB[0m
[32m  + schema size 3000, 3000 [0m
[32mUnsafeMapSuite:[0m
[32m- unsafe java serialization[0m
[32m- unsafe Kryo serialization[0m
[32mEliminateSortsSuite:[0m
[32m- Empty order by clause[0m
[32m- All the SortOrder are no-op[0m
[32m- Partial order-by clauses contain no-op SortOrder[0m
[32m- Remove no-op alias[0m
[32m- remove redundant order by[0m
[32m- do not remove sort if the order is different[0m
[32m- filters don't affect order[0m
[32m- limits don't affect order[0m
[32m- different sorts are not simplified if limit is in between[0m
[32m- range is already sorted[0m
[32m- sort should not be removed when there is a node which doesn't guarantee any order[0m
[32m- remove two consecutive sorts[0m
[32m- remove sorts separated by Filter/Project operators[0m
[32m- remove orderBy in groupBy clause with count aggs[0m
[32m- remove orderBy in groupBy clause with sum aggs[0m
[32m- should not remove orderBy in groupBy clause with first aggs[0m
[32m- should not remove orderBy in groupBy clause with first and count aggs[0m
[32m- should not remove orderBy in groupBy clause with PythonUDF as aggs[0m
[32m- should not remove orderBy in groupBy clause with ScalaUDF as aggs[0m
[32m- should not remove orderBy with limit in groupBy clause[0m
[32m- remove orderBy in join clause[0m
[32m- should not remove orderBy with limit in join clause[0m
[32m- should not remove orderBy in left join clause if there is an outer limit[0m
[32m- remove orderBy in right join clause event if there is an outer limit[0m
[32mJsonExpressionsSuite:[0m
[32m- $.store.bicycle[0m
[32m- $['store'].bicycle[0m
[32m- $.store['bicycle'][0m
[32m- $['store']['bicycle'][0m
[32m- $['key with spaces'][0m
[32m- $.store.book[0m
[32m- $.store.book[0][0m
[32m- $.store.book[*][0m
[32m- $[0m
[32m- $.store.book[0].category[0m
[32m- $.store.book[*].category[0m
[32m- $.store.book[*].isbn[0m
[32m- $.store.book[*].reader[0m
[32m- $.store.basket[0][1][0m
[32m- $.store.basket[*][0m
[32m- $.store.basket[*][0][0m
[32m- $.store.basket[0][*][0m
[32m- $.store.basket[*][*][0m
[32m- $.store.basket[0][2].b[0m
[32m- $.store.basket[0][*].b[0m
[32m- $.zip code[0m
[32m- $.fb:testid[0m
[32m- preserve newlines[0m
[32m- escape[0m
[32m- $.non_exist_key[0m
[32m- $..no_recursive[0m
[32m- $.store.book[10][0m
[32m- $.store.book[0].non_exist_key[0m
[32m- $.store.basket[*].non_exist_key[0m
[32m- SPARK-16548: character conversion[0m
[32m- non foldable literal[0m
[32m- some big value[0m
[32m- json_tuple - hive key 1[0m
[32m- json_tuple - hive key 2[0m
[32m- json_tuple - hive key 2 (mix of foldable fields)[0m
[32m- json_tuple - hive key 3[0m
[32m- json_tuple - hive key 3 (nonfoldable json)[0m
[32m- json_tuple - hive key 3 (nonfoldable fields)[0m
[32m- json_tuple - hive key 4 - null json[0m
[32m- json_tuple - hive key 5 - null and empty fields[0m
[32m- json_tuple - hive key 6 - invalid json (array)[0m
[32m- json_tuple - invalid json (object start only)[0m
[32m- json_tuple - invalid json (no object end)[0m
[32m- json_tuple - invalid json (invalid json)[0m
[32m- SPARK-16548: json_tuple - invalid json with leading nulls[0m
[32m- json_tuple - preserve newlines[0m
[32m- SPARK-21677: json_tuple throws NullPointException when column is null as string type[0m
[32m- SPARK-21804: json_tuple returns null values within repeated columns except the first one[0m
[32m- from_json[0m
[32m- from_json - invalid data[0m
[32m- from_json - input=array, schema=array, output=array[0m
[32m- from_json - input=object, schema=array, output=array of single row[0m
[32m- from_json - input=empty array, schema=array, output=empty array[0m
[32m- from_json - input=empty object, schema=array, output=array of single row with null[0m
[32m- from_json - input=array of single object, schema=struct, output=single row[0m
[32m- from_json - input=array, schema=struct, output=single row[0m
[32m- from_json - input=empty array, schema=struct, output=single row with null[0m
[32m- from_json - input=empty object, schema=struct, output=single row with null[0m
[32m- from_json null input column[0m
[32m- SPARK-20549: from_json bad UTF-8[0m
[32m- from_json with timestamp[0m
[32m- SPARK-19543: from_json empty input column[0m
[32m- to_json - struct[0m
[32m- to_json - array[0m
[32m- to_json - array with single empty row[0m
[32m- to_json - empty array[0m
[32m- to_json null input column[0m
[32m- to_json with timestamp[0m
[32m- SPARK-21513: to_json support map[string, struct] to json[0m
[32m- SPARK-21513: to_json support map[struct, struct] to json[0m
[32m- SPARK-21513: to_json support map[string, integer] to json[0m
[32m- to_json - array with maps[0m
[32m- to_json - array with single map[0m
[32m- to_json: verify MapType's value type instead of key type[0m
[32m- from_json missing fields[0m
[32m- SPARK-24709: infer schema of json strings[0m
[32m- infer schema of JSON strings by using options[0m
[32m- parse date with locale[0m
[32m- verify corrupt column[0m
[32m- parse decimals using locale[0m
[32m- inferring the decimal type using locale[0m
[32mMetadataSuite:[0m
[32m- metadata builder and getters[0m
[32m- metadata json conversion[0m
[32mStructTypeSuite:[0m
[32m- lookup a single missing field should output existing fields[0m
[32m- lookup a set of missing fields should output existing fields[0m
[32m- lookup fieldIndex for missing field should output existing fields[0m
[32m- SPARK-24849: toDDL - simple struct[0m
[32m- SPARK-24849: round trip toDDL - fromDDL[0m
[32m- SPARK-24849: round trip fromDDL - toDDL[0m
[32m- SPARK-24849: toDDL must take into account case of fields.[0m
[32m- SPARK-24849: toDDL should output field's comment[0m
[32m- Print up to the given level[0m
[32m- interval keyword in schema string[0m
[32mSortOrderExpressionsSuite:[0m
[32m- SortPrefix[0m
[32mConstraintPropagationSuite:[0m
[32m- propagating constraints in filters[0m
[32m- propagating constraints in aggregate[0m
[32m- propagating constraints in expand[0m
[32m- propagating constraints in aliases[0m
[32m- propagating constraints in union[0m
[32m- propagating constraints in intersect[0m
[32m- propagating constraints in except[0m
[32m- propagating constraints in inner join[0m
[32m- propagating constraints in left-semi join[0m
[32m- propagating constraints in left-outer join[0m
[32m- propagating constraints in right-outer join[0m
[32m- propagating constraints in full-outer join[0m
[32m- infer additional constraints in filters[0m
[32m- infer constraints on cast[0m
[32m- infer isnotnull constraints from compound expressions[0m
[32m- infer IsNotNull constraints from non-nullable attributes[0m
[32m- not infer non-deterministic constraints[0m
[32m- enable/disable constraint propagation[0m
[32mConvertToLocalRelationSuite:[0m
[32m- Project on LocalRelation should be turned into a single LocalRelation[0m
[32m- Filter on LocalRelation should be turned into a single LocalRelation[0m
[32m- SPARK-27798: Expression reusing output shouldn't override values in local relation[0m
[32mCodeBlockSuite:[0m
[32m- Block interpolates string and ExprValue inputs[0m
[32m- Literals are folded into string code parts instead of block inputs[0m
[32m- Block.stripMargin[0m
[32m- Block can capture input expr values[0m
[32m- concatenate blocks[0m
[32m- Throws exception when interpolating unexcepted object in code block[0m
[32m- transform expr in code block[0m
[32m- transform expr in nested blocks[0m
[32mTreeNodeSuite:[0m
[32m- top node changed[0m
[32m- one child changed[0m
[32m- no change[0m
[32m- collect[0m
[32m- pre-order transform[0m
[32m- post-order transform[0m
[32m- transform works on nodes with Option children[0m
[32m- mapChildren should only works on children[0m
[32m- preserves origin[0m
[32m- foreach up[0m
[32m- find[0m
[32m- collectFirst[0m
[32m- transformExpressions on nested expression sequence[0m
[32m- expressions inside a map[0m
[32m- toJSON[0m
[32m- toJSON should not throws java.lang.StackOverflowError[0m
[32m- transform works on stream of children[0m
[32m- withNewChildren on stream of children[0m
[32m- treeString limits plan length[0m
[32m- treeString limit at zero[0m
[32m- tags will be carried over after copy & transform[0m
[32m- clone[0m
[32mLookupCatalogWithDefaultSuite:[0m
[32m- catalog object identifier[0m
[32m- table identifier[0m
[32mEliminateMapObjectsSuite:[0m
[32m- SPARK-20254: Remove unnecessary data conversion for primitive array[0m
[32mCountMinSketchAggSuite:[0m
[32m- test data type ByteType[0m
[32m- test data type ShortType[0m
[32m- test data type IntegerType[0m
[32m- test data type LongType[0m
[32m- test data type StringType[0m
[32m- test data type BinaryType[0m
[32m- serialize and de-serialize[0m
[32m- fails analysis if eps, confidence or seed provided is not foldable[0m
[32m- fails analysis if parameters are invalid[0m
[32m- null handling[0m
[32mArithmeticExpressionSuite:[0m
[32m- + (Add)[0m
[32m- - (UnaryMinus)[0m
[32m- - (Minus)[0m
[32m- * (Multiply)[0m
[32m- / (Divide) basic[0m
[32m- / (Divide) for integral type[0m
[32m- % (Remainder)[0m
[32m- SPARK-17617: % (Remainder) double % double on super big double[0m
[32m- Abs[0m
[32m- pmod[0m
[32m- function least[0m
[32m- function greatest[0m
[32m- SPARK-22499: Least and greatest should not generate codes beyond 64KB[0m
[32m- SPARK-22704: Least and greatest use less global variables[0m
[32m- SPARK-28322: IntegralDivide supports decimal type[0m
[32m- SPARK-24598: overflow on long returns wrong result[0m
[32m- SPARK-24598: overflow on integer returns wrong result[0m
[32m- SPARK-24598: overflow on short returns wrong result[0m
[32m- SPARK-24598: overflow on byte returns wrong result[0m
[32mConstantFoldingSuite:[0m
[32m- eliminate subqueries[0m
[32m- Constant folding test: expressions only have literals[0m
[32m- Constant folding test: expressions have attribute references and literals in arithmetic operations[0m
[32m- Constant folding test: expressions have attribute references and literals in predicates[0m
[32m- Constant folding test: expressions have foldable functions[0m
[32m- Constant folding test: expressions have nonfoldable functions[0m
[32m- Constant folding test: expressions have null literals[0m
[32m- Constant folding test: Fold In(v, list) into true or false[0m
[32mCollapseRepartitionSuite:[0m
[32m- collapse two adjacent coalesces into one[0m
[32m- collapse two adjacent repartitions into one[0m
[32m- coalesce above repartition[0m
[32m- repartition above coalesce[0m
[32m- distribute above repartition[0m
[32m- distribute above coalesce[0m
[32m- repartition above distribute[0m
[32m- coalesce above distribute[0m
[32m- collapse two adjacent distributes into one[0m
[32mRewriteDistinctAggregatesSuite:[0m
[32m- single distinct group[0m
[32m- single distinct group with partial aggregates[0m
[32m- multiple distinct groups[0m
[32m- multiple distinct groups with partial aggregates[0m
[32m- multiple distinct groups with non-partial aggregates[0m
[32mJacksonGeneratorSuite:[0m
[32m- initial with StructType and write out a row[0m
[32m- SPARK-29444: initial with StructType and write out an empty row with ignoreNullFields=false[0m
[32m- SPARK-29444: initial with StructType field and write out a row with ignoreNullFields=false and struct inner null[0m
[32m- initial with StructType and write out rows[0m
[32m- initial with StructType and write out an array with single empty row[0m
[32m- initial with StructType and write out an empty array[0m
[32m- initial with Map and write out a map data[0m
[32m- initial with Map and write out an array of maps[0m
[32m- error handling: initial with StructType but error calling write a map[0m
[32m- error handling: initial with MapType and write out a row[0m
[32mDataTypeParserSuite:[0m
[32m- parse int[0m
[32m- parse integer[0m
[32m- parse BooLean[0m
[32m- parse tinYint[0m
[32m- parse smallINT[0m
[32m- parse INT[0m
[32m- parse INTEGER[0m
[32m- parse bigint[0m
[32m- parse float[0m
[32m- parse dOUBle[0m
[32m- parse decimal(10, 5)[0m
[32m- parse decimal[0m
[32m- parse Dec(10, 5)[0m
[32m- parse deC[0m
[32m- parse DATE[0m
[32m- parse timestamp[0m
[32m- parse string[0m
[32m- parse ChaR(5)[0m
[32m- parse ChaRacter(5)[0m
[32m- parse varchAr(20)[0m
[32m- parse cHaR(27)[0m
[32m- parse BINARY[0m
[32m- parse interval[0m
[32m- parse array<doublE>[0m
[32m- parse Array<map<int, tinYint>>[0m
[32m- parse array<struct<tinYint:tinyint>>[0m
[32m- parse MAP<int, STRING>[0m
[32m- parse MAp<int, ARRAY<double>>[0m
[32m- parse MAP<int, struct<varchar:string>>[0m
[32m- parse struct<intType: int, ts:timestamp>[0m
[32m- parse Struct<int: int, timestamp:timestamp>[0m
[32m- parse struct<  struct:struct<deciMal:DECimal, anotherDecimal:decimAL(5,2)>,  MAP:Map<timestamp, varchar(10)>,  arrAy:Array<double>,  anotherArray:Array<char(9)>>    [0m
[32m- parse struct<`x+y`:int, `!@#$%^&*()`:string, `1_2.345<>:"`:varchar(20)>[0m
[32m- parse strUCt<>[0m
[32m- it is not a data type is not supported[0m
[32m- struct<x+y: int, 1.1:timestamp> is not supported[0m
[32m- struct<x: int is not supported[0m
[32m- struct<x int, y string> is not supported[0m
[32m- Do not print empty parentheses for no params[0m
[32m- parse Struct<TABLE: string, DATE:boolean>[0m
[32m- parse struct<end: long, select: int, from: string>[0m
[32m- parse Struct<x: INT, y: STRING COMMENT 'test'>[0m
[32mApproxCountDistinctForIntervalsSuite:[0m
[32m- fails analysis if parameters are invalid[0m
[32m- merging ApproxCountDistinctForIntervals instances[0m
[32m- test findHllppIndex(value) for values in the range[0m
[32m- round trip serialization[0m
[32m- basic operations: update, merge, eval...[0m
[32m- test for different input types: numeric/date/timestamp[0m
[32mCsvExpressionsSuite:[0m
[32m- from_csv[0m
[32m- from_csv - invalid data[0m
[32m- from_csv null input column[0m
[32m- from_csv bad UTF-8[0m
[32m- from_csv with timestamp[0m
[32m- from_csv empty input column[0m
[32m- forcing schema nullability[0m
[32m- from_csv missing columns[0m
[32m- unsupported mode[0m
[32m- infer schema of CSV strings[0m
[32m- infer schema of CSV strings by using options[0m
[32m- to_csv - struct[0m
[32m- to_csv null input column[0m
[32m- to_csv with timestamp[0m
[32m- parse date with locale[0m
[32m- verify corrupt column[0m
[32mCatalogManagerSuite:[0m
[32m- CatalogManager should reflect the changes of default catalog[0m
[32m- CatalogManager should keep the current catalog once set[0m
[32m- current namespace should be updated when switching current catalog[0m
[32m- set current namespace[0m
[32mRegexpExpressionsSuite:[0m
[32m- LIKE Pattern[0m
[32m- LIKE Pattern ESCAPE '/'[0m
[32m- LIKE Pattern ESCAPE '#'[0m
[32m- LIKE Pattern ESCAPE '"'[0m
[32m- RLIKE Regular Expression[0m
[32m- RegexReplace[0m
[32m- SPARK-22570: RegExpReplace should not create a lot of global variables[0m
[32m- RegexExtract[0m
[32m- SPLIT[0m
[32mDecimalExpressionSuite:[0m
[32m- UnscaledValue[0m
[32m- MakeDecimal[0m
[32m- PromotePrecision[0m
[32m- CheckOverflow[0m
[32mRandomSuite:[0m
[32m- random[0m
[32m- SPARK-9127 codegen with long seed[0m
[32mCodeGenerationSuite:[0m
[32m- multithreaded eval[0m
[32m- metrics are recorded on compile[0m
[32m- SPARK-8443: split wide projections into blocks due to JVM code size limit[0m
[32m- SPARK-13242: case-when expression with large number of branches (or cases)[0m
[32m- SPARK-22543: split large if expressions into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide array creation into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide map creation into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide struct creation into blocks due to JVM code size limit[0m
[32m- SPARK-14793: split wide named struct creation into blocks due to JVM code size limit[0m
[32m- SPARK-14224: split wide external row creation into blocks due to JVM code size limit[0m
[32m- SPARK-17702: split wide constructor into blocks due to JVM code size limit[0m
[32m- SPARK-22226: group splitted expressions into one method per nested class[0m
[32m- test generated safe and unsafe projection[0m
[32m- */ in the data[0m
[32m- \u in the data[0m
[32m- check compilation error doesn't occur caused by specific literal[0m
[32m- SPARK-17160: field names are properly escaped by GetExternalRowField[0m
[32m- SPARK-17160: field names are properly escaped by AssertTrue[0m
[32m- should not apply common subexpression elimination on conditional expressions[0m
[32m- SPARK-22226: splitExpressions should not generate codes beyond 64KB[0m
[32m- SPARK-22543: split large predicates into blocks due to JVM code size limit[0m
[32m- SPARK-22696: CreateExternalRow should not use global variables[0m
[32m- SPARK-22696: InitializeJavaBean should not use global variables[0m
[32m- SPARK-22716: addReferenceObj should not add mutable states[0m
[32m- SPARK-18016: define mutable states by using an array[0m
[32m- SPARK-22750: addImmutableStateIfNotExists[0m
[32m- SPARK-23628: calculateParamLength should compute properly the param length[0m
[32m- SPARK-23760: CodegenContext.withSubExprEliminationExprs should save/restore correctly[0m
[32m- SPARK-23986: freshName can generate duplicated names[0m
[32m- SPARK-25113: should log when there exists generated methods above HugeMethodLimit[0m
[32m- SPARK-28916: subexrepssion elimination can cause 64kb code limit on UnsafeProjection[0m
[32mTableSchemaParserSuite:[0m
[32m- parse a int[0m
[32m- parse A int[0m
[32m- parse a INT[0m
[32m- parse `!@#$%.^&*()` string[0m
[32m- parse a int, b long[0m
[32m- parse a STRUCT<intType: int, ts:timestamp>[0m
[32m- parse a int comment 'test'[0m
[32m- complex hive type[0m
[32m- Negative cases[0m
[32mUnivocityParserSuite:[0m
[32m- Can parse decimal type values[0m
[32m- Nullable types are handled[0m
[32m- Throws exception for empty string with non null type[0m
[32m- Types are cast correctly[0m
[32m- Throws exception for casting an invalid string to Float and Double Types[0m
[32m- Float NaN values are parsed correctly[0m
[32m- Double NaN values are parsed correctly[0m
[32m- Float infinite values can be parsed[0m
[32m- Double infinite values can be parsed[0m
[32m- parse decimals using locale[0m
[32m- SPARK-27591 UserDefinedType can be read[0m
[32mRowEncoderSuite:[0m
[32m- encode/decode: struct<null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,decimal:decimal(38,18),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint> (codegen path)[0m
[32m- encode/decode: struct<null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,decimal:decimal(38,18),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint> (interpreted path)[0m
[32m- encode/decode: struct<arrayOfNull:array<null>,arrayOfString:array<string>,arrayOfArrayOfString:array<array<string>>,arrayOfArrayOfInt:array<array<int>>,arrayOfMap:array<map<string,string>>,arrayOfStruct:array<struct<str:string>>,arrayOfUDT:array<examplepoint>> (codegen path)[0m
[32m- encode/decode: struct<arrayOfNull:array<null>,arrayOfString:array<string>,arrayOfArrayOfString:array<array<string>>,arrayOfArrayOfInt:array<array<int>>,arrayOfMap:array<map<string,string>>,arrayOfStruct:array<struct<str:string>>,arrayOfUDT:array<examplepoint>> (interpreted path)[0m
[32m- encode/decode: struct<mapOfIntAndString:map<int,string>,mapOfStringAndArray:map<string,array<string>>,mapOfArrayAndInt:map<array<string>,int>,mapOfArray:map<array<string>,array<string>>,mapOfStringAndStruct:map<string,struct<str:string>>,mapOfStructAndString:map<struct<str:string>,string>,mapOfStruct:map<struct<str:string>,struct<str:string>>> (codegen path)[0m
[32m- encode/decode: struct<mapOfIntAndString:map<int,string>,mapOfStringAndArray:map<string,array<string>>,mapOfArrayAndInt:map<array<string>,int>,mapOfArray:map<array<string>,array<string>>,mapOfStringAndStruct:map<string,struct<str:string>>,mapOfStructAndString:map<struct<str:string>,string>,mapOfStruct:map<struct<str:string>,struct<str:string>>> (interpreted path)[0m
[32m- encode/decode: struct<structOfString:struct<str:string>,structOfStructOfString:struct<struct:struct<str:string>>,structOfArray:struct<array:array<string>>,structOfMap:struct<map:map<string,string>>,structOfArrayAndMap:struct<array:array<string>,map:map<string,string>>,structOfUDT:struct<udt:examplepoint>> (codegen path)[0m
[32m- encode/decode: struct<structOfString:struct<str:string>,structOfStructOfString:struct<struct:struct<str:string>>,structOfArray:struct<array:array<string>>,structOfMap:struct<map:map<string,string>>,structOfArrayAndMap:struct<array:array<string>,map:map<string,string>>,structOfUDT:struct<udt:examplepoint>> (interpreted path)[0m
[32m- encode/decode decimal type (codegen path)[0m
[32m- encode/decode decimal type (interpreted path)[0m
[32m- RowEncoder should preserve decimal precision and scale (codegen path)[0m
[32m- RowEncoder should preserve decimal precision and scale (interpreted path)[0m
[32m- SPARK-23179: RowEncoder should respect nullOnOverflow for decimals (codegen path)[0m
[32m- SPARK-23179: RowEncoder should respect nullOnOverflow for decimals (interpreted path)[0m
[32m- RowEncoder should preserve schema nullability (codegen path)[0m
[32m- RowEncoder should preserve schema nullability (interpreted path)[0m
[32m- RowEncoder should preserve nested column name (codegen path)[0m
[32m- RowEncoder should preserve nested column name (interpreted path)[0m
[32m- RowEncoder should support primitive arrays (codegen path)[0m
[32m- RowEncoder should support primitive arrays (interpreted path)[0m
[32m- RowEncoder should support array as the external type for ArrayType (codegen path)[0m
[32m- RowEncoder should support array as the external type for ArrayType (interpreted path)[0m
[32m- RowEncoder should throw RuntimeException if input row object is null (codegen path)[0m
[32m- RowEncoder should throw RuntimeException if input row object is null (interpreted path)[0m
[32m- RowEncoder should validate external type (codegen path)[0m
[32m- RowEncoder should validate external type (interpreted path)[0m
[32m- SPARK-25791: Datatype of serializers should be accessible (codegen path)[0m
[32m- SPARK-25791: Datatype of serializers should be accessible (interpreted path)[0m
[32m- encoding/decoding TimestampType to/from java.time.Instant (codegen path)[0m
[32m- encoding/decoding TimestampType to/from java.time.Instant (interpreted path)[0m
[32m- encoding/decoding DateType to/from java.time.LocalDate (codegen path)[0m
[32m- encoding/decoding DateType to/from java.time.LocalDate (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(IntegerType, containsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve array nullability: ArrayType(StringType, containsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, IntegerType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(IntegerType, StringType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, IntegerType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = true), nullable = false (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = true (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = true (interpreted path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = false (codegen path)[0m
[32m- RowEncoder should preserve map nullability: MapType(StringType, StringType, valueContainsNull = false), nullable = false (interpreted path)[0m
[32mSchemaUtilsSuite:[0m
[32m- Check column name duplication in case-sensitive cases[0m
[32m- Check column name duplication in case-insensitive cases[0m
[32m- Check no exception thrown for valid schemas[0m
[32mOptimizerExtendableSuite:[0m
[32m- Extending batches possible[0m
[32mFilterPushdownOnePassSuite:[0m
[32m- really simple predicate push down[0m
[32m- push down conjunctive predicates[0m
[32m- push down predicates for simple joins[0m
[32m- push down top-level filters for cascading joins[0m
[32m- push down predicates for tree-like joins[0m
[32m- push down through join and project[0m
[32m- push down through deep projects[0m
[32m- push down through aggregate and join[0m
[32mResolveInlineTablesSuite:[0m
[32m- validate inputs are foldable[0m
[32m- validate input dimensions[0m
[32m- do not fire the rule if not all expressions are resolved[0m
[32m- convert[0m
[32m- convert TimeZoneAwareExpression[0m
[32m- nullability inference in convert[0m
[32mPushProjectThroughUnionSuite:[0m
[32m- SPARK-25450 PushProjectThroughUnion rule uses the same exprId for project expressions in each Union child, causing mistakes in constant propagation[0m
[32mReusableStringReaderSuite:[0m
[32m- empty reader[0m
[32m- mark reset[0m
[32m- skip[0m
[32mLimitPushdownSuite:[0m
[32m- Union: limit to each side[0m
[32m- Union: limit to each side with constant-foldable limit expressions[0m
[32m- Union: limit to each side with the new limit number[0m
[32m- Union: no limit to both sides if children having smaller limit values[0m
[32m- Union: limit to each sides if children having larger limit values[0m
[32m- left outer join[0m
[32m- left outer join and left sides are limited[0m
[32m- left outer join and right sides are limited[0m
[32m- right outer join[0m
[32m- right outer join and right sides are limited[0m
[32m- right outer join and left sides are limited[0m
[32m- larger limits are not pushed on top of smaller ones in right outer join[0m
[32m- full outer join where neither side is limited and both sides have same statistics[0m
[32m- full outer join where neither side is limited and left side has larger statistics[0m
[32m- full outer join where neither side is limited and right side has larger statistics[0m
[32m- full outer join where both sides are limited[0m
[32mTableCatalogSuite:[0m
[32m- Catalogs can load the catalog[0m
[32m- listTables[0m
[32m- createTable[0m
[32m- createTable: with properties[0m
[32m- createTable: table already exists[0m
[32m- tableExists[0m
[32m- loadTable[0m
[32m- loadTable: table does not exist[0m
[32m- invalidateTable[0m
[32m- invalidateTable: table does not exist[0m
[32m- alterTable: add property[0m
[32m- alterTable: add property to existing[0m
[32m- alterTable: remove existing property[0m
[32m- alterTable: remove missing property[0m
[32m- alterTable: add top-level column[0m
[32m- alterTable: add required column[0m
[32m- alterTable: add column with comment[0m
[32m- alterTable: add nested column[0m
[32m- alterTable: add column to primitive field fails[0m
[32m- alterTable: add field to missing column fails[0m
[32m- alterTable: update column data type[0m
[32m- alterTable: update column data type and nullability[0m
[32m- alterTable: update optional column to required fails[0m
[32m- alterTable: update missing column fails[0m
[32m- alterTable: add comment[0m
[32m- alterTable: replace comment[0m
[32m- alterTable: add comment to missing column fails[0m
[32m- alterTable: rename top-level column[0m
[32m- alterTable: rename nested column[0m
[32m- alterTable: rename struct column[0m
[32m- alterTable: rename missing column fails[0m
[32m- alterTable: multiple changes[0m
[32m- alterTable: delete top-level column[0m
[32m- alterTable: delete nested column[0m
[32m- alterTable: delete missing column fails[0m
[32m- alterTable: delete missing nested column fails[0m
[32m- alterTable: table does not exist[0m
[32m- dropTable[0m
[32m- dropTable: table does not exist[0m
[32m- renameTable[0m
[32m- renameTable: fail if table does not exist[0m
[32m- renameTable: fail if new table name already exists[0m
[32m- listNamespaces: list namespaces from metadata[0m
[32m- listNamespaces: list namespaces from tables[0m
[32m- listNamespaces: list namespaces from metadata and tables[0m
[32m- loadNamespaceMetadata: fail if no metadata or tables exist[0m
[32m- loadNamespaceMetadata: no metadata, table exists[0m
[32m- loadNamespaceMetadata: metadata exists, no tables[0m
[32m- loadNamespaceMetadata: metadata and table exist[0m
[32m- createNamespace: basic behavior[0m
[32m- createNamespace: fail if metadata already exists[0m
[32m- createNamespace: fail if namespace already exists from table[0m
[32m- dropNamespace: drop missing namespace[0m
[32m- dropNamespace: drop empty namespace[0m
[32m- dropNamespace: drop even if it's not empty[0m
[32m- alterNamespace: basic behavior[0m
[32m- alterNamespace: create metadata if missing and table exists[0m
[32m- alterNamespace: fail if no metadata or table exists[0m
[32mTimeWindowSuite:[0m
[32m- time window is unevaluable[0m
[32m- blank intervals throw exception[0m
[32m- invalid intervals throw exception[0m
[32m- intervals greater than a month throws exception[0m
[32m- interval strings work with and without 'interval' prefix and return microseconds[0m
[32m- SPARK-21590: Start time works with negative values and return microseconds[0m
[32m- parse sql expression for duration in microseconds - string[0m
[32m- parse sql expression for duration in microseconds - integer[0m
[32m- parse sql expression for duration in microseconds - long[0m
[32m- parse sql expression for duration in microseconds - invalid interval[0m
[32m- parse sql expression for duration in microseconds - invalid expression[0m
[32m- SPARK-16837: TimeWindow.apply equivalent to TimeWindow constructor[0m
[32mBitwiseExpressionsSuite:[0m
[32m- BitwiseNOT[0m
[32m- BitwiseAnd[0m
[32m- BitwiseOr[0m
[32m- BitwiseXor[0m
[32mUnsafeRowConverterSuite:[0m
[32m- basic conversion with only primitive types with CODEGEN_ONLY[0m
[32m- basic conversion with only primitive types with NO_CODEGEN[0m
[32m- basic conversion with primitive, string and binary types with CODEGEN_ONLY[0m
[32m- basic conversion with primitive, string and binary types with NO_CODEGEN[0m
[32m- basic conversion with primitive, string, date and timestamp types with CODEGEN_ONLY[0m
[32m- basic conversion with primitive, string, date and timestamp types with NO_CODEGEN[0m
[32m- basic conversion with primitive, string and interval types with CODEGEN_ONLY[0m
[32m- basic conversion with primitive, string and interval types with NO_CODEGEN[0m
[32m- null handling with CODEGEN_ONLY[0m
[32m- null handling with NO_CODEGEN[0m
[32m- basic conversion with struct type with CODEGEN_ONLY[0m
[32m- basic conversion with struct type with NO_CODEGEN[0m
[32m- basic conversion with array type with CODEGEN_ONLY[0m
[32m- basic conversion with array type with NO_CODEGEN[0m
[32m- basic conversion with map type with CODEGEN_ONLY[0m
[32m- basic conversion with map type with NO_CODEGEN[0m
[32m- basic conversion with struct and array with CODEGEN_ONLY[0m
[32m- basic conversion with struct and array with NO_CODEGEN[0m
[32m- basic conversion with struct and map with CODEGEN_ONLY[0m
[32m- basic conversion with struct and map with NO_CODEGEN[0m
[32m- basic conversion with array and map with CODEGEN_ONLY[0m
[32m- basic conversion with array and map with NO_CODEGEN[0m
[32m- SPARK-25374 converts back into safe representation with CODEGEN_ONLY[0m
[32m- SPARK-25374 converts back into safe representation with NO_CODEGEN[0m
[36mRun completed in 7 minutes, 19 seconds.[0m
[36mTotal number of tests run: 4096[0m
[36mSuites: completed 225, aborted 0[0m
[36mTests: succeeded 4095, failed 1, canceled 0, ignored 3, pending 0[0m
[31m*** 1 TEST FAILED ***[0m
